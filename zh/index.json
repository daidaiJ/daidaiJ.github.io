[{"categories":null,"contents":"异步日志写实现 设计逻辑 内置 sync.pool 获取缓存，将 p 写入 bytes.Buffer，写入成功就将 buff 入队，然后使用轮询函数在循环中一直取队列里面的 buffer，使用刷写将缓存内容落盘，同时将缓存返回池中。在进程做优雅退出的时候，关联到异步写者，让其 for range 剩余的缓存，连续落盘； 落盘写函数逻辑：\n要写入的字节流长度是否大于缓存剩下的部分？ 是将当前写缓存内容刷写落盘， 否 直接追加到写缓存后面 异步轮询的 poller 函数会从队列里面取出 next writer，如果不是空的就直接执行缓存操作，这部分用 select 来做个定时操作，如果较长时间没有新的日志进来，就先把缓存里面有的数据落盘 go 代码实现 type FixSizeLargeBuff struct { buf []byte } const Megabit = 1024 * 1024 func NewFixSizeLargeBuff() *FixSizeLargeBuff { return \u0026amp;FixSizeLargeBuff{buf: make([]byte, 0, Megabit)} } func (f *FixSizeLargeBuff) Avail() int { return Megabit - len(f.buf) } func (f *FixSizeLargeBuff) Reset() { f.buf = f.buf[:0] } func (f *FixSizeLargeBuff) Append(p []byte) (int, error) { if f.Avail() \u0026lt; len(p) { return 0, fmt.Errorf(\u0026#34;no avail free bytes\u0026#34;) } f.buf = append(f.buf, p...) return len(p), nil } type SimpleAsyncWriter struct { data chan *FixSizeLargeBuff curbuff *FixSizeLargeBuff buffpool sync.Pool wt io.Writer lock sync.Mutex wg sync.WaitGroup ct *time.Ticker last time.Time active chan struct{} } func NewSimpleAsyncWriter(w io.Writer, limit int) *SimpleAsyncWriter { ret := \u0026amp;SimpleAsyncWriter{ data: make(chan *FixSizeLargeBuff, limit), buffpool: sync.Pool{New: func() any { return NewFixSizeLargeBuff() }}, wt: w, lock: sync.Mutex{}, active: make(chan struct{}), ct: time.NewTicker(1 * time.Second), } ret.addCount() go ret.poller() return ret } func (s *SimpleAsyncWriter) addCount() { s.wg.Add(1) } func (s *SimpleAsyncWriter) Write(p []byte) (int, error) { select { case \u0026lt;-s.active: return 0, ErrorWriteAsyncerIsClosed default: } s.last = time.Now() s.lock.Lock() defer s.lock.Unlock() select { case \u0026lt;-s.active: return 0, ErrorWriteAsyncerIsClosed case \u0026lt;-s.ct.C: if s.curbuff.Avail() \u0026gt; 0 \u0026amp;\u0026amp; time.Since(s.last) \u0026gt; 5*time.Second { s.data \u0026lt;- s.curbuff s.curbuff = s.buffpool.Get().(*FixSizeLargeBuff) } default: if s.curbuff == nil { s.curbuff = s.buffpool.Get().(*FixSizeLargeBuff) } if len(p) \u0026gt; s.curbuff.Avail() { s.data \u0026lt;- s.curbuff s.curbuff = s.buffpool.Get().(*FixSizeLargeBuff) } } if n, err := s.curbuff.Append(p); err != nil { return n, err } return len(p), nil } func (s *SimpleAsyncWriter) poller() { defer func() { for i := len(s.data); i \u0026gt; 0; i-- { d := \u0026lt;-s.data s.wt.Write(d.buf) } if s.curbuff.Avail() \u0026gt; 0 { s.wt.Write(s.curbuff.buf) } close(s.data) s.data = nil s.ct.Stop() s.wg.Done() }() for { select { case \u0026lt;-s.active: goto outer case d := \u0026lt;-s.data: s.wt.Write(d.buf) d.Reset() s.buffpool.Put(d) } } outer: } func (s *SimpleAsyncWriter) Stop() { s.active \u0026lt;- struct{}{} s.wg.Wait() } 基准测试 使用 law 的 benckmark 测试并给 BlackHoleWriter 类的 Writer 增加了时延模拟真实的落盘耗时，使用随机预先生成的字节数组队列来模拟真实负载填充，下面是 benckmark 测试的耗时\n第一列是测试项-cpu 数，第二列是每秒钟执行的次数，第三列是耗时，第四列是每个操作分配的字节数(这个可能是我预生成的随机字节数组拷贝时产生的)，第五列是每个操作的分配次数\n可以看到在 write 写操作平均耗时和平均分配的字节数来看，都有比较明显的优化，同时因为利用 defer 和 wait 机制联动，可以保证在调用 stop 是已经写入缓存的内容可以安全落盘，可以优雅推出，同步机制使用原生 channel 管道\n特点\n轻量级，代码简洁封装少，使用原生操作实现 多缓冲，使用sync.Pool 实现多级缓冲 安全并发， 虽然使用了互斥锁，但是仅用在write 方法上，对性能影响足够小 ","permalink":"http://localhost:1313/zh/chapter_1/","tags":null,"title":""},{"categories":null,"contents":"分布式一致性哈希 Golang 实现 哈希环定义 import \u0026#34;hash/crc32\u0026#34; type Hash func(data []byte) uint32 // 用crc32 var defaultHashFn = crc32.ChecksumIEEE // 哈希环 // 注意，非线程安全，业务需要自行加锁 type HashRing struct { hash Hash // 每个真实节点的虚拟节点数量 replicas int // 哈希环，按照节点哈希值排序 ring []int // 用来在新增节点时去重的 keys []string // 节点哈希值到真实节点字符串，哈希映射的逆过程 nodes map[int]string } func NewHashRing(r int,fn Hash)*HashRing(){ if fn==nil{ fn = defaultHashFn } return \u0026amp;HashRing{ replicas: r, hash: fn, ring: make([]int,0,8*r), keys: make([]string,0,8), nodes: make(map[int]string,8), } } 方法实现 import ( \u0026#34;slices\u0026#34; \u0026#34;sort\u0026#34; \u0026#34;strconv\u0026#34; ) func (h *HashRing) Add(nodes ...string) { for _, k := range nodes { exist := false for _, val := range h.keys { if val == k { exist = true } } if !exist { h.keys = append(h.keys, k) for i := 0; i \u0026lt; h.replicas; i++ { hash := int(h.hash([]byte(strconv.Itoa(i) + k))) h.ring = append(h.ring, hash) h.nodes[hash] = k } } } slices.Sort(h.ring) } func (h *HashRing) Len() int { return len(h.keys) } func (h *HashRing) Get(key string) string { if h.Len() == 0 { return \u0026#34;\u0026#34; } hash := int(h.hash([]byte(key))) // Binary search for appropriate replica. idx := sort.Search(len(h.ring), func(i int) bool { return h.ring[i] \u0026gt;= hash }) // Means we have cycled back to the first replica. if idx == len(h.ring) { idx = 0 } return h.nodes[h.ring[idx]] } func (h *HashRing)Rest(){ h.ring = h.ring[:0] h.keys = h.keys[:0] clear(h.nodes) } Add() 方法是用来创建节点环的,在一些节点退出后，可能需要reset 哈希环来再次重构哈希环 Reset() 是用来重置hash环信息的 Get() 就是用来获取 一致性哈希映射到的 节点的字符串上 ","permalink":"http://localhost:1313/zh/chapter_2/","tags":null,"title":""},{"categories":null,"contents":"文件多格式压缩 使用了一个压缩集合库来实现的这部分功能 compress\n代码实现 package kit import ( \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;syscall\u0026#34; gzip \u0026#34;github.com/klauspost/compress/gzip\u0026#34; snappy \u0026#34;github.com/klauspost/compress/s2\u0026#34; zip \u0026#34;github.com/klauspost/compress/zip\u0026#34; zstd \u0026#34;github.com/klauspost/compress/zstd\u0026#34; ) /* compress file func start */ // go build:+ linux var osChown = os.Chown type ( CompressType int ZWriter interface { io.WriteCloser Flush() error } ) const ( GZIP_TYEP CompressType = iota ZSTD_TYPE SNAPPY_TYPE ZIP_TYPE ) var suffixs = [4]string{\u0026#34;.gz\u0026#34;, \u0026#34;.zst\u0026#34;, \u0026#34;.snappy\u0026#34;, \u0026#34;.zip\u0026#34;} func chown(name string, info os.FileInfo) error { f, err := os.OpenFile(name, os.O_CREATE|os.O_WRONLY|os.O_TRUNC, info.Mode()) if err != nil { return err } f.Close() stat := info.Sys().(*syscall.Stat_t) return osChown(name, int(stat.Uid), int(stat.Gid)) } func CompressLogFile(src, dst string, ct CompressType, rm bool) (err error) { f, err := os.Open(src) if err != nil { return fmt.Errorf(\u0026#34;failed to open log file: %v\u0026#34;, err) } defer f.Close() if dst == src { dst += \u0026#34;out\u0026#34; } fi, err := os.Stat(src) if err != nil { return fmt.Errorf(\u0026#34;failed to stat log file: %v\u0026#34;, err) } if !strings.HasSuffix(dst, suffixs[ct]) { dst += suffixs[ct] } if err := chown(dst, fi); err != nil { return fmt.Errorf(\u0026#34;failed to chown compressed log file: %v\u0026#34;, err) } // If this file already exists, we presume it was created by // a previous attempt to compress the log file. zf, err := os.OpenFile(dst, os.O_CREATE|os.O_TRUNC|os.O_WRONLY, fi.Mode()) if err != nil { return fmt.Errorf(\u0026#34;failed to open compressed log file: %v\u0026#34;, err) } defer zf.Close() if ct == ZIP_TYPE { w := zip.NewWriter(zf) _, filename := filepath.Split(src) temp, err := w.Create(filename) if err != nil { return err } if _, err := io.Copy(temp, f); err != nil { w.Close() return err } if err := w.Close(); err != nil { return err } } else { var zwriter ZWriter switch ct { case GZIP_TYEP: zwriter = gzip.NewWriter(zf) case SNAPPY_TYPE: zwriter = snappy.NewWriter(zf, snappy.WriterSnappyCompat()) case ZSTD_TYPE: zwriter, _ = zstd.NewWriter(zf) } defer func() { if err != nil { os.Remove(dst) err = fmt.Errorf(\u0026#34;failed to compress log file: %v\u0026#34;, err) } }() if n, err := io.Copy(zwriter, f); err != nil { zwriter.Close() return err } else { fmt.Printf(\u0026#34;debug %d byets has beed write to dst path %s\u0026#34;, n, dst) } zwriter.Flush() if err := zwriter.Close(); err != nil { return err } } if err := zf.Close(); err != nil { return err } if err := f.Close(); err != nil { return err } if rm { if err := os.Remove(src); err != nil { return err } } return nil } 这部分主要是通过枚举加分支逻辑来处理不同压缩格式的处理，通过io.Copy 来做数据管道\n","permalink":"http://localhost:1313/zh/chapter_3/","tags":null,"title":""},{"categories":null,"contents":"ETCD 手册 KV 操作 WithIgnoreLease() 使用租约时可以用这个，当key 不存在时会返回错误 WithPrevKV() 可以返回更新前的KV值 WithIgnoreValue() 普通put 使用这个key不存在时会返回错误 WithSort(clientv3.SortByKey, clientv3.SortDescend) 可以让在查询的时候使用特定的排序方式 WithPrefix() 这可以可以按照key，查找前缀是key字符串的所有值； Get() 使用的WithRev(presp.Header.Revision) ，中的版本号可以时某次put操作返回的版本号，我觉得get的其实也是可以的；\n看了下源码 ResponseHeader 这东西里面塞了： ClusterId 和这个消息交互的集群的id\nMemberId 节点id Revision 消息版本 RaftTerm 选举的周期\n// 这里获取版本后，该版本之前的历史数据存储开始进行合并压缩 // 这里会生成快照吗？ 按照文档上说这个操作应该是要定时进行的 compRev := resp.Header.Revision // specify compact revision of your choice ctx, cancel = context.WithTimeout(context.Background(), requestTimeout) _, err = cli.Compact(ctx, compRev) func (Maintenance).Status(ctx Context, endpoint string) 可以获取集群的状态 func (Maintenance).Defragment(ctx Context, endpoint string) 这可以开启etcd 的碎片整理\n授权管理 先是简单的通过用户名密码来验证\n// 这部分可以手动来进行的 // etcdctl --user root role add r if _, err = cli.RoleAdd(context.TODO(), \u0026#34;r\u0026#34;); err != nil { log.Fatal(err) } // etcdctl --user root role grant-permission r foo zoo // 使用 -prefix=true 可以仅指定开头前缀 if _, err = cli.RoleGrantPermission(context.TODO(),\u0026#34;r\u0026#34;, \u0026#34;foo\u0026#34;, \u0026#34;zoo\u0026#34;, clientv3.PermissionType(clientv3.PermReadWrite),); err != nil { log.Fatal(err) } // etcdctl --user root user add u --new-user-password 123 if _, err = cli.UserAdd(context.TODO(), \u0026#34;u\u0026#34;, \u0026#34;123\u0026#34;); err != nil { log.Fatal(err) } // etcdctl --user root user grant-role u r if _, err = cli.UserGrantRole(context.TODO(), \u0026#34;u\u0026#34;, \u0026#34;r\u0026#34;); err != nil { log.Fatal(err) } // etcdctl auth enable if _, err = cli.AuthEnable(context.TODO()); err != nil { log.Fatal(err) } // 这里使用 root 角色的用户来登录 rootCli, err := clientv3.New(clientv3.Config{ Endpoints: exampleEndpoints(), DialTimeout: dialTimeout, Username: \u0026#34;root\u0026#34;, Password: \u0026#34;123\u0026#34;, }) if err != nil { log.Fatal(err) } defer rootCli.Close() // root 用户可以获取别的 用户或者角色的数据 etcdctl --user root role get r resp, err := rootCli.RoleGet(context.TODO(), \u0026#34;r\u0026#34;) if err != nil { log.Fatal(err) } // 可以获得 角色权限的信息 fmt.Printf(\u0026#34;user u permission: key %q, range end %q\\n\u0026#34;, resp.Perm[0].Key, resp.Perm[0].RangeEnd) // 这里关闭身份校验 etcdctl auth disable if _, err = rootCli.AuthDisable(context.TODO()); err != nil { log.Fatal(err) } 建立客户端连接时使用的证书\ntlsInfo := transport.TLSInfo{ CertFile: \u0026#34;/tmp/test-certs/test-name-1.pem\u0026#34;, KeyFile: \u0026#34;/tmp/test-certs/test-name-1-key.pem\u0026#34;, TrustedCAFile: \u0026#34;/tmp/test-certs/trusted-ca.pem\u0026#34;, } tlsConfig, err := tlsInfo.ClientConfig() if err != nil { log.Fatal(err) } cli, err := clientv3.New(clientv3.Config{ Endpoints: exampleEndpoints(), DialTimeout: dialTimeout, TLS: tlsConfig, }) 事务 STM is an interface for software transactional memory. 事务使用 MVCC多版本控制，在事务执行的函数类使用 STM 来读写键值\n// Txn 这个简单的事务接口，还是基于客户端连接来的 kvc := clientv3.NewKV(cli) _, err = kvc.Put(context.TODO(), \u0026#34;key\u0026#34;, \u0026#34;xyz\u0026#34;) if err != nil { log.Fatal(err) } ctx, cancel := context.WithTimeout(context.Background(), requestTimeout) // if 条件成立 会执行 then 分支的修改，否则会执行else 分支的操作 _, err = kvc.Txn(ctx). // txn value comparisons are lexical If(clientv3.Compare(clientv3.Value(\u0026#34;key\u0026#34;), \u0026#34;\u0026gt;\u0026#34;, \u0026#34;abc\u0026#34;)). // the \u0026#34;Then\u0026#34; runs, since \u0026#34;xyz\u0026#34; \u0026gt; \u0026#34;abc\u0026#34; Then(clientv3.OpPut(\u0026#34;key\u0026#34;, \u0026#34;XYZ\u0026#34;)). // the \u0026#34;Else\u0026#34; does not run Else(clientv3.OpPut(\u0026#34;key\u0026#34;, \u0026#34;ABC\u0026#34;)). Commit() // exchange := func(stm concurrency.STM) { from, to := rand.Intn(totalAccounts), rand.Intn(totalAccounts) if from == to { // nothing to do return } // read values fromK, toK := fmt.Sprintf(\u0026#34;accts/%d\u0026#34;, from), fmt.Sprintf(\u0026#34;accts/%d\u0026#34;, to) fromV, toV := stm.Get(fromK), stm.Get(toK) fromInt, toInt := 0, 0 fmt.Sscanf(fromV, \u0026#34;%d\u0026#34;, \u0026amp;fromInt) fmt.Sscanf(toV, \u0026#34;%d\u0026#34;, \u0026amp;toInt) // transfer amount xfer := fromInt / 2 fromInt, toInt = fromInt-xfer, toInt+xfer // write back stm.Put(fromK, fmt.Sprintf(\u0026#34;%d\u0026#34;, fromInt)) stm.Put(toK, fmt.Sprintf(\u0026#34;%d\u0026#34;, toInt)) return } // concurrently exchange values between accounts var wg sync.WaitGroup wg.Add(10) for i := 0; i \u0026lt; 10; i++ { go func() { defer wg.Done() if _, serr := concurrency.NewSTM(cli, func(stm concurrency.STM) error { exchange(stm) return nil }); serr != nil { log.Fatal(serr) } }() } wg.Wait() 普通的 kv api 其实也有一个Txn ,但是同一个key 只能修改一次\norderingKv := ordering.NewKV(cli.KV, func(op clientv3.Op, resp clientv3.OpResponse, prevRev int64) error { return errOrderViolation }) orderingTxn := orderingKv.Txn(ctx) _, err = orderingTxn.If( clientv3.Compare(clientv3.Value(\u0026#34;b\u0026#34;), \u0026#34;\u0026gt;\u0026#34;, \u0026#34;a\u0026#34;), ).Then( clientv3.OpGet(\u0026#34;foo\u0026#34;), ).Commit() if err != nil { t.Fatal(err) } 租约 租约有点像 go 里面的上下文，租约过期时会撤销掉这期间的更改；同时在func (Lease).Revoke(ctx Context, id LeaseID) 释放租约的时候，之前修改会被视作失效了；func (Lease).KeepAliveOnce(ctx Context, id LeaseID) 可以手动续约，避免租约超期被取消了；\nkey 和 Lease 是多对一的关系。一个 key 最多只能挂绑定一个 Lease ，但是一个 Lease 上能挂多个 key 。租约在申请下来后，关联的操作，我觉得全是修改，会被关联到这个租约的 map 里面，这段事件应该是独占这些个 key 的所有权，所以加进来的key修改，在租约失效的时候，反向调用Txn 来删除这些key，就能把之前的版本恢复\nlease, err := cli.Grant(context.Background(), 100) if err != nil { t.Fatal(err) } // 每个会话会有一个唯一的ID 和TTL 存活时间 s, err := concurrency.NewSession(cli, concurrency.WithLease(lease.ID)) if err != nil { t.Fatal(err) } defer s.Close() assert.Equal(t, s.Lease(), lease.ID) go s.Orphan() select { case \u0026lt;-s.Done(): case \u0026lt;-time.After(time.Millisecond * 100): t.Fatal(\u0026#34;session did not get orphaned as expected\u0026#34;) } 使用租约来控制的会话会比租约更早结束，以免出现并发控制的问题？这个和上面的互斥锁连用就可以实现租约时长来控制的互斥锁，超时会退出，并撤销操作？ 另外可以给租约设置 TTL 也就是生存时间\ns, err := concurrency.NewSession(cli, concurrency.WithTTL(setTTL)) if err != nil { t.Fatal(err) } defer s.Close() leaseID := s.Lease() // TTL retrieved should be less than the set TTL, but not equal to default:60 or exprired:-1 resp, err := cli.Lease.TimeToLive(context.Background(), leaseID) if err != nil { t.Log(err) } if resp.TTL == -1 { t.Errorf(\u0026#34;client lease should not be expired: %d\u0026#34;, resp.TTL) } if resp.TTL == 60 { t.Errorf(\u0026#34;default TTL value is used in the session, instead of set TTL: %d\u0026#34;, setTTL) } if resp.TTL \u0026gt;= int64(setTTL) || resp.TTL \u0026lt; int64(setTTL)-20 { t.Errorf(\u0026#34;Session TTL from lease should be less, but close to set TTL %d, have: %d\u0026#34;, setTTL, resp.TTL) } 这里可以看到 租约的实际时间是比设置的要短的\nlease, err := cli.Grant(context.Background(), 100) if err != nil { t.Fatal(err) } s, err := concurrency.NewSession(cli, concurrency.WithLease(lease.ID)) if err != nil { t.Fatal(err) } defer s.Close() assert.Equal(t, s.Lease(), lease.ID) // 主要是通过 会话的上下文的Done 来控制会话内操作的退出 childCtx, cancel := context.WithCancel(s.Ctx()) defer cancel() go s.Orphan() select { case \u0026lt;-childCtx.Done(): case \u0026lt;-time.After(time.Millisecond * 100): t.Fatal(\u0026#34;child context of session context is not canceled\u0026#34;) } 会话和 go 原生的上下文的使用； 总结一下：\n租约加 会话加互斥锁 可以实现分布式锁 租约加会话加 上下文，可以取消会话内协程的执行 分布式锁 etcd 3有个并发api ，调用这个api 可以实现分布式锁，锁会持有到主动解锁或者租期到了\n// 新建会话是一个标准流程表，因为下面申请锁需要通过一个会话来进行 s1, err := concurrency.NewSession(cli) if err != nil { t.Fatal(err) } defer s1.Close() m1 := concurrency.NewMutex(s1, \u0026#34;/my-lock/\u0026#34;) if err = m1.Lock(context.TODO()); err != nil { t.Fatal(err) } // 这之间就是s1 获得锁的临界区 if err := m1.Unlock(context.TODO()); err != nil { t.Fatal(err) } 如果先调用解锁，会得到ErrLockReleased 也就是锁已经被释放了，或者没有获得锁，总而言之就是当前没有持有锁\n服务发现和注册 实际是etcd根据mainID去磁盘查数据，磁盘中数据以revision.main+revision.sub为key(bbolt 数据库中的key)，所以就会依次遍历出所有的版本数据。同时判断遍历到的value中的key(etcd中的key)是不是用户watch的，是则推送给用户。\n这里每次都会遍历数据库性能可能会很差，实际使用时一般用户只会关注最新的revision，不会去关注旧数据。\n采用了MVCC，以一种优雅的方式解决了锁带来的问题。执行写操作或删除操作时不会再原数据上修改而是创建一个新版本。这样并发的读取操作仍然可以读取老版本的数据，写操作也可以同时进行。这个模式的好处在于读操作不再阻塞，事实上根本就不需要锁。 客户端读key的时候指定一个版本号，服务端保证返回比这个版本号更新的数据，但不保证返回最新的数据。 MVCC能最大化地实现高效地读写并发，尤其是高效地读，非常适合读多写少的场景。\n客户端使用watch 来获取服务端地址\nvar serviceTarget = \u0026#34;Hello\u0026#34; type remoteService struct { name string nodes map[string]string mutex sync.Mutex } service = \u0026amp;remoteService { name: serviceTarget } kv := clientv3.NewKV(etcdClient) rangeResp, err := kv.Get(context.TODO(), service.name, clientv3.WithPrefix()) if err != nil { panic(err) } service.mutex.Lock() for _, kv := range rangeResp.Kvs { service.nodes[string(kv.Key)] = string(kv.Value) } service.mutex.Unlock() go watchServiceUpdate(etcdClient, service) // 监控服务目录下的事件 func watchServiceUpdate(etcdClient clientv3.Client, service *remoteService) { watcher := clientv3.NewWatcher(client) // Watch 服务目录下的更新 watchChan := watcher.Watch(context.TODO(), service.name, clientv3.WithPrefix()) for watchResp := range watchChan { // 这里对增删时间的响应，会使用互斥锁来解决并发的数据修改问题 for _, event := range watchResp.Events { service.mutex.Lock() switch (event.Type) { case mvccpb.PUT://PUT事件，目录下有了新key service.nodes[string(event.Kv.Key)] = string(event.Kv.Value) case mvccpb.DELETE://DELETE事件，目录中有key被删掉(Lease过期，key 也会被删掉) delete(service.nodes, string(event.Kv.Key)) } service.mutex.Unlock() } } } 服务端主要是注意租约的维护\n// 将服务注册到etcd上 func RegisterServiceToETCD(ServiceTarget string, value string) { dir = strings.TrimRight(ServiceTarget, \u0026#34;/\u0026#34;) + \u0026#34;/\u0026#34; client, err := clientv3.New(clientv3.Config{ Endpoints: []string{\u0026#34;localhost:2379\u0026#34;}, DialTimeout: 5 * time.Second, }) if err != nil { panic(err) } kv := clientv3.NewKV(client) lease := clientv3.NewLease(client) var curLeaseId clientv3.LeaseID = 0 for { if curLeaseId == 0 { leaseResp, err := lease.Grant(context.TODO(), 10) if err != nil { panic(err) } key := ServiceTarget + fmt.Sprintf(\u0026#34;%d\u0026#34;, leaseResp.ID) if _, err := kv.Put(context.TODO(), key, value, clientv3.WithLease(leaseResp.ID)); err != nil { panic(err) } curLeaseId = leaseResp.ID } else { // 续约租约，如果租约已经过期将curLeaseId复位到0重新走创建租约的逻辑 if _, err := lease.KeepAliveOnce(context.TODO(), curLeaseId); err == rpctypes.ErrLeaseNotFound { curLeaseId = 0 continue } } time.Sleep(time.Duration(1) * time.Second) } } 使用 watch 监视的时候 clientv3.WithRev(1) 可以指定从哪个版本开始获取，clientv3.WithFragment() 会允许服务端将事件分页发送过来\nselect { case ws := \u0026lt;-wch: // 没启用分页的时候，因为对应的 key 的值太大了，旧没接收到 if !fragment \u0026amp;\u0026amp; exceedRecvLimit { if len(ws.Events) != 0 { t.Fatalf(\u0026#34;expected 0 events with watch fragmentation, got %d\u0026#34;, len(ws.Events)) } exp := \u0026#34;code = ResourceExhausted desc = grpc: received message larger than max (\u0026#34; if !strings.Contains(ws.Err().Error(), exp) { t.Fatalf(\u0026#34;expected \u0026#39;ResourceExhausted\u0026#39; error, got %v\u0026#34;, ws.Err()) } return } // 启用分页将每次发送的数据分成限制内大小后，拿到的分页数，这个事件本身是键值对的一个切片，里面的元素是类似CPP 的 pair 这种键值二元组 if len(ws.Events) != 10 { t.Fatalf(\u0026#34;expected 10 events with watch fragmentation, got %d\u0026#34;, len(ws.Events)) } if ws.Err() != nil { t.Fatalf(\u0026#34;unexpected error %v\u0026#34;, ws.Err()) } case \u0026lt;-time.After(testutil.RequestTimeout): t.Fatalf(\u0026#34;took too long to receive events\u0026#34;) } 使用 cfg.ClientMaxCallRecvMsgSize = 1.5 * 1024 * 1024 修改集群配置时，会限制集群给客户端发送消息大小\n观测 import( grpcprom \u0026#34;github.com/grpc-ecosystem/go-grpc-prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; ) // 这样在客户端的 grpc 连接里面塞两个Prometheus的中间件进去 cli, err := clientv3.New(clientv3.Config{ Endpoints: exampleEndpoints(), DialOptions: []grpc.DialOption{ grpc.WithUnaryInterceptor(grpcprom.UnaryClientInterceptor), grpc.WithStreamInterceptor(grpcprom.StreamClientInterceptor), }, }) if err!=nil{ log.Fatal(err) } defer cli.close() // 开个 http 服务端 ln, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:0\u0026#34;) if err != nil { log.Fatal(err) } defer ln.close() http.Serve(ln, promhttp.Handler()) // 现在就可以被监听到了 调优 io优先级\nsudo ionice -c2 -n0 -p `pgrep etcd` 快照触发数量\netcd --snapshot-count=5000 心跳和选举时间\netcd --heartbeat-interval=100 --election-timeout=500 cpu\necho performance | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor 一些维护操作 etcdctl member list -w table # 可以查看节点信息 etcdctl move-leader XXID --endpoints 127.0.0.1:2379 etcdctl member remove xxxID # 重新将一个节点添加到集群里面来 etcdctl member add etcd01 --peer-urls=\u0026#34;https://xxxxxxx:2380\u0026#34; # 对某个节点存储快照 etcdctl --endpoints=https://10.184.4.240:2380 snapshot save snapshot.db # 从节点快照恢复数据 etcdctl snapshot restore snapshot.db --name etcd01 --initial-cluster etcd01=https://10.184.4.238:2379,etcd02=https://10.184.4.239:2379,etcd03=https://10.184.4.240:2379 --initial-cluster-token etcd-cluster --initial-advertise-peer-urls https://10.184.4.238:2380 使用客户端api 也是可以实现上面的操作的\n// 添加一个 节点进来 2380 一般是这个端口，用来做集群间通信的，那个2379的是用监听客户端的 mresp, err := cli.MemberAdd(context.Background(), []string{\u0026#34;http://localhost:32380\u0026#34;}) if err != nil { log.Fatal(err) } fmt.Println(\u0026#34;added member.PeerURLs:\u0026#34;, mresp.Member.PeerURLs) fmt.Println(\u0026#34;members count:\u0026#34;, len(mresp.Members)) // Restore original cluster state _, err = cli.MemberRemove(context.Background(), mresp.Member.ID) if err != nil { log.Fatal(err) } // 这个添加进来做从节点？ mresp, err := cli.MemberAddAsLearner(context.Background(), []string{\u0026#34;http://localhost:32381\u0026#34;}) if err != nil { log.Fatal(err) } // 这里用来获取集群的节点列表 resp, err := cli.MemberList(context.Background()) if err != nil { log.Fatal(err) } // 修改节点的内部通信地址 peerURLs := []string{\u0026#34;http://localhost:12380\u0026#34;} _, err = cli.MemberUpdate(context.Background(), resp.Members[0].ID, peerURLs) if err != nil { log.Fatal(err) } 快照 etcd 的快照和虚拟机的快照比较类似，是摸一个时间点etcd 节点的所有数据；快照是一个checkpoint，避免因为wal 数据被无限制写入，导致体量超大，通过checkpoint做一个记录，后续的wal可以做增量，checkpoint生成的快照充当的应该是快照前的数据，发生修改后的数据会在wal上，(也不能这么说，因为wal记录本来就是修改记录)。\n","permalink":"http://localhost:1313/zh/etcd/","tags":null,"title":""},{"categories":null,"contents":"faketcp 原理简介 背景 通过udp 实现多路传输时，可能会被运营商通过qos策略给丢掉；通过rawsocket 机制在udp报文上增加tcp 头来伪装成tcp包，通过增加一部封拆包头的代价来换取udp通信的可用性\n代码实现 这里没有做具体的udp包体封装的例子，但是字节流数据传输可见是完整的，将一个udp报文转成[]byte切片来使用\npackage main import ( \u0026#34;encoding/binary\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;sync\u0026#34; // \u0026#34;github.com/xitongsys/ethernet-go/header\u0026#34; ) type TCP struct { SrcPort uint16 DstPort uint16 Seq uint32 Ack uint32 Offset uint8 Flags uint8 Win uint16 Checksum uint16 UrgPointer uint16 Opt uint32 } type IPv4Pseudo struct { Src uint32 Dst uint32 Reserved uint8 Protocol uint8 Len uint16 } func (h *IPv4Pseudo) Marshal() []byte { headerLen := int(12) res := make([]byte, headerLen) binary.BigEndian.PutUint32(res[0:], h.Src) binary.BigEndian.PutUint32(res[4:], h.Dst) res[8] = byte(h.Reserved) res[9] = byte(h.Protocol) binary.BigEndian.PutUint16(res[10:], h.Len) return res } func (h *TCP) Marshal() []byte { res := make([]byte, 20) binary.BigEndian.PutUint16(res, h.SrcPort) binary.BigEndian.PutUint16(res[2:], h.DstPort) binary.BigEndian.PutUint32(res[4:], h.Seq) binary.BigEndian.PutUint32(res[8:], h.Ack) res[12] = byte(h.Offset) res[13] = byte(h.Flags) binary.BigEndian.PutUint16(res[14:], h.Win) binary.BigEndian.PutUint16(res[16:], h.Checksum) binary.BigEndian.PutUint16(res[18:], h.UrgPointer) return res } func ReCalTcpCheckSum(bs []byte, src, dst uint32) error { if len(bs) \u0026lt; 20 { return fmt.Errorf(\u0026#34;too short\u0026#34;) } ipps := IPv4Pseudo{} ipps.Src = src ipps.Dst = dst ipps.Reserved = 0 ipps.Protocol = 6 ipps.Len = uint16(len(bs)) ippsbs := ipps.Marshal() tcpbs := bs tcpbs[16] = 0 tcpbs[17] = 0 if len(tcpbs)%2 == 1 { tcpbs = append(tcpbs, byte(0)) } s := uint32(0) for i := 0; i \u0026lt; len(ippsbs); i += 2 { s += uint32(binary.BigEndian.Uint16(ippsbs[i : i+2])) } for i := 0; i \u0026lt; len(tcpbs); i += 2 { s += uint32(binary.BigEndian.Uint16(tcpbs[i : i+2])) } for (s \u0026gt;\u0026gt; 16) \u0026gt; 0 { s = (s \u0026gt;\u0026gt; 16) + (s \u0026amp; 0xffff) } binary.BigEndian.PutUint16(tcpbs[16:], ^uint16(s)) return nil } // https://sourcegraph.com/github.com/xitongsys/ethernet-go@master/-/blob/header/tcp.go func server(ch chan\u0026lt;- struct{}, wg *sync.WaitGroup) { ipaddr, _ := net.ResolveIPAddr(\u0026#34;ip4\u0026#34;, \u0026#34;127.0.0.2\u0026#34;) con, err := net.ListenIP(\u0026#34;ip4:6\u0026#34;, ipaddr) if err != nil { log.Fatalf(\u0026#34;tcp listening failed %+v\\n\u0026#34;, err) } buf := make([]byte, 1024) fmt.Printf(\u0026#34;开始接收%v\\n\u0026#34;, con.LocalAddr()) ch \u0026lt;- struct{}{} for range 3 { { n, from, _ := con.ReadFromIP(buf) if n \u0026gt; 20 { fmt.Printf(\u0026#34;[%s] for [%s]\\n\u0026#34;, string(buf[20:n]), from.String()) } else { fmt.Printf(\u0026#34;too short %d\\n\u0026#34;, n) } } } close(ch) con.Close() wg.Done() } const ( FIN = 0x01 SYN = 0x02 RST = 0x04 PSH = 0x08 ACK = 0x10 URG = 0x20 ECE = 0x40 CWR = 0x80 ) func BuildTcpHeader(src, dst uint32, sport, dport uint16, flag uint8, data []byte) []byte { tcpheader := \u0026amp;TCP{ SrcPort: sport, DstPort: dport, Seq: 1, Ack: 1, Offset: 0x50, Flags: flag, Win: ^uint16(0), Checksum: 0, UrgPointer: 0, } result := make([]byte, len(data)+20) copy(result, tcpheader.Marshal()) copy(result[20:], data) ReCalTcpCheckSum(result, src, dst) return result } func client(ch \u0026lt;-chan struct{}) { laddr, _ := net.ResolveIPAddr(\u0026#34;ip4\u0026#34;, \u0026#34;127.0.0.3\u0026#34;) raddr, _ := net.ResolveIPAddr(\u0026#34;ip4\u0026#34;, \u0026#34;127.0.0.2\u0026#34;) con, err := net.DialIP(\u0026#34;ip4:6\u0026#34;, laddr, raddr) if err != nil { log.Fatalf(\u0026#34;ip conn create failed %+v\\n\u0026#34;, err) } \u0026lt;-ch for i := range 3 { buf := BuildTcpHeader(binary.BigEndian.Uint32(laddr.IP.To4()), binary.BigEndian.Uint32(raddr.IP.To4()), 7742, 7743, SYN, []byte(fmt.Sprintf(\u0026#34;test data packet%d\u0026#34;, i))) con.Write(buf) } \u0026lt;-ch fmt.Println(\u0026#34;has write\u0026#34;) con.Close() } func main() { var wg sync.WaitGroup ch := make(chan struct{}) wg.Add(1) go server(ch, \u0026amp;wg) fmt.Println(\u0026#34;server start\u0026#34;) client(ch) fmt.Println(\u0026#34;client close\u0026#34;) wg.Wait() } 后期会补一些原理图上去\n","permalink":"http://localhost:1313/zh/faketcp/","tags":null,"title":""},{"categories":null,"contents":"Prometheus exporter pprof 优化 这个工程是 基于 Prometheus client-go 的库来开发的，这个库的主要流程是将 collector 的数据用http server的形式通过metrics 路由交出去，背景状况是现在这个收集器的收集间隔很长基本上是6个小时更新一次，但是负责抓取这些指标数据的服务leader 又不同意改成6小时的大跨度，所以之前的措施是把硬件采集的数据缓存起来，但是更新后的版本cpu占用率和响应时间都没太大优化，需要定位这个问题\n怎么采集后台运行进程性能数据 使用ps -ef | grep xxx 来获取进程号pid pidstat -u -p pid 15 4 来采集进程的cpu 占用率 top 按t 和m 来切换 cpu 和 内存排序 go 使用 _ \u0026ldquo;net/http/pprof\u0026rdquo; 然后非http 网络服务就再增加一个拉起http 服务的几行代码 使用curl -o cpupgo.out http://your_address:your_port/debug/pprof/profile?seconds=60 来采样一分钟的运行数据 go tool pprof -http=:9000 cpupgo.out 使用 pprof 工具开启一个网络服务在web网页上查看性能采样 使用 perf 工具来采样 基本上会用到 record 和 report 这些，然后转成火山图来分析 问题追踪 问题1. 缓存为什么没有生效(降低延迟减少耗时操作) 通过go 的pprof 对后台运行的服务采样后发现大部分cpu 时间在生成Metric 相关的结构体上，同时缓存的数据格式是json，取json 数据会用到仿射，这使得组装Metric 的过程中充斥着大量的耗时操作，于是选择将json 数据缓存改成 Metric 数据缓冲，和时效时间戳一起封装成一个抽象的容器，在未过期时会将，slice 里面所有的Metric 通过管道发送出去，过期时会将slice 长度重置，发送Metric 的同时将其append 到slice 里面缓存，总结来说，缓存生效了，但是又没完全覆盖到所有耗时操作上。\n改进后的缓存实现，在应用中遇到了新的问题，缓存应用后没被触发？\n问题2. 缓存为什么没被触发？ 复盘对比了两种缓存机制和Collect 方法被调用的过程时发现，Collect 中声明的对象在每次调用时重新创建的，之前json 缓存是用的全局变量，所以创建前后用的都是一个缓存；这里将新的缓存实现也没大改，给新建的这些对象实例也做个全局缓存，没过期失效前这些实例就不会被重新创建，减少了一些再分配构建的过程，通过预留的 cache stat handler 可以看到缓存除了初次和过期时未命中外，其余时刻缓存全命中，符合预期\n新的全局缓冲实现生效了，将cpu 占用率降低到原先的30%，同时内存占用差别不大，但剩下的30%还能不能继续优化呢？\n终极优化方案 通过对 Prometheus client go 的源码阅读，确定了相应http 响应的整个构造流程，脑中浮现了一个比较极端的想法，缓冲响应；\n这个适合用来缓存响应内容在一段时间内不会发生改动的 http handler 接口对象\n/* get resp cache code start */ type respCacheWriter struct { header http.Header expireat time.Time statuscode int buf []byte update bool } func newRespCacheWriter() *respCacheWriter { return \u0026amp;respCacheWriter{ header: make(http.Header, 3), buf: make([]byte, 0, 1024*4), } } func (r *respCacheWriter) NotExpire() bool { return time.Now().Before(r.expireat)\u0026amp;\u0026amp;!r.update } func (r *respCacheWriter) Update(interval time.Duration) { r.expireat = time.Now().Add(interval) r.update = false } func (r *respCacheWriter) Header() http.Header { return r.header } func (r *respCacheWriter) SetUpdate() http.Header { return r.update = true } func (r *respCacheWriter) WriteHeader(statusCode int) { r.statuscode = statusCode } func (r *respCacheWriter) Write(p []byte) (int, error) { if p == nil { return 0, fmt.Errorf(\u0026#34;Write []byte length should not be zero\u0026#34;) } r.buf = append(r.buf, p...) return len(r.buf), nil } type GetRespCache struct { interval time.Duration cache map[string]*respCacheWriter next http.Handler } func NewGetRespCache(i time.Duration,next http.Handler)GetRespCache{ return GetRespCache{ interval: i, cache: make(map[string]*respCacheWriter,1), next: next, } } // 这个方法其实也可以 转成私有的，但是不会修改cache 状态所以无所谓 func (g *GetRespCache)UpdateCache()bool{ key := fmt.Sprintf(\u0026#34;key%v\u0026#34;, r.URL.Query()) if c,ok:=g.cache[key];key\u0026amp;\u0026amp;c!=nil{ c.SetUpdate() } } func (g *GetRespCache) ServeHTTP(w http.ResponseWriter, r *http.Request) { key := fmt.Sprintf(\u0026#34;key%v\u0026#34;, r.URL.Query()) if val, ok := g.cache[key]; ok \u0026amp;\u0026amp; val != nil\u0026amp;\u0026amp;val.NotExpire() { g.generateResp(val, w) } else { // 调用 write 方法时 缓存响应的 header resp := newRespCacheWriter() g.next.ServeHTTP(resp, r) g.generateResp(resp, w) g.update(key, resp) } } func (g *GetRespCache) update(key string, resp *respCacheWriter) { // 这里会更新过期时间和下一个响应状态 resp.Update(g.interval) g.cache[key] = resp } func (*GetRespCache) generateResp(val *respCacheWriter, w http.ResponseWriter) { for k, s := range val.header { for _, v := range s { w.Header().Set(k, v) } } w.WriteHeader(val.statuscode) w.Write(val.buf) } /* get resp cache code end */ 可以看到其实就是通过中间的代理接口，将被代理的handler 函数的修改缓存起来，根据get 请求的query值来做hash 返回响应的； 过期的时效间隔这里倒是比较粗，用的是同一个过期间隔；\n这个get 缓存方案是我最看好的:\n第一点基本上是即插即用，迁移性好兼容性好， 第二点是性能更好，缓存占用少，还剩去了内部handler 处理的时间 这个之所以能用在这个场景上，其实是需求造成的，抓取端不改动，数据供应端又允许缓存；所以这个getcache 的方案理论上有奇效，但是最终还是没应用上这个，确定是按照问题2解决后的实现方案来。\n","permalink":"http://localhost:1313/zh/gopprof/","tags":null,"title":""},{"categories":null,"contents":"gRPC 使用手册 环境准备 grpc 是使用protobuf 协议的 需要安装对应的编译器\ngo install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28 go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2 定义好 .proto 文件之后可以使用 protoc 编译器来生成对应语言的代码\nprotoc --go_out=./proto/ --go_opt=paths=source_relative --go-grpc_out=./proto/ --go-grpc_opt=paths=source_relative ./proto/your.proto 基础的流程 // 单次调用 ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() resp, err := client.UnaryEcho(ctx, \u0026amp;ecpb.EchoRequest{Message: message}) if err != nil { log.Fatalf(\u0026#34;client.UnaryEcho(_) = _, %v: \u0026#34;, err) } // 流接收 func recvMessage(stream pb.Echo_BidirectionalStreamingEchoClient, wantErrCode codes.Code) { res, err := stream.Recv() if status.Code(err) != wantErrCode { log.Fatalf(\u0026#34;stream.Recv() = %v, %v; want _, status.Code(err)=%v\u0026#34;, res, err, wantErrCode) } if err != nil { fmt.Printf(\u0026#34;stream.Recv() returned expected error %v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;received message %q\\n\u0026#34;, res.GetMessage()) } // 在接受流的时候要验证 err 是不是EOF for { in, err := stream.Recv() if err != nil { fmt.Printf(\u0026#34;server: error receiving from stream: %v\\n\u0026#34;, err) if err == io.EOF { return nil } return err } fmt.Printf(\u0026#34;echoing message %q\\n\u0026#34;, in.Message) stream.Send(\u0026amp;pb.EchoResponse{Message: in.Message}) } OAuth token 验证 因为有两种rpc 调用 一种是 单次调用 一种是流式调用； 在客户端 client 建立连接时使用的opts 中使用\n// fetchToken 表示获取token 的动作,使用 tokensource 获取带时效时间的 token perRPC := oauth.TokenSource{TokenSource: oauth2.StaticTokenSource(fetchToken())} creds, err := credentials.NewClientTLSFromFile(data.Path(\u0026#34;x509/ca_cert.pem\u0026#34;), \u0026#34;x.test.example.com\u0026#34;) if err != nil { log.Fatalf(\u0026#34;failed to load credentials: %v\u0026#34;, err) } opts := []grpc.DialOption{ // In addition to the following grpc.DialOption, callers may also use // the grpc.CallOption grpc.PerRPCCredentials with the RPC invocation // itself. // See: https://godoc.org/google.golang.org/grpc#PerRPCCredentials grpc.WithPerRPCCredentials(perRPC), // oauth.TokenSource requires the configuration of transport // credentials. grpc.WithTransportCredentials(creds), } 在服务端则是要通过拦截器来分别处理两种 rpc 调用的验证\n// 流式的 验证 func ensureValidToken(ctx context.Context, req any, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (any, error) { md, ok := metadata.FromIncomingContext(ctx) if !ok { return nil, errMissingMetadata } // 下面这个是将客户端传的token 和服务器端的校验逻辑来比较 if !valid(md[\u0026#34;authorization\u0026#34;]) { return nil, errInvalidToken } // Continue execution of handler after ensuring a valid token. return handler(ctx, req) } cert, err := tls.LoadX509KeyPair(data.Path(\u0026#34;x509/server_cert.pem\u0026#34;), data.Path(\u0026#34;x509/server_key.pem\u0026#34;)) if err != nil { log.Fatalf(\u0026#34;failed to load key pair: %s\u0026#34;, err) } opts := []grpc.ServerOption{ grpc.UnaryInterceptor(ensureValidToken), // Enable TLS for all incoming connections. grpc.Creds(credentials.NewServerTLSFromCert(\u0026amp;cert)), } 这里也是可以用 go-grpc-middleware 提供的auth 中间件来实现验证函数的包装\n取消调用 取消调用里面要在用grpc 调用时传入上下文作为第一个参数来控制rpc 调用过程；\nctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) stream, err := c.BidirectionalStreamingEcho(ctx) if err != nil { log.Fatalf(\u0026#34;error creating stream: %v\u0026#34;, err) } cancel() // 此时已经取消任务了， 压缩请求 // 旧版本在 NewClient() 的时候传一个 grpc.WithCompressor(grpc.NewGZIPCompressor()) // 新版本需要在调用的时候传入 grpc.UseCompressor(gzip.Name) grpc限流 用go-grpc-middleware实现一个接口来在grpc 中间件里做限流，限流中间件必须排在后面，避免令牌被浪费了，使用原生的方式可以基于服务来做特定任务的限流； 在示例中用定时器触发模拟限流机制产生，当服务端调用阻塞的时候，退出后续的批量任务，\n请求失败重试策略配置 var retryPolicy = `{ \u0026#34;methodConfig\u0026#34;: [{ \u0026#34;name\u0026#34;: [{\u0026#34;service\u0026#34;: \u0026#34;grpc.examples.echo.Echo\u0026#34;}], //应用的服务 \u0026#34;waitForReady\u0026#34;: true,\t// 是否等待 \u0026#34;retryPolicy\u0026#34;: { \u0026#34;MaxAttempts\u0026#34;: 4, \u0026#34;InitialBackoff\u0026#34;: \u0026#34;.01s\u0026#34;, \u0026#34;MaxBackoff\u0026#34;: \u0026#34;.01s\u0026#34;, \u0026#34;BackoffMultiplier\u0026#34;: 1.0, \u0026#34;RetryableStatusCodes\u0026#34;: [ \u0026#34;UNAVAILABLE\u0026#34; ] } }]}` // use grpc.WithDefaultServiceConfig() to set service config func retryDial() (*grpc.ClientConn, error) { return grpc.NewClient(*addr, grpc.WithTransportCredentials(insecure.NewCredentials()), grpc.WithDefaultServiceConfig(retryPolicy)) } 等待对端恢复 // 在需要等待对端恢复服务的时候可以加入这个option grpc.WaitForReady(true) 携带元数据 这个元数据有点像 http 里面的 header 的作用，携带一些用于配置的的内容 客户端这边需要用\nmetadata.Pairs(\u0026#34;timestamp\u0026#34;, time.Now().Format(timestampFormat)) // 来添加组装键值对，两个字符串作为一组，转换成一个KV对，键值对的键可以有重复的 ctx := metadata.NewOutgoingContext(context.Background(), md) // 然后 封装成一个上下文通过 grpc 调用传过去 var header, trailer metadata.MD r, err := c.UnaryEcho(ctx, \u0026amp;pb.EchoRequest{Message: message}, grpc.Header(\u0026amp;header), grpc.Trailer(\u0026amp;trailer)) // 这个 header 和 trailer 是 服务端对这个元数据做交互\nmd, ok := metadata.FromIncomingContext(ctx) header := metadata.New(map[string]string{\u0026#34;location\u0026#34;: \u0026#34;MTV\u0026#34;, \u0026#34;timestamp\u0026#34;: time.Now().Format(timestampFormat)}) grpc.SendHeader(ctx, header) // 执行grpc 服务 // 下面逻辑要在defer 函数里面执行 trailer := metadata.Pairs(\u0026#34;timestamp\u0026#34;, time.Now().Format(timestampFormat)) grpc.SetTrailer(ctx, trailer) 感觉可以用来做rpc 调用的时延监控，或者调用前后状态的跟踪点\ngrpc 长连接保活 var kacp = keepalive.ClientParameters{ Time: 10 * time.Second, // send pings every 10 seconds if there is no activity Timeout: time.Second, // wait 1 second for ping ack before considering the connection dead PermitWithoutStream: true, // send pings even without active streams } // 新建客户端时带上这个 grpc.DialOption conn, err := grpc.NewClient(*addr, grpc.WithTransportCredentials(insecure.NewCredentials()), grpc.WithKeepaliveParams(kacp)) 负载平衡 默认的连接构建策略是 使用首个配置构建两件，如果需要使用负载平衡机制\n// 使用轮转策略 roundrobinConn, err := grpc.NewClient( fmt.Sprintf(\u0026#34;%s:///%s\u0026#34;, exampleScheme, exampleServiceName), grpc.WithDefaultServiceConfig(`{\u0026#34;loadBalancingConfig\u0026#34;: [{\u0026#34;round_robin\u0026#34;:{}}]}`), // This sets the initial balancing policy. grpc.WithTransportCredentials(insecure.NewCredentials()), ) ","permalink":"http://localhost:1313/zh/grpc/","tags":null,"title":""},{"categories":null,"contents":"llm 知识抽取管线 利用prompt模板，从文档中抽取出知识，并控制输出格式，完成知识抽取管线\nprompt模板 KE_PROMPT= \u0026#34;\u0026#34;\u0026#34;你现在是一个用于抽取结构化信息的知识抽取模型，请按照遵守下面的步骤提取结构化的实体关系: - 步骤 - 1. 识别所有在实体类型列表:{node_lists}中给出类型的实体，提取以下信息，同时保持实体一致性： - name 实体名称，尽量简单明确，不要包含多余信息； - type 实体类型，必须是实体类型列表中给出的类型； - desc 实体描述，可以实体属性和相关活动的描述； 将每个实体输出为json格式，其格式如下，键值内容不要包含单双引号，格式如下： {{\u0026#34;name\u0026#34;:\u0026#34;\u0026lt;实体名称\u0026gt;\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;\u0026lt;实体类型\u0026gt;\u0026#34;,\u0026#34;desc\u0026#34;:\u0026#34;\u0026lt;实体描述\u0026gt;\u0026#34;}}； 2. 针对步骤1中获取的实体，识别所有在关系类型列表:{relation_lists}中给出类型的关系，提取以下信息： - src 源实体的名称，即步骤1中标识的name - dst 目标实体的名称，即步骤1中标识 - rel 关系类型，必须是关系类型列表中给出的类型； - rel_desc 说明源实体和目的实体存在实体关系的原因 将每个关系输出转化成以下的json格式:格式如下: {{\u0026#34;src\u0026#34;:\u0026#34;\u0026lt;源实体名称\u0026gt;\u0026#34;,\u0026#34;dst\u0026#34;:\u0026#34;\u0026lt;目标实体名称\u0026gt;\u0026#34;,\u0026#34;rel\u0026#34;:\u0026#34;\u0026lt;关系类型\u0026gt;\u0026#34;,\u0026#34;rel_desc\u0026#34;:\u0026#34;\u0026lt;关系描述\u0026gt;\u0026#34;\u0026#34;}} 3. 请保证按照上述规则输出，不要输出其他内容。 - 真实数据 - ############# {text} ############# 输出:\u0026#34;\u0026#34;\u0026#34; def main(): node_lists = [\u0026#34;人物\u0026#34;, \u0026#34;地点\u0026#34;, \u0026#34;组织\u0026#34;,\u0026#34;事件\u0026#34;] relation_lists = [\u0026#34;位于\u0026#34;, \u0026#34;就职于\u0026#34;,\u0026#34;发生了\u0026#34;,\u0026#34;谈论\u0026#34;] test = \u0026#34;昨天实验室的牛师兄带着常师哥去面馆吃了八十八碗面，然后谈论面上项目的一些筹划，准备结合从所在的大数据实验室的重点方向挖掘项目创新点\u0026#34; print(KE_PROMPT.format(node_lists=node_lists,relation_lists=relation_lists,text=test)) if __name__ == \u0026#39;__main__\u0026#39;: main() openai 或者gpt 大模型http 非流式调用 这部分可以参照之前的那个chatgpt 桌面版调用不同厂商的sdk 接口，用来组装成流水线\n","permalink":"http://localhost:1313/zh/llmke/","tags":null,"title":""},{"categories":null,"contents":"一个gptapi 桌面套件 使用了pyside6 和qfluentwidget 做的界面，qfluent 主要是用了侧边导航栏，后期考虑去掉这个依赖进一步减少编译后执行文件大小。 chatdesk 桌面套件连接 功能 markdown 自动导出 国产api {千问，豆包，混元，千帆，DeepSeek，Kimi} 支持多轮对话，但是暂时没做单轮多轮的动态切换 支持api 厂商动态切换，左下角label 显示当前使用的api后端 work线程实现异步读取响应流，但是考虑到防误触重发的问题，读取完成前，发送按钮不能再次点击 支持静态模型切换，目前配置文件存于工作目录，比使用环境变量更安全些 实验特性，多api 同时响应回答，目前没对多轮会话做支持； 主页面动态增长对话框 这边没有完全考虑仿照qq 那种一左一右的对话布局，因为比较麻烦；主要是在Scrollarea可滚动区域里面，添加重写的文本框；文本框在内容更新和变动时会自动适应内容大小，固定宽度；文本框超出滚动区域可视高度时会调整整个滚动区域的最小高度，避免压缩文本框；\nTODO： 因为会根据滚动区域里面文本框的累积高度来调整滚动区域最小高度，避免压缩文本框，会在特定情况下发生侧边滚动条调变，后期可能考虑优化； 给模型切换设计更加灵活的触发方式； 支持历史记忆管理，增强多轮会话的关联性； 支持一些链模式，或者说对内容做多轮归纳增强；\n会话窗口 # 一个单线程的异步执行者 class MyThead(QThread): def __init__(self,parent): super().__init__(parent) self.queue =Queue(10) self.exist = False def addtask(self,fn): self.queue.put(fn) def stop(self): self.exist =True self.queue.put(lambda: logging.debug(\u0026#34;线程shutdown\u0026#34;) ) def run(self): while not self.exist: try: fn =self.queue.get(timeout=1) if fn is None: break if self.exist: break try: fn() except Exception as e: logging.debug(f\u0026#34;线程内部执行错误{e=}\u0026#34;) break except Empty: if self.exist: break continue # 简单的 对话界面窗口 class Flow(QWidget): def __init__(self): super().__init__() self.layout_ = QVBoxLayout(self) self.layout_.setAlignment(Qt.AlignmentFlag.AlignTop) self.setLayout(self.layout_) def addWidget(self,w:QWidget): self.layout_.addWidget(w) class TextArea(QTextEdit): def __init__(self,parent): super().__init__() self.setReadOnly(True) self.setVerticalScrollBarPolicy(Qt.ScrollBarPolicy.ScrollBarAlwaysOff) self.m_txtDefaultHeight = 50 self.setMaximumHeight(50) self.setMinimumHeight(50) self.top,self.bottom = self.contentsMargins().top(), self.contentsMargins().bottom() self.ctx = \u0026#34;\u0026#34; self.pa = parent # 隐藏时间戳，后续可能会用到类似的注释 self.ud = f\u0026#34;\u0026lt;!-- TS:{datetime.now().strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;)} --\u0026gt;\u0026#34; self.ds = f\u0026#34;{datetime.now().strftime(\u0026#39;%Y_%m_%d\u0026#39;)}\u0026#34; self.saved = False self.callback :Callable[[float],int] self.lastH = -1.0 def insertMarkdown(self,s:str): self.ctx +=s self.setMarkdown(self.ctx) self.checkResize() def regist(self,callback:Callable[[float],int] ): self.callback = callback # 适应窗口变动 @Slot() def checkResize(self): # print(\u0026#34;更新变动\u0026#34;) h = self.document().size().height() outer =-1 if h\u0026gt;max(self.lastH,outer): self.lastH = h outer =self.callback(h+self.top+self.bottom) if h\u0026gt;self.m_txtDefaultHeight: self.m_txtDefaultHeight = math.floor(h) self.setMaximumHeight(math.floor(h)) self.setMinimumHeight(math.floor(h)) rect = self.geometry() self.setGeometry(rect.x(),rect.y(),rect.width(),math.floor(h)+self.top+self.bottom) # 导出md 文件 @Slot() def export2File(self): if self.saved: return with open(f\u0026#34;./chart_{self.ds}.md\u0026#34;,mode=\u0026#34;a+\u0026#34;,encoding=\u0026#34;utf8\u0026#34;) as fw: size = self.ctx.split(\u0026#34;-----\u0026#34;,maxsplit=1) if len(size) \u0026gt;1 and len(size[1])\u0026gt;0: fw.write(self.ctx) fw.write(f\u0026#34;\\n{self.ud}\\n\u0026#34;) self.saved =True def wheelEvent(self,e :QWheelEvent): self.pa.wheelEvent(e) def resizeEvent(self, e: QResizeEvent) -\u0026gt; None: self.document().setTextWidth(self.viewport().width()) margins = self.contentsMargins() height = int(self.document().size().height() + margins.top() + margins.bottom()) self.setMaximumHeight(height) self.setMinimumHeight(height) class MyApp(QWidget): sig = Signal(str) def __init__(self, *args,**kwargs) -\u0026gt; None: super().__init__(*args,**kwargs) self.setWindowTitle(\u0026#34;PYSIDE6 demo\u0026#34;) self.id =0 self.gid =0 self.resize(800,450) self.m_end =0 self.set_ui() self.ctx =\u0026#34;\u0026#34; # 这部分还可以整理一下 def set_ui(self): self.scrollarea = SingleDirectionScrollArea() self.scrollarea.setWidgetResizable(True) self.scrollbar = self.scrollarea.verticalScrollBar() self.flow = Flow() self.create_txt() self.flow.addWidget(self.txt) self.flow.setMinimumHeight(self.scrollarea.size().height()) self.scrollarea.setWidget(self.flow) self.button = PrimaryPushButton(\u0026#34;发送\u0026#34;) self.inputtext = LineEdit() self.inputtext.setPlaceholderText(\u0026#34;请输入你的想法\u0026#34;) self.laout = QGridLayout(self) # self.appendWS([self.text,self.button,self.inputtext]) self.laout.addWidget(self.scrollarea,0,0,3,4) self.laout.addWidget(self.inputtext,4,0,1,3) self.laout.addWidget(self.button,4,3,1,1) self.button.clicked.connect(self.magic) self.agent = None self.worker = MyThead(self) self.setStyleSheet(\u0026#34;QPushButton{border-width:1px;border-radius:5px;background-color: #3366FF ;}\\ QLabel{border:3px solid black ;border-radius:5px;color:black}\u0026#34;) # 动态切换api 后端 def changeAgent(self,s): if s in Agents: if s==da: return self.agent = Agents[s] # 根据内容更新滚动条高度 def changeScrollBar(self, f:float): nexth =-1 if self.m_end +f \u0026gt;= self.flow.minimumHeight(): nexth = int(self.m_end+f)+50 self.flow.setMinimumHeight(nexth) self.scrollbar.setValue(self.scrollbar.maximum()) return nexth def create_txt(self): if self.gid\u0026gt;1: self.m_end =self.txt.geometry().y()+self.txt.geometry().height()+self.txt.top+self.txt.bottom self.sig.disconnect(self.txt.insertMarkdown) self.txt = TextArea(self) self.scrollbar.setValue(self.scrollbar.maximum()) self.txt.regist(self.changeScrollBar) self.flow.addWidget(self.txt) self.sig.connect(self.txt.insertMarkdown) def appendWS(self,l:list[QWidget]): for it in l: self.layout.addWidget(it) # 异步任务 def loopread(self,s:str): # 多api 响应，目前没做多轮兼容 if useALL: for k,a in Agents.items(): try: self.append(f\u0026#34;\\n\\n`{k}`: \\n\\n\u0026#34;) a.StreamCall(s,self.append) except Exception as e: logging.debug(f\u0026#34;多轮提问错误{e=}\u0026#34;) else: self.agent.StreamCall(s,self.append) # type: ignore self.button.setEnabled(True) if saveHistory: self.txt.export2File() def append(self,s): self.sig.emit(s) def showMessage(self,ty:str,s:str): TeachingTip.create( target=self.button, # type: ignore icon=InfoBarIcon.ERROR, title=ty, content=s, isClosable=True, tailPosition=TeachingTipTailPosition.BOTTOM_RIGHT, duration=2000, parent=self ) logging.debug(f\u0026#34;type={ty},{s}\u0026#34;) @Slot() def magic(self): if self.agent is None: # 延迟代理配置，使用户可以先通过界面生成配置文件后再重启后使用 if len(Agents)==0 or da == \u0026#34;\u0026#34;: self.showMessage(\u0026#34;Error:\u0026#34;,\u0026#34;请先在设置页面配置一个chatgpt api的接口设置\u0026#34;) else: self.agent = Agents[da] self.gid +=1 if self.gid\u0026gt;1: self.create_txt() if self.inputtext.text()!=\u0026#34;\u0026#34;: self.button.setDisabled(True) self.txt.insertMarkdown(f\u0026#34;#### Q: {self.inputtext.text()}\\n-----\\n**A:** \\n\u0026#34;) # 延迟启动异步线程 if not self.worker.isRunning(): self.worker.start() self.worker.addtask(lambda : self.loopread(self.inputtext.text())) # 用来处理线程关闭问题 def close(self) -\u0026gt; bool: self.worker.stop() self.worker.wait() self.worker.quit() self.worker.terminate() return super().close() def closeEvent(self, event) -\u0026gt; None: self.worker.stop() self.worker.wait() self.worker.quit() self.worker.terminate() event.accept() return super().closeEvent(event) api 后端 # 千问，Kimi,deepseek # 一直有ssl 验证的ca调用后面会排查一下怎么跳过 class openai: def __init__(self,a:auth) -\u0026gt; None: self.key =a.sk self.url =urlMap[a.type] self.client = OpenAI(api_key=self.key,base_url=self.url) self.history = [{\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;You are a helpful assistant.\u0026#39;}] self.model = ModelMap[a.type][0] self.isMulti = False def setModel(self,s:str): self.model = s def StreamCall(self,s:str,fn:Callable[[str],None]): if not self.isMulti: self.history = [] self.history.append({\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: f\u0026#39;{s}\u0026#39;}) cmp = self.client.chat.completions.create( model=self.model, messages=self.history, # type: ignore stream=True ) temp = \u0026#34;\u0026#34; for chunk in cmp: ctx = chunk.choices[0].delta.content try: if ctx and len(ctx)\u0026gt;0: fn(ctx) except Exception as e: logging.debug(f\u0026#34;streamcall {e=}\u0026#34;) if ctx: temp +=ctx if self.isMulti: self.history.append({\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: f\u0026#39;{temp}\u0026#39;}) # 百度千帆 class qf: def __init__(self,sk=None,ak=None) -\u0026gt; None: if sk==None or ak==None: raise ValueError(f\u0026#34;{sk=},{ak=} is invalid\u0026#34;) self.client = qianfan.ChatCompletion(ak=ak,sk=sk) self.model = ModelMap[\u0026#34;千帆\u0026#34;][0] self.history = [] self.isMulti =False def setModel(self,s:str): self.model = s def StreamCall(self,s:str,fn:Callable[[str],None]): temp = \u0026#34;\u0026#34; if not self.isMulti: self.history = [] self.history.append({\u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;,\u0026#34;content\u0026#34;:f\u0026#34;{s}\u0026#34;}) resp = self.client.do(model=self.model,messages=self.history,stream=True) for chunk in resp: ctx = chunk[\u0026#34;body\u0026#34;][\u0026#34;result\u0026#34;] try: if len(ctx)\u0026gt;0: fn(ctx) except Exception as e: showMessage(\u0026#34;错误\u0026#34;,f\u0026#34;{e=}\u0026#34;,None,None) temp +=ctx if self.isMulti: self.history.append({\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: f\u0026#39;{temp}\u0026#39;}) import json # 混元 from tencentcloud.common.common_client import CommonClient from tencentcloud.common import credential from tencentcloud.common.profile.client_profile import ClientProfile from tencentcloud.common.profile.http_profile import HttpProfile class hy: def __init__(self,sk=None,ak=None) -\u0026gt; None: if sk==None or ak==None: raise ValueError(f\u0026#34;{sk=},{ak=} is invalid\u0026#34;) self.model = ModelMap[\u0026#34;混元\u0026#34;][0] self.message = {\u0026#34;Messages\u0026#34;:[],\u0026#34;Stream\u0026#34;:True,\u0026#34;Model\u0026#34;:self.model} self.history =self.message[\u0026#34;Messages\u0026#34;] self.isMulti =False cred = credential.Credential(ak, sk) httpProfile = HttpProfile() httpProfile.endpoint = \u0026#34;hunyuan.tencentcloudapi.com\u0026#34; clientProfile = ClientProfile() clientProfile.httpProfile = httpProfile params = \u0026#34;{}\u0026#34;; self.client = CommonClient(\u0026#34;hunyuan\u0026#34;, \u0026#34;2023-09-01\u0026#34;, cred, \u0026#34;\u0026#34;, profile=clientProfile) def setModel(self,s:str): self.model = s self.message[\u0026#34;Model\u0026#34;] = self.model def StreamCall(self,s:str,fn:Callable[[str],None]): temp = \u0026#34;\u0026#34; if not self.isMulti: self.message[\u0026#34;Messages\u0026#34;] = [] self.message[\u0026#34;Messages\u0026#34;].append({\u0026#34;Role\u0026#34;:\u0026#34;user\u0026#34;,\u0026#34;Content\u0026#34;:f\u0026#34;{s}\u0026#34;}) resp = self.client.call_sse(\u0026#34;ChatCompletions\u0026#34;, self.message) for chunk in resp: ctx = json.loads(chunk[\u0026#34;data\u0026#34;])[\u0026#34;Choices\u0026#34;][0][\u0026#34;Delta\u0026#34;][\u0026#34;Content\u0026#34;] # ctx = chunk[\u0026#34;body\u0026#34;][\u0026#34;result\u0026#34;] try: if len(ctx)\u0026gt;0: fn(ctx) except Exception as e: showMessage(\u0026#34;错误\u0026#34;,f\u0026#34;{e=}\u0026#34;,None,None) temp +=ctx if self.isMulti: self.history.append({\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: f\u0026#39;{temp}\u0026#39;}) # 豆包的这个是比较恶心的，在ak，sk，上还要一个modelid 形式的key from volcenginesdkarkruntime import Ark class db: def __init__(self,sk=None,ak=None,model=None) -\u0026gt; None: if sk==None or ak==None or model==None: raise ValueError(f\u0026#34;{sk=},{ak=} is invalid\u0026#34;) self.client = Ark(ak=ak,sk=sk,base_url=\u0026#34;https://ark.cn-beijing.volces.com/api/v3\u0026#34;) self.model = model self.history = [] self.isMulti =False def setModel(self,s:str): self.model = s def StreamCall(self,s:str,fn:Callable[[str],None]): temp = \u0026#34;\u0026#34; if not self.isMulti: self.history = [] self.history.append({\u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;,\u0026#34;content\u0026#34;:f\u0026#34;{s}\u0026#34;}) resp: Stream[ChatCompletionChunk] = self.client.chat.completions.create(model=self.model,messages=self.history,stream=True) # type: ignore for chunk in resp: if not chunk.choices: continue ctx: str = chunk.choices[0].delta.content # type: ignore try: if len(ctx)\u0026gt;0: fn(ctx) except Exception as e: showMessage(\u0026#34;错误\u0026#34;,f\u0026#34;{e=}\u0026#34;,None,None) temp +=ctx if self.isMulti: self.history.append({\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: f\u0026#39;{temp}\u0026#39;}) 实现这个异步响应就是通过调用agent 接口的 streamcall ，通过传入的s 表示请求消息，fn表示每轮执行的回调函数，这里的ctx 之类的返回需要增加一个if ctx检查防止某些情况下拿到的对象是None 出现错误\n","permalink":"http://localhost:1313/zh/openai/","tags":null,"title":""},{"categories":null,"contents":"ebpf postgresql慢查询日志服务 因为需要使用用户控件探针来做绑定，需要可执行文件有符号信息，所以需要在编译pg 时启用 \u0026ndash;enable-debug\n用户态程序 这里主要是实现一个日志，可以通过systemd 来管理这个慢查询日志，目前没给这个程序做配置文件管理的部分，但是实现起来也是很简单的\n//go:build amd64 package main import ( \u0026#34;bytes\u0026#34; \u0026#34;encoding/binary\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/signal\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;github.com/cilium/ebpf/link\u0026#34; \u0026#34;github.com/cilium/ebpf/perf\u0026#34; \u0026#34;github.com/cilium/ebpf/rlimit\u0026#34; \u0026#34;golang.org/x/sys/unix\u0026#34; ) // 这个是bpf2go 的生成代码 //go:generate go run github.com/cilium/ebpf/cmd/bpf2go -cc clang -target amd64 -type event bpf dbstats.c -- -I../headers const ( symbol = \u0026#34;exec_simple_query\u0026#34; ) var binPath string var logPath string var logFile *os.File var pid int var slow int func init() { flag.StringVar(\u0026amp;binPath, \u0026#34;P\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;the path of postgres\u0026#34;) flag.IntVar(\u0026amp;pid, \u0026#34;p\u0026#34;, 0, \u0026#34;pid\u0026#34;) flag.IntVar(\u0026amp;slow, \u0026#34;t\u0026#34;, 200, \u0026#34;the threshold of slow query\u0026#34;) flag.StringVar(\u0026amp;logPath, \u0026#34;l\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;the log output file path\u0026#34;) } func Config() bool { flag.Parse() if binPath == \u0026#34;\u0026#34; \u0026amp;\u0026amp; pid == 0 { fmt.Printf(\u0026#34;invalid argument path [%s] pid %d\u0026#34;, binPath, pid) return true } if binPath == \u0026#34;\u0026#34; { if path, err := filepath.EvalSymlinks(fmt.Sprintf(\u0026#34;/proc/%d/exe\u0026#34;, pid)); err != nil { fmt.Printf(\u0026#34;pid %d err %v\u0026#34;, pid, err) return true } else { binPath = path } } if logPath != \u0026#34;\u0026#34; { path, err := filepath.Abs(logPath) if err != nil { fmt.Printf(\u0026#34;get abs path failed %v\\n\u0026#34;, path) } if file, err := os.OpenFile(path, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666); err != nil { fmt.Printf(\u0026#34;open log file failed %v\\n\u0026#34;, err) } else { logFile = file log.SetOutput(file) } } if slow \u0026lt; 0 || slow \u0026gt; 2000 { return true } return false } func main() { shouldReturn := Config() if shouldReturn { return } defer func(file *os.File) { if file != nil { file.Close() } }(logFile) stopper := make(chan os.Signal, 1) signal.Notify(stopper, os.Interrupt, syscall.SIGTERM) // Allow the current process to lock memory for eBPF resources. if err := rlimit.RemoveMemlock(); err != nil { log.Fatal(err) } // Load pre-compiled programs and maps into the kernel. objs := bpfObjects{} if err := loadBpfObjects(\u0026amp;objs, nil); err != nil { log.Fatalf(\u0026#34;loading objects: [%v]\u0026#34;, err) } defer objs.Close() // 200ms 是慢查询上限 objs.SlowMap.Put(0, slow) // Open an ELF binary and read its symbols. ex, err := link.OpenExecutable(binPath) if err != nil { log.Fatalf(\u0026#34;opening executable: %s\u0026#34;, err) } // Open a Uretprobe at the exit point of the symbol and attach // the pre-compiled eBPF program to it. up, err := ex.Uretprobe(symbol, objs.UretprobeExecSimpleQuery, nil) if err != nil { log.Fatalf(\u0026#34;creating uretprobe: %s\u0026#34;, err) } defer up.Close() do, err := ex.Uprobe(symbol, objs.UprobeExecSimpleQuery, nil) if err != nil { log.Fatalf(\u0026#34;creating uprobe: %s\u0026#34;, err) } defer do.Close() // Open a perf event reader from userspace on the PERF_EVENT_ARRAY map // described in the eBPF C program. rd, err := perf.NewReader(objs.Events, os.Getpagesize()) if err != nil { log.Fatalf(\u0026#34;creating perf event reader: %s\u0026#34;, err) } defer rd.Close() go func() { // Wait for a signal and close the perf reader, // which will interrupt rd.Read() and make the program exit. \u0026lt;-stopper log.Println(\u0026#34;Received signal, exiting program..\u0026#34;) if err := rd.Close(); err != nil { log.Fatalf(\u0026#34;closing perf event reader: %s\u0026#34;, err) } }() log.Printf(\u0026#34;Listening for events..\u0026#34;) // bpfEvent is generated by bpf2go. var event bpfEvent for { record, err := rd.Read() if err != nil { if errors.Is(err, perf.ErrClosed) { return } log.Printf(\u0026#34;reading from perf event reader: %s\u0026#34;, err) continue } if record.LostSamples != 0 { log.Printf(\u0026#34;perf event ring buffer full, dropped %d samples\u0026#34;, record.LostSamples) continue } // Parse the perf event entry into a bpfEvent structure. if err := binary.Read(bytes.NewBuffer(record.RawSample), binary.LittleEndian, \u0026amp;event); err != nil { log.Printf(\u0026#34;parsing perf event: %s\u0026#34;, err) continue } log.Printf(\u0026#34;%s:%f/ms\\n\u0026#34;, unix.ByteSliceToString(event.Cmd[:]), float64(event.Timestamp)/1000000.0) } } ebpf uprobe 这部分代码主要是对命令的拷贝和exec_simple_query函数执行的计时\n//go:build ignore #include \u0026#34;common.h\u0026#34; #include \u0026#34;bpf_tracing.h\u0026#34; char __license[] SEC(\u0026#34;license\u0026#34;) = \u0026#34;Dual MIT/GPL\u0026#34;; #define MAX_DATA_LEN 256 u32 slowThreshold=0; const u32 timebase = 1000000; struct event { u64 pid; u64 timestamp; u8 cmd[MAX_DATA_LEN]; }; struct bpf_map_def SEC(\u0026#34;maps\u0026#34;) slow_map = { .type = BPF_MAP_TYPE_ARRAY, .key_size = sizeof(u32), .value_size = sizeof(u32), .max_entries = 1, }; struct bpf_map_def SEC(\u0026#34;maps\u0026#34;) kprobe_map = { .type = BPF_MAP_TYPE_HASH, .key_size = sizeof(u64), .value_size = sizeof(struct event), .max_entries = 256, }; struct { __uint(type, BPF_MAP_TYPE_PERF_EVENT_ARRAY); } events SEC(\u0026#34;.maps\u0026#34;); // Force emitting struct event into the ELF. const struct event *unused __attribute__((unused)); SEC(\u0026#34;uretprobe/exec_simple_query\u0026#34;) int uretprobe_exec_simple_query(struct pt_regs *ctx) { u64 pid = bpf_get_current_pid_tgid()\u0026gt;\u0026gt;32; struct event * valp = bpf_map_lookup_elem(\u0026amp;kprobe_map, \u0026amp;pid); if (valp) { valp-\u0026gt;timestamp = bpf_ktime_get_ns()- (valp-\u0026gt;timestamp); if(slowThreshold\u0026gt;0||valp-\u0026gt;timestamp\u0026gt;slowThreshold){ bpf_perf_event_output(ctx, \u0026amp;events, BPF_F_CURRENT_CPU, valp, sizeof(struct event)); } bpf_map_delete_elem(\u0026amp;kprobe_map, \u0026amp;pid); return 0; } return 0; } SEC(\u0026#34;uprobe/exec_simple_query\u0026#34;) int uprobe_exec_simple_query(struct pt_regs *ctx) { if(slowThreshold==0){ int key =0; u32 * ret = (u32*)bpf_map_lookup_elem(\u0026amp;slow_map, \u0026amp;key); if(ret) slowThreshold = *ret*timebase; } struct event event; u64 pid = bpf_get_current_pid_tgid()\u0026gt;\u0026gt;32; u64 ts = bpf_ktime_get_ns(); event.pid = pid; event.timestamp = ts; char * sql_string = (char*)PT_REGS_PARM1(ctx); bpf_probe_read(\u0026amp;event.cmd,sizeof(event.cmd),sql_string); bpf_map_update_elem(\u0026amp;kprobe_map,\u0026amp;pid,\u0026amp;event,BPF_ANY); return 0; } 这里因为ebp-go 没有从go 程序处修改ebpf 中全局变量的接口，这里是通过一个map array 来间接传递这个慢查询阈值，最大支持2s 的慢查询上限，要是觉得这个上限比较低，可以将数据类型改成 u64，这里理论上只会在初次触发时进行全局变量的赋值，后续使用全局变量做快速路径\n","permalink":"http://localhost:1313/zh/pgslow/","tags":null,"title":""},{"categories":null,"contents":"一个range 风格的范围迭代器封装实现 go 里面有个range 的表达式可以遍历很多容器，最近应该是1.21支持range int,这看起来很像 python的 range 了，很舒服，想着很久没用cpp 写东西了就准备按照迭代器整一个，做小点，也不用什么模板；\n第一种宏定义 这种其实类似 哪些OIer 常用的 for_each 宏\n#define rangeI(_I, _end) for (i = 0; i \u0026lt; (_end); i++) 其实就是一个包装常见的 for 表达式头的一个宏，限制挺多，优点是几乎零开销；\n迭代器类封装实现 这个东西实现上并未考虑太多性能,应该会需要创建两个对象，这个有点像cpp 20里面增加的，ranges::itoa_views()；但是这个功能更单一，仅用于整型范围迭代，而且接口也是更使用，提供，start，end，step，支持for range双向遍历\nclass RangeIter { private: int step; int cur_val; public: RangeIter(int step, int val) : step(step), cur_val(val) {} // these three methods form the basis of an iterator for use with a rangeIter-based for loop bool operator!=(const RangeIter \u0026amp;other) const { if (step \u0026lt; 0) { return cur_val != other.cur_val \u0026amp;\u0026amp; cur_val \u0026gt; other.cur_val; } return cur_val != other.cur_val \u0026amp;\u0026amp; cur_val \u0026lt; other.cur_val; } // this method must be defined after the definition of IntVector since it needs to use it int operator*() const { return cur_val; } const RangeIter \u0026amp;operator++() { cur_val += step; return *this; } }; class Range { private: int _st, _end, _step; bool args_check() const { if (_step == 0) { return false; } if (_step \u0026lt; 0 \u0026amp;\u0026amp; _st \u0026lt;= _end) { return false; } return _step \u0026lt;= 0 || _st \u0026lt; _end; } public: Range(int start, int end, int step) : _st(start), _end(end), _step(step) {} Range(int start, int end) : _st(start), _end(end), _step(1) {} explicit Range(int end) : _st(0), _end(end), _step(1) {} RangeIter iter() { return RangeIter{_step, _st}; } RangeIter cbegin() const { if (!args_check()) { throw std::invalid_argument(\u0026#34;step should match the exper start+n*step\u0026gt;end\u0026#34;); } return RangeIter{_step, _st}; } RangeIter cend() const { // if (!args_check()){ // throw std::invalid_argument(\u0026#34;step should match the exper start+n*step\u0026gt;end\u0026#34;); // } return RangeIter{_step, _end}; } RangeIter begin() { if (!args_check()) { throw std::invalid_argument(\u0026#34;step should match the exper start+n*step\u0026gt;end\u0026#34;); } return RangeIter{_step, _st}; } RangeIter end() { return RangeIter{_step, _end}; } }; 这里对 begin 做了参数检查，避免意外的错误情况，导致无限循环之类的情况，其实正常情况应该是把RangeIter 这个迭代器实现给塞到 private 域里面去，避免他人骚操作；但这里只是一个简单的演示demo就不用考虑太多；\n调用 int main() { std::vector\u0026lt;int\u0026gt; val{1, 2, 3}; int i = 0; rangeI(i, val.size()) { std::cout \u0026lt;\u0026lt; val[i] \u0026lt;\u0026lt; std::endl; } // 1 // 2 // 3 try { for (auto i : Range(static_cast\u0026lt;int\u0026gt;(val.size()), 0, -1)) { std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;;\\n\u0026#34;; } } catch (std::logic_error \u0026amp;e) { std::cout \u0026lt;\u0026lt; \u0026#34;error\u0026#34; \u0026lt;\u0026lt; e.what() \u0026lt;\u0026lt; std::endl; } // 3; // 2; // 1; std::cout \u0026lt;\u0026lt; std::endl; } 总之复习了for range和迭代器实现\n","permalink":"http://localhost:1313/zh/range/","tags":null,"title":""},{"categories":null,"contents":"雪花算法实现 雪花算法 0 - 0000000000 0000000000 0000000000 0000000000 0 - 0000000000 - 000000000000\n符号位 时间戳 机器码 序列号\n41位毫秒级时间戳，10位机器id 12位序列号 这个起始时间sEpoch其实不太重要，因为一般业务不会超过10年，这个69-(10~20)总还有50年的余量的，比较重要的一点就是如果一个毫秒内生产的序列id 过多就得等待到下一个毫秒窗口内\n代码 package utils // https://sourcegraph.com/github.com/sohaha/zlsgo/-/blob/zstring/snowflake.go // 学习 zlsgo的总结 import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) const ( sEpoch = 1474802888000 // 这个可以是业务上线的时间 TimeBase = int64(1000000) MaxWorkID = 1\u0026lt;\u0026lt;5 - 1 MaxDataID = MaxWorkID MaxSeqID = 1\u0026lt;\u0026lt;12 - 1 MaskWorkID = 1\u0026lt;\u0026lt;10 - 1 MaskSeqID = 1\u0026lt;\u0026lt;12 - 1 ) type SnowFlakeBuilder struct { start int64 workid int64 seqid int64 lock sync.Mutex } func NewBuilder(st time.Time, data int, worker int) *SnowFlakeBuilder { if data \u0026lt; 0 || data \u0026gt; MaxDataID || worker \u0026lt; 0 || worker \u0026gt; MaxWorkID || st.After(time.Now()) { panic(\u0026#34;invaild argument error\u0026#34;) } return \u0026amp;SnowFlakeBuilder{ start: st.UnixNano() / TimeBase, workid: int64(data\u0026lt;\u0026lt;5|worker) \u0026amp; MaskWorkID, } } // 类似自旋锁的逻辑，持续试图获取下一毫秒的时间资源 func (s *SnowFlakeBuilder) waitToNextMS(last int64) int64 { ts := time.Now().UnixNano() / TimeBase for { if ts \u0026lt;= last { ts = time.Now().UnixNano() / TimeBase } else { break } } return ts } func (s *SnowFlakeBuilder) GetID() (int64, error) { s.lock.Lock() defer s.lock.Unlock() ts := time.Now().UnixNano() / TimeBase if ts == s.start { s.seqid = (s.seqid + 1) \u0026amp; MaskSeqID if s.seqid == 0 { ts = s.waitToNextMS(ts) } } else { s.seqid = 0 } if ts \u0026lt; s.start { return 0, fmt.Errorf(\u0026#34;clock moved backwards, refuse gen id\u0026#34;) } s.start = ts ts = (ts-sEpoch)\u0026lt;\u0026lt;22 | s.workid\u0026lt;\u0026lt;12 | s.seqid return ts, nil } ","permalink":"http://localhost:1313/zh/snowflake/","tags":null,"title":""},{"categories":null,"contents":"消息订阅实现 原理 go 的消息管道可以看成一个并发安全的队列，每个订阅者将字节的收信队列添加到SubHub 里面，按照订阅的topic 和 channel 的关系用一个 map[string]chan 来实现关联，当Hub 接受到对应topic 的消息推送的时候，会给slice 里面的收信队列发事件消息\n实现 import ( \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) type HEvent struct { Data interface{} Topic string } type HEventData chan HEvent type HEventDataArray []HEventData //一个topic 可以有多个消费者 type HEventBus struct { sub map[string]HEventDataArray rm sync.RWMutex } func HEventSrv() *HEventBus { return h } func (h *HEventBus) Sub(topic string, ch HEventData) { h.rm.Lock() if chanEvent, ok := h.sub[topic]; ok { h.sub[topic] = append(chanEvent, ch) } else { h.sub[topic] = append([]HEventData{}, ch) } defer h.rm.Unlock() } func (h *HEventBus) Push(topic string, data interface{}) { h.rm.RLock() defer h.rm.RUnlock() if chanEvent, ok := h.sub[topic]; ok { for _, ch := range chanEvent { ch \u0026lt;- HEvent{ Data: data, Topic: topic, } } } } func (h *HEventBus) PushFullDrop(topic string, data interface{}) { h.rm.RLock() defer h.rm.RUnlock() if chanEvent, ok := h.sub[topic]; ok { for _, ch := range chanEvent { select { case ch \u0026lt;- HEvent{ Data: data, Topic: topic, }: case \u0026lt;-time.After(time.Second): } } } } ","permalink":"http://localhost:1313/zh/subpush/","tags":null,"title":""},{"categories":null,"contents":"TailReader 一个反向迭代器 面对大文本获取最后的消息，向前遍历go 目前没有现成的接口\n设计思路 seek 反向移动offset ，然后bytes 判断不同系统上的换行符在哪里？ 将一次分割后的字节数组，缓存起来，留待下次取分行字节 在多次未能查看到换行符的时候，默认是3KB 就提前终止提交失败，避免因为错误遍历大二进制文件 有缓存机制，大小文件效率都不差，适合做类似 tail -n 3 这种文本获取行为 目前支持的 empty 行跳过行为比较简单，其实更接近于剔除前缀换行符, 如果需要跳过空行，可以嵌套个if 判断len(temp)\u0026gt;0 面对极小文件可以直接用ReadFile 来切split，避免带来额外的复杂度 实现细节 type TailReader struct { rc *os.File buf []byte // 用来缓存剩余字节 temp []byte // 提供给 Read sep []byte // 兼容不同系统架构分隔符 offset int64 // 记录offset size int64 // 文件大小 skipempty bool // 控制是否跳过空行行为 atEnd bool // 记录offset 是否被移动到文件开始位置了 } var ( Sep_win = []byte(\u0026#34;\\r\\n\u0026#34;) Sep_linux = []byte(\u0026#34;\\n\u0026#34;) ) func NewTailReader(fname string, sep []byte, skip bool) (*TailReader, error) { file, err := os.Open(fname) if err != nil { return nil, err } stat, _ := file.Stat() size := stat.Size() var offset int64 = 1024 if size \u0026lt; offset { offset = size } _, errs := file.Seek(int64(-offset), 2) if errs != nil { return nil, errs } offset2, _ := file.Seek(0, io.SeekCurrent) fmt.Printf(\u0026#34;seek to offset %d, file size is %d\\n\u0026#34;, offset2, size) atEnd := false if offset == size { atEnd = true } return \u0026amp;TailReader{rc: file, buf: make([]byte, 0, 1024), temp: make([]byte, 1024), sep: sep, skipempty: skip, offset: int64(offset), size: size, atEnd: atEnd}, nil } func (t *TailReader) Close() { t.rc.Close() } func (t *TailReader) ReadBytes() ([]byte, error) { // 如果上次缓存没清完，检查是否有换行符 sepsize := 0 if t.skipempty { sepsize = len(t.sep) } // 处理上次遗留的缓存 if len(t.buf) \u0026gt; 0 { if idx := bytes.LastIndex(t.buf, t.sep); idx != -1 { temp := append([]byte{}, t.buf[idx+sepsize:]...) t.buf = t.buf[:idx] return temp, nil } if t.atEnd { p := slices.Clone(t.buf[:len(t.buf)]) t.buf = t.buf[:0] return p, nil } } if t.size \u0026lt; t.offset { return nil, io.EOF } // 拷贝重置缓存 var p []byte // 先将这部分尾巴给卸除出去 if len(t.buf) \u0026gt; 0 { p = append([]byte{}, t.buf...) } n, err := t.rc.Read(t.temp) if err == nil \u0026amp;\u0026amp; n \u0026gt; 0 { idx := bytes.LastIndex(t.temp[:n], t.sep) if idx != -1 { temp := append([]byte{}, t.temp[idx+sepsize:n]...) temp = append(temp, p...) t.buf = t.buf[:0] t.buf = append(t.buf, t.temp[:idx]...) if err := t.move(n); err != nil { return nil, err } return temp, nil } var cur, next []byte cur = slices.Concat(t.temp[:n], p) if err := t.move(n); err != nil { return nil, err } // 用来预防二进制大文件，堆爆slice for i := 0; i \u0026lt; 3 \u0026amp;\u0026amp; idx == -1; i++ { n, err = t.rc.ReadAt(t.buf, 0) if err != nil { return nil, err } if err := t.move(n); err != nil { return nil, err } idx = bytes.LastIndex(t.buf[:n], t.sep) if idx != -1 { next = slices.Concat(t.temp[idx:n], cur) // 尽量复用 t.buf = t.buf[:0] t.buf = append(t.buf, t.temp[:idx]...) break } next = slices.Concat(t.temp[:n], cur) } if idx != -1 { return nil, errors.New(\u0026#34;cant found sep in many times try\u0026#34;) } return next, nil } return nil, err } func (t *TailReader) move(delta int) error { t.offset += int64(delta) if t.offset \u0026gt; t.size { t.offset = t.size } // 避免重复移动 if t.offset \u0026lt;= t.size \u0026amp;\u0026amp; !t.atEnd { _, err := t.rc.Seek(-t.offset, 2) if err != nil { return err } if t.offset == t.size { t.atEnd = true } } if len(t.buf) \u0026gt; 0 { return nil } return io.EOF } 简单使用示例 f, err := NewTailReader(`test.txt`, utils.Sep_win, true) if err != nil { fmt.Printf(\u0026#34;error %s\u0026#34;, err.Error()) } defer f.Close() for { b, e := f.ReadBytes() // 一般是 io.EOF if e != nil { break } fmt.Printf(\u0026#34;%s\\t%v\\n\u0026#34;, string(b), e) } 1\n","permalink":"http://localhost:1313/zh/tailreader/","tags":null,"title":""},{"categories":null,"contents":"TLS 和 H2 配置 TLS 证书签发流程 # 下面是 v3.ext 的内容 # authorityKeyIdentifier=keyid,issuer # basicConstraints=CA:FALSE # keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment # extendedKeyUsage = serverAuth # subjectAltName = @alt_names # [alt_names] # DNS.1=localhost # DNS.2=10.110.8.36 openssl genrsa \u0026gt; serve.key openssl req -new -key serve.key -out serve.csr -subj \u0026#34;/C=GB/L=China/O=hx/CN=localhost\u0026#34; -days 365 -addext \u0026#34;subjectAltName = DNS:localhost\u0026#34; # 个人感觉 这边加个 -addext \u0026#34;subjectAltName = DNS:localhost\u0026#34; 也可以替代 extfile openssl x509 -req -sha512 -days 365 -extfile v3.ext -signkey serve.key -in serve.csr -out serve.crt 三个文件：\nserve.key 私钥 serve.csr 公钥 serve.crt 给客户端用来验证服务的签名证书 代码实现TLS 和H2 服务端这边\nhttp.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, req *http.Request) { io.WriteString(w, \u0026#34;hello, world!\\n\u0026#34;) }) // 这里要指向两个文件的路径 if e := http.ListenAndServeTLS(\u0026#34;:443\u0026#34;, \u0026#34;serve.crt\u0026#34;, \u0026#34;serve.key\u0026#34;, nil); e != nil { log.Fatal(\u0026#34;ListenAndServe: \u0026#34;, e) } 客户端这边\npackage main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;golang.org/x/net/http2\u0026#34; ) func loadCA(caFile string) *x509.CertPool { pool := x509.NewCertPool() if ca, e := os.ReadFile(caFile); e != nil { log.Fatal(\u0026#34;ReadFile: \u0026#34;, e) } else { pool.AppendCertsFromPEM(ca) } return pool } func main() { c := \u0026amp;http.Client{ // 这个会拿到 h2的协议 Transport: \u0026amp;http2.Transport{ TLSClientConfig: \u0026amp;tls.Config{RootCAs: loadCA(\u0026#34;../serve.crt\u0026#34;)}, AllowHTTP: true, }, // 启用下面这个注释的就是 https 的协议 // Transport: \u0026amp;http.Transport{ // TLSClientConfig: \u0026amp;tls.Config{RootCAs: loadCA(\u0026#34;../serve.crt\u0026#34;)}, // }, } if resp, e := c.Get(\u0026#34;https://localhost:443/\u0026#34;); e != nil { log.Fatal(\u0026#34;http.Client.Get: \u0026#34;, e) } else { defer resp.Body.Close() io.Copy(os.Stdout, resp.Body) fmt.Printf(\u0026#34;Got response %d: %s \\n\u0026#34;, resp.StatusCode, resp.Proto) } } ","permalink":"http://localhost:1313/zh/tls_h2/","tags":null,"title":""},{"categories":null,"contents":"矢量化扩展 #include \u0026lt;array\u0026gt; #include \u0026lt;chrono\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026#34;boost/core/noncopyable.hpp\u0026#34; using std::chrono::duration_cast; using std::chrono::high_resolution_clock; class BenchTimer : boost::noncopyable { high_resolution_clock::time_point tp; std::string name; std::vector\u0026lt;std::string\u0026gt; rd; std::vector\u0026lt;double\u0026gt; dt; public: inline void st(const std::string \u0026amp;name) { this-\u0026gt;name = name; tp = high_resolution_clock::now(); } inline void end() { dt.emplace_back(duration_cast\u0026lt;std::chrono::nanoseconds\u0026gt;(high_resolution_clock::now() - tp).count()); rd.emplace_back(this-\u0026gt;name); } void showBenchTest() { for (int i = 0, size = static_cast\u0026lt;int\u0026gt;(rd.size()); i \u0026lt; size; i++) { std::cout \u0026lt;\u0026lt; rd[i] \u0026lt;\u0026lt; \u0026#34;:\u0026#34; \u0026lt;\u0026lt; dt[i] / 1000 \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } } }; int main() { std::vector\u0026lt;int\u0026gt; arr_a(256, 0); std::vector\u0026lt;int\u0026gt; arr_b(256, 100); typedef int int256 __attribute__((vector_size(256 * sizeof(int)))); int256 a; int256 b; for (int i = 0; i \u0026lt; 256; i++) { arr_a[i] = i; a[i] = i; arr_b[i] = 256 - i; b[i] = 256 - i; } int t = 10000; auto bench = BenchTimer(); #pragma clang optimize off bench.st(\u0026#34;foreach\u0026#34;); while (t \u0026gt; 0) { for (auto i = 0; i \u0026lt; 256; i++) { arr_a[i] = 2 * arr_b[i]; } t--; } bench.end(); t = 10000; bench.st(\u0026#34;vec\u0026#34;); while (t \u0026gt; 0) { a = 2 * b; t--; } bench.end(); #pragma clang optimize on bench.showBenchTest(); std::cout \u0026lt;\u0026lt; \u0026#34;2*b[0]==\u0026#34; \u0026lt;\u0026lt; a[0] \u0026lt;\u0026lt; std::endl; } bench测试结果 foreach:673.4 vec:4.8 2*b[0]==512\n按照600/5 也有120倍的加速差距了，据我观察这种大轮次的外部循环可能对vec 的缓存hit更友好点，总之是有优化效果的\n","permalink":"http://localhost:1313/zh/vectorizer/","tags":null,"title":""}]