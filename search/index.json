[{"content":"lumberjack 日志库组件权限陷阱 之前一直使用go zero 的logx 组件来创建一个滚动日志，对filebeat 日志收集容器兼容性很好；近期在换成zero log+ lumberjack 的这套方案时用，和filebeat 的边车模式产生了冲突；\n现象 filebeat 服务告警，指向的日志文件权限不可访问；使用webssh 上去查看时发现ls -al 看到的权限是\u0026quot;0600\u0026quot;，也就是说不支持其他用户组的服务去访问，于是检查代码，发现并未手动创建日志，赋予0600文件权限，所以问题是lumberjack组件内部的默认文件权限是0600。\n那到底是哪部分导致的？ 源码\n通过翻看上述部分源码，确认是默认给的权限就是0600，搜索issue 发现自定义文件权限里面提到：\nLumberjack will copy the permissions of the existing file when it creates the new file. See here: https://github.com/natefinch/lumberjack/blob/v3/lumberjack.go#L267\nSo if you want different file permissions, the easiest thing to do is create the file with the permissions you want, first.\n另外附加源头pr 链接：https://github.com/natefinch/lumberjack/pull/112, 这里做出这个变更的原因据称是为了审计要求，收紧默认权限。十分甚至一百分的不优雅，加个Options 让使用者自己选不是比默认0600好很多？\n所以处理策略就是在使用前，先创建一个644权限的日志文件，然后关闭,再将该文件名交给lumberjack 组件。这样lumberjack会复制原始文件的权限，创建新的日志文件，避免边车容器因为用户组不同导致600无法访问文件的事情；\n处理 简单的加三行代码\n1 2 3 if fp, err := os.OpenFile(fileName, os.O_WRONLY|os.O_APPEND|os.O_CREATE, 0644); err == nil { fp.Close() } 总结 使用不熟悉的库遇到问题的时候，光翻找官方教程和在线博客可能是不够的，需要结合源码和issue 去排查。\n","date":"2025-07-20T15:09:23+08:00","image":"https://picsum.photos/seed/f2c285b0/800/600","permalink":"https://daidaij.github.io/p/lumberjack/","title":"Fixlog"},{"content":"视频流切片服务设计 这里记录下终端录制视频流切片并上传的流程设计与优化过程： 1. ffmpeg -i rtmp://xxxxx:1935/yyyy/zzzz/vision -c:v h264 -flags +cgop -g 0 -hls_flags program_date_time -hls_list_size 0 -hls_time 10 /app/data/record.m3u8 -y\n2. ffmpeg -hide_banner -f concat -safe 0 -i files.txt -ss 00:00:18 -t 00:00:40 -c copy -hls_list_size 0 ./output.m3u8\n这里是实现最原始切片方案的核心逻辑，下面会解释流程，然后列出自己的优化过程。\n最简原型 录制逻辑 ffmpeg -i rtmp://xxxxx:1935/yyyy/zzzz/vision -c:v h264 -flags +cgop -g 0 -hls_flags program_date_time -hls_list_size 0 -hls_time 10 /app/data/record.m3u8 -y\n这里从rtmp://xxxxx:1935/yyyy/zzzz/vision 视频流获取 然后添加：\n-hls_flags program_date_time 使用#EXT-X-PROGRAM-DATE-TIME时间扩展 ,在m3u8 文件中记录每个切片的开始时间，重要 -hls_list_size 0 配置 m3u8 为记录所有ts 文件 -c:v h264 确保格式为h264 切片逻辑 切片最重要的是解析m3u8 文件，判断请求的时间是否在文件记录的范围内，是的话将其加入到concat 文件中，然后更新ss 也就是切片开始时间，最后执行下述切片指令; ffmpeg -hide_banner -f concat -safe 0 -i files.txt -ss 00:00:18 -t 00:00:40 -c copy -hls_list_size 0 ./output.m3u8 -i files.txt 输入concat 文件，这里是切片的核心逻辑 -ss 00:00:18 -t 00:00:40 切片起始时间和持续时间 -c copy 不做编码，直接复制 -hls_list_size 0 配置 m3u8 为记录所有ts 文件 上传逻辑 使用tar 对目标目录下所有非tar 后缀文件进行归档； 创建分片上传请求，拿到上传id 使用上传id 去分片上传大文件，设置有效时间 确认上传成功后主动清理 目标目录 优化 上述实现流程中存在以下问题:\n切片太慢，多个视角有放大效应 录制太占用资源，观察到最极端场景 113% cpu 占用 top 录制逻辑优化 首先在测试需要录制的视频流格式，获得以下信息：\nMetadata:\n|RtmpSampleAccess: true\nfileSize : 0\ntitle : Streamed by ZLMediaKit(git hash:f3026f5/2025-01-12T10:16:45+08:00,branch:master,build time:2025-01-12T02:24:04)\nDuration: 00:00:00.00, start: 2196.413000, bitrate: N/A\nStream #0:0: Data: none\nStream #0:1: Video: h264 (Constrained Baseline), yuv420p(progressive), 768x432, 10 fps, 10 tbr, 1k tbn\nStream #0:2: Audio: aac (LC), 8000 Hz, mono, fltp\n可以看到视频编码格式是 H264 ，和切片的编码相同，使用 -c copy 直接复制即可。 上下分别是修改前后的资源占用情况 TOP 最低也节省了40%*4 的cpu 资源 这里还可以利用hls_warp 来实现滚动录制，但是得需要手动拷贝到目标目录然后重建m3u8,长期录制需要去做这个，另外这里的优化为后面的切片逻辑让出了余裕。\n切片逻辑优化 切片主要是有以下几个问题：\n会随着 -ss 开始切片的值，导致切片耗时增长 多个视角使用串行一次完成录制 针对问题1，可以检索到ffmpeg 命令行文档中记录\n-ss position (input/output)\nWhen used as an input option (before -i), seeks in this input file to position. Note that in most formats it is not possible to seek exactly, so ffmpeg will seek to the closest seek point before position. 当使用-ss 时，会先seek 到指定位置，（如果可能的话），然后开始解码\n应对策略，在需要切片的视频只命中了一个M3U8文件，去掉合并逻辑，直接从m3u8 切片，使用-ss 优化 切片耗时：\n在-i 之前使用 -ss 可以利用seek 操作，快速定位，提升切片效率，测试后观察切片时间平均减半，更正优化策略，通过跟踪观察对HLS类型的录制，直接使用-ss 无法实现预先中的seek 定位，需要手动从原始M3U8 中切出子播放列表，是的可以用 -ss 0 来从子切片的位置开头的位置处直接提取，优化效果1分钟的切片可以在秒级完成，可以满足合规要求\n1 2 2025-07-16T14:45:18+08:00 | INFO | command.go:15 \u0026gt; ExecCMD: ffmpeg -y -hide_banner -ss 00:00:00 -t 00:01:32 -i /app/data/temp/back_bd650cb7-15f3-467b-99d4-fb4c90c5093e.m3u8 -c copy -hls_list_size 0 /app/data/upload/bd650cb7-15f3-467b-99d4-fb4c90c5093e/back.m3u8 | 2025-07-16T14:45:20+08:00 | INFO | transform.go:76 \u0026gt; spend 00:00:02 time | 针对问题2，决定使用多协程并发处理：\n使用视角+uuid 方式分离-i参数给的合并输入文件命名，分离多协程之间的干扰； 利用sync.WaitGroup 和 context.Context 来协同父子进程，在子进程完成多个视角切片后，解除阻塞，继续后面的流程tar-\u0026gt; 分片上传； 结果 ffmpeg -i rtmp://xxxxx:1935/yyyy/zzzz/vision -c:v copy -flags +cgop -g 0 -hls_flags program_date_time -hls_list_size 0 -hls_time 10 /app/data/record.m3u8 -y 大幅减少cpu 资源消耗 ffmpeg -y -hide_banner -ss 00:00:18 -t 00:00:40 -i xxx.m3u8 -c copy -hls_list_size 0 ./output.m3u8 seek 快速定位，避免读取开头的切片，并发协同 ","date":"2025-07-14T16:01:50+08:00","image":"https://picsum.photos/seed/ccdc7ec6/800/600","permalink":"https://daidaij.github.io/p/video_cutter/","title":"Videocutter"},{"content":"cpp 内存序 cpp 的内存序在以往开发的经历中很少有相关的需求，因此也一直没去学，近期在交流群里面谈到这方面技术后，对相关的知识做一个串联，整理成此篇笔记；\n内存屏障 说到cpp 的内存序，先不去罗列有几种内存序，首先内存屏障在cpp11 和c11 版本中就已经正式发布，其声明为 extern \u0026quot;C\u0026quot; void atomic_thread_fence( std::memory_order order ) noexcept; 下面是其三种内存屏障:\n在 x86（包括 x86-64）上，atomic_thread_fence 函数不发出 CPU 指令，仅影响编译时代码移动，但 std::atomic_thread_fence(std::memory_order_seq_cst) 除外。 atomic_thread_fence 施加的同步约束比具有相同 std::memory_order 的原子存储操作更强。 虽然原子存储-释放操作阻止所有先前的读取和写入移动到存储-释放之后，但具有 std::memory_order_release 排序的 atomic_thread_fence 阻止所有先前的读取和写入移动到所有后续存储之后。\n可以发现这三种内存屏障是约束编译时内存操作的顺序，然后解释释放和获取两种屏障，先具体到一个线程内，线程内的操作在插入释放屏障时，语义上应该在插入点之前执行的操作都应该在之前生效，不可被放到屏障之后去执行；获取屏障则是相反，是约束插入屏障后的操作不可以被提前执行，这个约束不是使用在单线程中的，因为单线程中使用最宽松的Relaxed 内存序，也就是能保证最终原子数据值的一致性就行。\n在多线程并发的场景中，如果使用原子量作为条件来约束多个线程之前的多个操作的条件，就需要注意这点，以免因为重排序，使得单个线程中语义上被原子量保证的内存操作，在跨线程的场景中被提前读到，或者延后写入。\n内存序 现在回到内存序上来，其表现上就是原子读前写后插入（release/acquire）两种相应的内存屏障，来保证跨线程的内存操作数据，是一种轻量级的同步机制，其中对应的开销更重的互斥锁的加解锁操作，内部本身就携带了内存屏障，内存序主要是用于多线程并发场景中的无锁并发数据结构，或者在高性能场景中避免直接加锁带来的性能瓶颈。\n下面给出一个伪代码解释的两组内存序的使用场景，来帮助理解这个知识点：\nmemory_order_acquire/memory_order_release 读写内存序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // 生产者线程（事务提交） std::atomic\u0026lt;bool\u0026gt; transaction_complete(false); void commit_transaction() { // 1. 写入事务数据到数据库 write_data_to_database(...); // 2. 使用Release语义标记事务完成 transaction_complete.store(true, std::memory_order_release); } // 消费者线程（读取事务结果） void read_transaction() { // 1. 使用Acquire语义检查事务状态 while (!transaction_complete.load(std::memory_order_acquire)); // 2. 确保能看到事务提交前的所有写入 read_data_from_database(...); } memory_order_acq_rel 读写同步内存序，这个会禁用读前写后的重排序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 std::atomic\u0026lt;int\u0026gt; connection_count(0); std::mutex pool_mutex; Connection* acquire_connection() { // 1. 使用AcqRel语义原子地增加连接计数 int count = connection_count.fetch_add(1, std::memory_order_acq_rel); if (count \u0026lt; MAX_CONNECTIONS) { // 有可用连接，无需加锁 return get_connection_from_pool(); } // 无可用连接，加锁等待 std::lock_guard\u0026lt;std::mutex\u0026gt; lock(pool_mutex); // ... } void release_connection(Connection* conn) { // 1. 使用AcqRel语义原子地减少连接计数 connection_count.fetch_sub(1, std::memory_order_acq_rel); // 2. 将连接放回池中 return_connection_to_pool(conn); } ","date":"2025-07-06T15:19:34+08:00","image":"https://picsum.photos/seed/91031763/800/600","permalink":"https://daidaij.github.io/p/memory_order/","title":"Memory_fence"},{"content":"双向认证 最近的一个项目合规要求使用国密算法实现双向认证，这里就将国密和常见的X509 RSA 的双向认证的实现都分享一下\nRSA 双向认证 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;net\u0026#34; ) func UseX509TLS(carootpath,certpath,keypath string)(*tls.Config,error){ certPool := x509.NewCertPool() cacert, err := ioutil.ReadFile(carootpath) if err != nil { return nil, err } certPool.AppendCertsFromPEM(cacert) authKeypair, err := tls.LoadX509KeyPair(certpath, keypath) if err != nil { return nil, err } return \u0026amp;tls.Config{ MaxVersion: tls.VersionTLS12, RootCAs: certPool, Certificates: []tls.Certificate{authKeypair}, InsecureSkipVerify: false, }, nil } func ClientConn(addr string)(*net.Conn,error){ // 三个分别是 ca证书路径，客户端证书路径，客户端私钥路径 cfg,err := UseX509TLS(\u0026#34;ca.crt\u0026#34;,\u0026#34;client.crt\u0026#34;,\u0026#34;client.key\u0026#34;) // conn,err := tls.Dial(\u0026#34;tcp\u0026#34;, addr, cfg) if err!= nil{ return nil, err } return \u0026amp;conn, nil } 这里的客户端证书和密钥是给服务端用于认证客户端的身份，同时使用指定的ca 根证书 这里用在mqtt 上就很简单了，甚至不用写连接创建的函数opts.SetTLSConfig(tlsConfig) 里面将tls 配置进去，mqtt 库会自动完成\n国密双向认证 这里就比较复杂一些了，因为目前go 的标准库的暂不支持国密认证，需要使用github.com/tjfoc/gmsm 这个仓库，来建立国密认证连接，仅配置tls 是不行的，需要\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 func GMTLSDialer(broker string, ca, key, cert string) (net.Conn, error) { cfg, err := BothAuthConfig(ca, key, cert) if err != nil { return nil, err } addr := broker if strings.HasPrefix(broker, \u0026#34;mqtt://\u0026#34;) { addr = broker[7:] } fmt.Printf(\u0026#34;addr: %s\\n\u0026#34;, addr) conn, err := gmtls.Dial(\u0026#34;tcp\u0026#34;, addr, cfg) if err != nil { return nil, err } return conn, nil } func BothAuthConfig(ca, key, cert string) (*gmtls.Config, error) { // 信任的根证书 certPool := x509.NewCertPool() cacert, err := os.ReadFile(ca) if err != nil { return nil, err } certPool.AppendCertsFromPEM(cacert) authKeypair, err := gmtls.LoadX509KeyPair(cert, key) if err != nil { return nil, err } return \u0026amp;gmtls.Config{ GMSupport: \u0026amp;gmtls.GMSupport{}, RootCAs: certPool, Certificates: []gmtls.Certificate{authKeypair}, InsecureSkipVerify: false, }, nil } func MqttOpenConnectionFn(uri *url.URL, options mqtt.ClientOptions) (net.Conn, error) { return cert.GMTLSDialer(cfg.Mqtt.Addr, \u0026#34;ca.crt\u0026#34;, \u0026#34;client.crt\u0026#34;, \u0026#34;client.key\u0026#34;， ) } 最后如何在mqtt 中使用呢opts.SetCustomOpenConnectionFn(MqttOpenConnectionFn) 这样就行\n","date":"2025-07-03T15:27:35+08:00","image":"https://picsum.photos/seed/8ebdf5d2/800/600","permalink":"https://daidaij.github.io/p/gmtls/","title":"Gmtls"},{"content":"rust 项目组织 项目结构 1 2 3 4 5 6 7 8 . ├── Cargo.toml └── src ├── deno │ ├── data.rs │ └── hello.rs ├── deno.rs └── main.rs 主代码示例 首先声明mod deno; 表明从 src/deno.rs 中导入mod 定义\n1 2 3 4 5 6 7 mod deno; use deno::hello; use deno::get_data; fn main() { hello(\u0026#34;panda\u0026#34;); print!(\u0026#34;data: {}\u0026#34;,get_data()); } 模块示例 这里先声明 src/deno 目录下的两个子模块，通过pub use 从当前模块中导出定义的两个函数 deno.rs\n1 2 3 4 mod data; mod hello; pub use self::hello::hello; pub use self::data::get_data; data.rs\n1 2 3 pub fn get_data()-\u0026gt;\u0026amp;\u0026#39;static str{ return \u0026#34;data\u0026#34;; } hello.rs\n1 2 3 pub fn hello(name:\u0026amp;str){ println!(\u0026#34;Hello ,{}\u0026#34;,name); } 备注 这里是一个最简单的mod 组织示例，用于快速回忆rust 的代码组织概念\n","date":"2025-05-19T17:09:22+08:00","image":"https://picsum.photos/seed/52d60af3/800/600","permalink":"https://daidaij.github.io/p/rust_mod/","title":"Rustmod"},{"content":"sse gin 的花式推送 众所周知ai llm 这些东西让SSE 这个技术传遍\u0026quot;石河子\u0026quot;东西南北\n其实按照推送技术来说还是有不少的：\nchunk 分块 这个其实是SSE 的基础 sse 服务端推送事件 websocket 这个可以支持双向的，但是无法兼容h2 代码示例 下面给一下处了 websocket 以外的 推送示例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 func ChunkDemo(c *gin.Context) { w := c.Writer header := w.Header() header.Set(\u0026#34;Transfer-Encoding\u0026#34;, \u0026#34;chunked\u0026#34;) header.Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/html\u0026#34;) w.WriteHeader(http.StatusOK) w.Write([]byte(` \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; `)) w.(http.Flusher).Flush() for i := 0; i \u0026lt; 10; i++ { w.Write([]byte(fmt.Sprintf(` \u0026lt;h1\u0026gt;%d\u0026lt;/h1\u0026gt; `, i))) w.(http.Flusher).Flush() time.Sleep(time.Duration(1) * time.Second) } w.Write([]byte(` \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; `)) w.(http.Flusher).Flush() }) // h1 sse func sseHandlerHTTP1(c *gin.Context) { // 设置响应头 c.Writer.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/event-stream\u0026#34;) c.Writer.Header().Set(\u0026#34;Cache-Control\u0026#34;, \u0026#34;no-cache\u0026#34;) c.Writer.Header().Set(\u0026#34;Connection\u0026#34;, \u0026#34;keep-alive\u0026#34;) c.Writer.Header().Set(\u0026#34;Access-Control-Allow-Origin\u0026#34;, \u0026#34;*\u0026#34;) // 确保响应头被立即发送 c.Writer.(http.Flusher).Flush() ticker := time.NewTicker(2 * time.Second) defer ticker.Stop() for { select { case \u0026lt;-c.Request.Context().Done(): return case t := \u0026lt;-ticker.C: // 发送SSE消息 c.Writer.Write([]byte(\u0026#34;data: \u0026#34; + t.Format(time.RFC3339) + \u0026#34;\\n\\n\u0026#34;)) c.Writer.(http.Flusher).Flush() } } } func sseHandlerHTTP2(c *gin.Context) { // 设置响应头 c.Writer.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/event-stream\u0026#34;) c.Writer.Header().Set(\u0026#34;Access-Control-Allow-Origin\u0026#34;, \u0026#34;*\u0026#34;) // 确保响应头被立即发送 c.Writer.(http.Flusher).Flush() ticker := time.NewTicker(2 * time.Second) defer ticker.Stop() for { select { case \u0026lt;-c.Request.Context().Done(): return case t := \u0026lt;-ticker.C: // 发送SSE消息 c.Writer.Write([]byte(\u0026#34;data: \u0026#34; + t.Format(time.RFC3339) + \u0026#34;\\n\\n\u0026#34;)) c.Writer.(http.Flusher).Flush() } } } 上面的区别其实是 h2 不用设置客户端缓存控制这个响应头, 比较统一的是推送开始和结束都需要Flush 刷写一下\n","date":"2025-04-18T12:33:29+08:00","image":"https://picsum.photos/seed/51efc8ca/800/600","permalink":"https://daidaij.github.io/p/sse/","title":"sse"},{"content":"链路追踪示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 package main import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp\u0026#34; \u0026#34;go.opentelemetry.io/otel\u0026#34; \u0026#34;go.opentelemetry.io/otel/attribute\u0026#34; \u0026#34;go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp\u0026#34; \u0026#34;go.opentelemetry.io/otel/propagation\u0026#34; \u0026#34;go.opentelemetry.io/otel/sdk/resource\u0026#34; tracesdk \u0026#34;go.opentelemetry.io/otel/sdk/trace\u0026#34; semconv \u0026#34;go.opentelemetry.io/otel/semconv/v1.17.0\u0026#34; ) // initTracer 初始化 OpenTelemetry 追踪器 func initTracer() (*tracesdk.TracerProvider, error) { ctx := context.Background() // 创建一个 OTLP HTTP 导出器 exp, err := otlptracehttp.New(ctx, otlptracehttp.WithInsecure(), otlptracehttp.WithEndpoint(\u0026#34;localhost:4318\u0026#34;), ) if err != nil { return nil, err } // 创建资源，指定服务名称 r, err := resource.Merge( resource.Default(), resource.NewWithAttributes( semconv.SchemaURL, semconv.ServiceName(\u0026#34;my-http-service\u0026#34;), ), ) if err != nil { return nil, err } // 创建追踪器提供程序 tp := tracesdk.NewTracerProvider( tracesdk.WithBatcher(exp), tracesdk.WithResource(r), ) // 设置全局传播器 otel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(propagation.TraceContext{}, propagation.Baggage{})) // 设置全局追踪器提供程序 otel.SetTracerProvider(tp) return tp, nil } func main() { // 初始化追踪器 tp, err := initTracer() if err != nil { log.Fatalf(\u0026#34;Failed to initialize tracer: %v\u0026#34;, err) } defer func() { if err := tp.Shutdown(context.Background()); err != nil { log.Printf(\u0026#34;Error shutting down tracer provider: %v\u0026#34;, err) } }() // 定义一个简单的 HTTP 处理函数 http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { // 创建一个新的 span ctx, span := otel.Tracer(\u0026#34;example-tracer\u0026#34;).Start(r.Context(), \u0026#34;handle-root\u0026#34;) defer span.End() // 添加一些属性到 span span.SetAttributes(attribute.String(\u0026#34;http.method\u0026#34;, r.Method)) // 模拟一些工作 w.WriteHeader(http.StatusOK) w.Write([]byte(\u0026#34;Hello, World!\u0026#34;)) }) // 使用 otelhttp 包装 HTTP 处理程序 http.Handle(\u0026#34;/\u0026#34;, otelhttp.NewHandler(http.DefaultServeMux, \u0026#34;my-http-service\u0026#34;)) log.Println(\u0026#34;Starting server on :8080\u0026#34;) if err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil); err != nil { log.Fatalf(\u0026#34;Failed to start server: %v\u0026#34;, err) } } 示例代码很明确了，另外就是使用 otle 作为exporter 时可以通过环境变量来配置endpoint 端点 OTEL_EXPORTER_OTLP_ENDPOINT 这样可以不用Option 显式配置更加灵活\n","date":"2025-04-18T12:33:17+08:00","image":"https://picsum.photos/seed/55031e83/800/600","permalink":"https://daidaij.github.io/p/otle/","title":"otle"},{"content":"记一次helm 部署重构实践 背景: 项目上会通过一个自研的部署管理平台来向设备上推送服务部署，随着时间和合作项目的增多，我负责业务的服务也多起来了，为了简化项目管理，将同业务的项目的部署整合的需求被发下来了：\n需要支持多项目组合，通过values.yaml 来选择不同的项目部署方案 要兼容现有部署平台，避免额外的修改 依赖和子Charts 我们内部用的部署方案是 helm chart ， 基于这一点最简单的组合策略就是通过依赖和condtion 的组合来控制不同依赖的子chart 是否被启用。 要实现这一方案有以下的几点需要注意：\ncondition 所引用的key 如果在values 中访问不到，默认会是启用，所以需要在values 中显式禁用未使用的服务 使用依赖去实现多应用组合部署需要，增加一个更新或者构建helm 依赖的流程： helm dependency update helm dependency build 构建时通过 \u0026ndash;set 传入的变量，只能通过 global 来传递给子Chart ，因为values 中显式覆盖子Chart 的变量才会在子Chart 中生效 上述 1. 2两点是迫使我放弃子Chart 方案的主要原因\n使用条件模板 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 {{ if and ( hasKey .Values \u0026#34;service_xxx\u0026#34;) .Values.service_xxx.enabled }} {{ $projectScopeValue := .Values.service_xxx }} {{ $projectName := \u0026#34;your-project-name\u0026#34; }} apiVersion: v1 kind: ConfigMap metadata: namespace: yourNamespace name: {{ printf \u0026#34;%-configmap\u0026#34; $projectName }} data: {{ (.Files.Glob (printf \u0026#34;%s/etc/*\u0026#34; $projectName)).AsConfig | indent 2 }} ----- {{- $customValues := tpl (.Files.Get (printf \u0026#34;%s/%s\u0026#34; $projectName (default \u0026#34;values.yaml\u0026#34; $projectScopeValue.valuesFile) )) . | fromYaml -}} apiVersion: apps/v1 kind: Deployment metadata: namespace: yourNamespace name: {{ $projectName }} ..... spec: selector: matchLabels: .... replicas: 1 strategy: type: RollingUpdate template: metadata: labels: ..... spec: containers: - name: yourImageName image: \u0026#34;yourImage:{{ $customValues.imageVersion }}\u0026#34; imagePullPolicy: IfNotPresent tty: true securityContext: privileged: true env: - name: TZ value: \u0026#39;Asia/Shanghai\u0026#39; {{ end }} 上述模板文件中通过最外围的 service_xxx.enabled 的values 变量来控制使用启用服务，同时不显式声明为true 的情况下，服务默认不会启用 可以通过.Values.xxx 来传递一些和部署环境有关的控制信息 将原始存储镜像版本号和指定运行时配置的数据通过嵌套子目录给复用上了，同时在自动构建过程中各个服务的镜像版本号更新也可以通过子目录隔离来减小风险 如下values模板 示例说明了如何灵活配置组合不同服务和配置文件\n1 2 3 4 5 6 7 # 修改 配置项，对应 chart 配置文件目录名 service_a: enabled: true config_file: config.test # 可以用来指定非子目录下的 etc 中使用的配置文件 service_b: enabled: true # 是否启用此服务，默认关闭 valuesFile: values.yaml # 可以用来指定非子目录的 values 文件中的镜像版本号 目录结构 1 2 3 4 5 6 7 8 9 10 11 --project-a/ --.--etc/ ---.--test.yaml ---.--default.yaml # 不同values 配置 --project-b/ --templates/ --.--a.deploy.yaml --.--b.deploy.yaml --Chart.yaml --values-a.yaml --values-b.yaml ","date":"2025-04-10T16:05:57+08:00","image":"https://picsum.photos/seed/f30dabd9/800/600","permalink":"https://daidaij.github.io/p/helm_practice/","title":"记一次helm部署重构实践"},{"content":"一个restful http client 实现 主要是基于日常开发中遇到的常见http 请求需求，做了简单的封装，不做过度设计, 特点是：\n链式调用 响应处理 原始响应缓存 易于复用 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 package httpx import ( \u0026#34;bytes\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;io\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;github.com/rs/zerolog/log\u0026#34; ) type HttpClientX struct { header http.Header query map[string]string tmpBody *bytes.Buffer tmpResp *bytes.Buffer respStautCode int respErr error } func NewHttpClientX() *HttpClientX { return \u0026amp;HttpClientX{ header: make(http.Header), query: make(map[string]string), tmpBody: bytes.NewBuffer(nil), tmpResp: bytes.NewBuffer(nil), } } func (h *HttpClientX) SetHeader(k, v string) *HttpClientX { h.header.Set(k, v) return h } func (h *HttpClientX) ClearHeader() *HttpClientX { h.header = make(http.Header) return h } func (h *HttpClientX) SetContentType(v string) *HttpClientX { h.header.Set(\u0026#34;Content-Type\u0026#34;, v) return h } func (h *HttpClientX) AddHeader(k, v string) *HttpClientX { h.header.Add(k, v) return h } func (h *HttpClientX) SetAuthorization(v string) *HttpClientX { h.header.Set(\u0026#34;Authorization\u0026#34;, v) return h } func (h *HttpClientX) SetQuery(k, v string) *HttpClientX { h.query[k] = v return h } func (h *HttpClientX) ClearQuery() *HttpClientX { h.query = make(map[string]string) return h } func (h *HttpClientX) Post(url string, obj any) *HttpClientX { return h.request(\u0026#34;Post\u0026#34;, url, obj) } func (h *HttpClientX) Get(url string) *HttpClientX { return h.request(\u0026#34;Get\u0026#34;, url, nil) } func (h *HttpClientX) Put(url string, obj any) *HttpClientX { return h.request(\u0026#34;Put\u0026#34;, url, obj) } func (h *HttpClientX) Delete(url string, obj any) *HttpClientX { return h.request(\u0026#34;Delete\u0026#34;, url, obj) } func (h *HttpClientX) request(method, url string, obj any) *HttpClientX { var body io.Reader if obj != nil { h.tmpBody.Reset() buf, err := json.Marshal(obj) if err != nil { log.Error().Err(err).Msg(\u0026#34;SetBody\u0026#34;) return h } h.tmpBody.Write(buf) body = h.tmpBody } req, err := http.NewRequest(method, url, body) if err != nil { log.Error().Err(err).Msg(method) h.respErr = err return h } if h.header[\u0026#34;Content-Type\u0026#34;] == nil { req.Header.Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) } for k, v := range h.header { if len(v) == 1 { req.Header.Set(k, v[0]) } } if len(h.query) \u0026gt; 0 { q := req.URL.Query() for k, v := range h.query { q.Set(k, v) } req.URL.RawQuery = q.Encode() } resp, err := http.DefaultClient.Do(req) if err != nil { log.Error().Err(err).Msg(method) h.respErr = err return h } defer resp.Body.Close() h.respStautCode = resp.StatusCode h.tmpResp.Reset() _, err = io.Copy(h.tmpResp, resp.Body) if err != nil { log.Error().Err(err).Msg(method) h.tmpResp.Reset() h.respErr = err return h } return h } // 应该是用指针对象 func (h *HttpClientX) Then(obj any) *HttpClientX { if h.respErr != nil { return h } if len(h.tmpResp.Bytes()) == 0 { return h } if err := json.Unmarshal(h.tmpResp.Bytes(), obj); err != nil { log.Error().Err(err).Msg(\u0026#34;Then\u0026#34;) h.respErr = err } return h } func (h *HttpClientX) Catch(errHandle func(error)) *HttpClientX { if h.respErr != nil { errHandle(h.respErr) } return h } func (h *HttpClientX) GetRawResp() string { return h.tmpResp.String() } func (h *HttpClientX) GetStatusCode() int { return h.respStautCode } 使用注意 先设置 header 和 query 可以设置 Content-Typ 然后通过 POST/PUT/GET/DELETE 发起实际请求 通过 Then 和 Catch 来解析响应类型，这里只是简单的做了Json 解析，可以自行扩展 可以通过GetRawResp和GetStatusCode 获取原始响应和 响应状态码，避免json 解析错误后丢失原始内容 ","date":"2025-04-03T18:13:44+08:00","image":"https://picsum.photos/seed/c73dd60a/800/600","permalink":"https://daidaij.github.io/p/go/","title":"简单的restful http client 包实现 "},{"content":"Turn 服务 在常用的 Webrtc 教程中通常会为ICE 引入Stun 或者Turn 服务，一般可以用谷歌或者Cloudflare 提供的 stun 服务，也有用开源项目自行搭建的，这里主要是介绍Pion Turn 库的使用方法。\nCredentials 长期凭证 1 2 3 4 u, p, _ := turn.GenerateLongTermCredentials(*authSecret, time.Minute) if _, err := os.Stdout.WriteString(fmt.Sprintf(\u0026#34;%s=%s\u0026#34;, u, p)); err != nil { // For use with xargs log.Panicf(\u0026#34;Failed to write to stdout: %s\u0026#34;, err) } 核心就是根据 鉴权secret 生成长期凭证\n简单的STUN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 package main import ( \u0026#34;flag\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/signal\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;github.com/pion/turn/v4\u0026#34; ) func main() { publicIP := flag.String(\u0026#34;public-ip\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;IP Address that STUN can be contacted by.\u0026#34;) port := flag.Int(\u0026#34;port\u0026#34;, 3478, \u0026#34;Listening port.\u0026#34;) flag.Parse() if len(*publicIP) == 0 { log.Fatalf(\u0026#34;\u0026#39;public-ip\u0026#39; is required\u0026#34;) } // Create a UDP listener to pass into pion/turn // pion/turn itself doesn\u0026#39;t allocate any UDP sockets, but lets the user pass them in // this allows us to add logging, storage or modify inbound/outbound traffic udpListener, err := net.ListenPacket(\u0026#34;udp4\u0026#34;, \u0026#34;0.0.0.0:\u0026#34;+strconv.Itoa(*port)) if err != nil { log.Panicf(\u0026#34;Failed to create STUN server listener: %s\u0026#34;, err) } s, err := turn.NewServer(turn.ServerConfig{ // PacketConnConfigs is a list of UDP Listeners and the configuration around them PacketConnConfigs: []turn.PacketConnConfig{ { PacketConn: udpListener, // RelayAddressGenerator: \u0026amp;turn.RelayAddressGeneratorPortRange{ // RelayAddress: net.ParseIP(*publicIP), // Address: \u0026#34;0.0.0.0\u0026#34;, // But actually be listening on every interface // MinPort: 50000, // MaxPort: 55000, // }, }, }, }) if err != nil { log.Panic(err) } // Block until user sends SIGINT or SIGTERM sigs := make(chan os.Signal, 1) signal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM) \u0026lt;-sigs if err = s.Close(); err != nil { log.Panic(err) } } 这里需要的公网IP 应该用于做中继的，通过RelayAddressGenerator 系列函数来实现一个动态负载均衡？ 上述turn.NewServer 函数的参数中，可以添加鉴权hanlder\n鉴权机制 1 2 3 4 5 6 AuthHandler: func(username string, realm string, srcAddr net.Addr) ([]byte, bool) { // nolint: revive if key, ok := usersMap[username]; ok { return key, true } return nil, false }, 这个可以和内置的两种LongTermTURNRESTAuthHandler处理时间:用户名对应的验证 对应GenerateLongTermTURNRESTCredentials; NewLongTermAuthHandler 处理长期凭证的验证,对应GenerateLongTermCredentials 可以先过滤对应鉴权的用户是否存在，再进行下一步的验证 中间注释的RelayAddressGenerator 中继地址生成器的配置，可以用于控制中继端口的分配范围，配合防火墙来使用\n传输层 Turn 支持udp、tcp、tls，配置起来也很简单，直接在turn.NewServer 函数中添加ListenerConfigs.Listener,例如使用TLS 的tcp 协议\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 // 这里两个 File 参数指向的是文件路径 cer, err := tls.LoadX509KeyPair(*certFile, *keyFile) if err != nil { log.Println(err) return } // Create a TLS listener to pass into pion/turn // pion/turn itself doesn\u0026#39;t allocate any TLS listeners, but lets the user pass them in // this allows us to add logging, storage or modify inbound/outbound traffic tlsListener, err := tls.Listen(\u0026#34;tcp4\u0026#34;, \u0026#34;0.0.0.0:\u0026#34;+strconv.Itoa(*port), \u0026amp;tls.Config{ MinVersion: tls.VersionTLS12, Certificates: []tls.Certificate{cer}, }) if err != nil { log.Println(err) return } s, err := turn.NewServer(turn.ServerConfig{ Realm: *realm, // Set AuthHandler callback // This is called every time a user tries to authenticate with the TURN server // Return the key for that user, or false when no user is found AuthHandler: func(username string, realm string, srcAddr net.Addr) ([]byte, bool) { // nolint: revive if key, ok := usersMap[username]; ok { return key, true } return nil, false }, // ListenerConfig is a list of Listeners and the configuration around them ListenerConfigs: []turn.ListenerConfig{ { Listener: tlsListener, RelayAddressGenerator: \u0026amp;turn.RelayAddressGeneratorStatic{ RelayAddress: net.ParseIP(*publicIP), Address: \u0026#34;0.0.0.0\u0026#34;, MinPort: 50000, MaxPort: 55000, }, }, }, }) 实现黑名单 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 var permissions turn.PermissionHandler = func(clientAddr net.Addr, peerIP net.IP) bool { for _, cidr := range conf.TurnDenyPeersParsed { if cidr.Contains(peerIP) { return false } } return true } _, err = turn.NewServer(turn.ServerConfig{ Realm: Realm, AuthHandler: svr.authenticate, // TCP 连接 ListenerConfigs: []turn.ListenerConfig{ {Listener: tcpListener, RelayAddressGenerator: gen, PermissionHandler: permissions}, }, // UDP 连接 PacketConnConfigs: []turn.PacketConnConfig{ {PacketConn: udpListener, RelayAddressGenerator: gen, PermissionHandler: permissions}, }, }) 使用多线程 使用多线程，看着更像是多个监听线程，所以会有端口重用的需要\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 addr, err := net.ResolveUDPAddr(\u0026#34;udp\u0026#34;, \u0026#34;0.0.0.0:\u0026#34;+strconv.Itoa(*port)) // Create `numThreads` UDP listeners to pass into pion/turn // pion/turn itself doesn\u0026#39;t allocate any UDP sockets, but lets the user pass them in // this allows us to add logging, storage or modify inbound/outbound traffic // UDP listeners share the same local address:port with setting SO_REUSEPORT and the kernel // will load-balance received packets per the IP 5-tuple listenerConfig := \u0026amp;net.ListenConfig{ Control: func(network, address string, conn syscall.RawConn) error { // nolint: revive var operr error if err = conn.Control(func(fd uintptr) { // 将socket 关联的fd 描述符配置成 通用套接字配置 地址和端口重用 operr = syscall.SetsockoptInt(int(fd), syscall.SOL_SOCKET, unix.SO_REUSEPORT, 1) }); err != nil { return err } return operr }, } relayAddressGenerator := \u0026amp;turn.RelayAddressGeneratorStatic{ RelayAddress: net.ParseIP(*publicIP), // Claim that we are listening on IP passed by user Address: \u0026#34;0.0.0.0\u0026#34;, // But actually be listening on every interface } packetConnConfigs := make([]turn.PacketConnConfig, *threadNum) for i := 0; i \u0026lt; *threadNum; i++ { // 创建一个可以用的本地UDP 监听器 // 要用net.Listen conn, listErr := listenerConfig.ListenPacket(context.Background(), addr.Network(), addr.String()) if listErr != nil { log.Fatalf(\u0026#34;Failed to allocate UDP listener at %s:%s\u0026#34;, addr.Network(), addr.String()) } packetConnConfigs[i] = turn.PacketConnConfig{ PacketConn: conn, RelayAddressGenerator: relayAddressGenerator, } log.Printf(\u0026#34;Server %d listening on %s\\n\u0026#34;, i, conn.LocalAddr().String()) } 总体来说Pione/turn 还是挺简单易用的；\n","date":"2024-12-27T14:10:10+08:00","image":"https://picsum.photos/seed/26399c1f/800/600","permalink":"https://daidaij.github.io/p/turn_go/","title":"Webrtc_go Turn 服务器相关"},{"content":"Kafka_go 主要是记录下 sarama 库的部署和使用，异步生产者的实现注意\ndocker compose 部署 首先 注意下 kafka 数据包所有者\n1 chown 1001:1001 ./data 下面是 docker-compose.yaml 文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 name: \u0026#34;kafka\u0026#34; services: kafka: image: \u0026#39;bitnami/kafka:3.6.2\u0026#39; container_name: kafka restart: always ulimits: nofile: soft: 65536 hard: 65536 environment: - TZ=Asia/Shanghai - KAFKA_CFG_NODE_ID=0 - KAFKA_CFG_PROCESS_ROLES=controller,broker - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093 - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094 - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://127.0.0.1:9094 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER networks: - app-tier ports: - \u0026#39;9092:9092\u0026#39; - \u0026#39;9094:9094\u0026#39; volumes: - ./data:/bitnami/kafka networks: app-tier: name: app-tier driver: bridge 注意这里仅仅是非集群模式的部署\n代码与结果 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 package main import ( \u0026#34;context\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/signal\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/IBM/sarama\u0026#34; ) var version = sarama.DefaultVersion var eps = []string{\u0026#34;127.0.0.1:9094\u0026#34;} var wg sync.WaitGroup func Producer(topic string, ctx context.Context) { defer wg.Done() config := sarama.NewConfig() config.Version = version config.Producer.RequiredAcks = sarama.NoResponse config.Producer.Compression = sarama.CompressionSnappy config.Producer.Return.Successes = true producer, err := sarama.NewAsyncProducer(eps, config) if err != nil { panic(err) } defer producer.Close() // 注意这里一定要让这些等结果的协程先行退出 iwg := \u0026amp;sync.WaitGroup{} iwg.Add(1) go func(ctx context.Context, p sarama.AsyncProducer) { iwg.Done() for { select { case \u0026lt;-ctx.Done(): return case err := \u0026lt;-p.Errors(): fmt.Println(err) case \u0026lt;-p.Successes(): } } }(ctx, producer) cnt := 1 for { msg := \u0026amp;sarama.ProducerMessage{ Topic: topic, Value: sarama.StringEncoder(fmt.Sprintf(\u0026#34;hello %d\u0026#34;, cnt)), } select { case \u0026lt;-ctx.Done(): // 等待子协程退出 iwg.Wait() return case producer.Input() \u0026lt;- msg: cnt++ } time.Sleep(1 * time.Second) } } type TestKafkaGroup struct { ctx context.Context } func (t *TestKafkaGroup) Setup(session sarama.ConsumerGroupSession) error { fmt.Println(\u0026#34;setup\u0026#34;) return nil } func (t *TestKafkaGroup) Cleanup(session sarama.ConsumerGroupSession) error { fmt.Println(\u0026#34;cleanup\u0026#34;) return nil } func (t *TestKafkaGroup) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error { for { select { case message, ok := \u0026lt;-claim.Messages(): if !ok { fmt.Printf(\u0026#34;message channel was closed\u0026#34;) return nil } fmt.Printf(\u0026#34;Message claimed: value = %s, partid %d timestamp = %v\\n\u0026#34;, string(message.Value), message.Partition, message.Timestamp) session.MarkMessage(message, \u0026#34;\u0026#34;) // Should return when `session.Context()` is done. // If not, will raise `ErrRebalanceInProgress` or `read tcp \u0026lt;ip\u0026gt;:\u0026lt;port\u0026gt;: i/o timeout` when kafka rebalance. see: // https://github.com/IBM/sarama/issues/1192 case \u0026lt;-session.Context().Done(): return nil case \u0026lt;-t.ctx.Done(): return nil } } } func Consumer(topic string, ctx context.Context) { defer wg.Done() config := sarama.NewConfig() config.Version = version config.Consumer.Return.Errors = true consumer, err := sarama.NewConsumerGroup(eps, topic, config) if err != nil { fmt.Println(err) return } defer consumer.Close() tc := \u0026amp;TestKafkaGroup{ctx: ctx} for { if err := consumer.Consume(ctx, []string{topic}, tc); err != nil { fmt.Println(err) if errors.Is(err, sarama.ErrClosedConsumerGroup) { return } } if ctx.Err() != nil { return } } } func main() { ctx, cancel := context.WithCancel(context.Background()) defer cancel() quit := make(chan os.Signal, 1) signal.Notify(quit, os.Interrupt,syscall.SIGINT, syscall.SIGTERM) wg.Add(1) go Producer(\u0026#34;test\u0026#34;, ctx) go Consumer(\u0026#34;test\u0026#34;, ctx) \u0026lt;-quit cancel() wg.Wait() } 输出结果如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 setup Message claimed: value = hello 11, partid 0 timestamp = 2024-12-27 11:29:27.321 +0800 CST Message claimed: value = hello 1, partid 0 timestamp = 2024-12-27 11:38:33.201 +0800 CST Message claimed: value = hello 2, partid 0 timestamp = 2024-12-27 11:38:34.201 +0800 CST Message claimed: value = hello 3, partid 0 timestamp = 2024-12-27 11:38:35.202 +0800 CST Message claimed: value = hello 4, partid 0 timestamp = 2024-12-27 11:38:36.202 +0800 CST Message claimed: value = hello 5, partid 0 timestamp = 2024-12-27 11:38:37.203 +0800 CST Message claimed: value = hello 6, partid 0 timestamp = 2024-12-27 11:38:38.203 +0800 CST Message claimed: value = hello 7, partid 0 timestamp = 2024-12-27 11:38:39.203 +0800 CST Message claimed: value = hello 8, partid 0 timestamp = 2024-12-27 11:38:40.203 +0800 CST Message claimed: value = hello 9, partid 0 timestamp = 2024-12-27 11:38:41.204 +0800 CST Message claimed: value = hello 10, partid 0 timestamp = 2024-12-27 11:38:42.206 +0800 CST Message claimed: value = hello 11, partid 0 timestamp = 2024-12-27 11:38:43.206 +0800 CST Message claimed: value = hello 12, partid 0 timestamp = 2024-12-27 11:38:44.206 +0800 CST Message claimed: value = hello 13, partid 0 timestamp = 2024-12-27 11:38:45.207 +0800 CST Message claimed: value = hello 14, partid 0 timestamp = 2024-12-27 11:38:46.208 +0800 CST Message claimed: value = hello 15, partid 0 timestamp = 2024-12-27 11:38:47.208 +0800 CST Message claimed: value = hello 16, partid 0 timestamp = 2024-12-27 11:38:48.209 +0800 CST Message claimed: value = hello 17, partid 0 timestamp = 2024-12-27 11:38:49.209 +0800 CST Message claimed: value = hello 18, partid 0 timestamp = 2024-12-27 11:38:50.21 +0800 CST Message claimed: value = hello 19, partid 0 timestamp = 2024-12-27 11:38:51.211 +0800 CST Message claimed: value = hello 20, partid 0 timestamp = 2024-12-27 11:38:52.211 +0800 CST ^C cleanup ","date":"2024-12-27T11:43:59+08:00","image":"https://picsum.photos/seed/8391ff80/800/600","permalink":"https://daidaij.github.io/p/kafka_go/","title":"Kafka_go"},{"content":"Wails 构建桌面程序实操 Wails 是一个用于构建桌面应用程序的 Go 框架。官网如下 wails\n简单前端项目封装 首先在 go install github.com/wailsapp/wails/v2/cmd/wails@latest 安装完wails 后用 wails doctor 来检查下依赖环境\n用 wails init -n name -t -t vanilla[-ts] 带-ts 会从js 切换到ts 可以将 t 参数后面的模板换成其他 例如 vue react svelte 将前端项目迁移到frontend 目录下 将项目根目录下的wails.json 和 frontend 目录下的package.json 里面的构建命令对应起来 将builid/appicon.png 替换成你想要的图标图片，最好是无背景1024*1024 大小的 下载upx 放到 path 变量所在的路径 wails build -upx -upxflags \u0026ndash;lzma -platform windows/amd64 构建过程中 加 -nsis 会生成安装程序（需要另外安装）， 加-devtools 可以在软件右键开发者模式查看 加-webview2 指定webview 依赖处理方式 Download（下载） Embed（内嵌） Browser（浏览器） Error（报错） wails json 里面 有个 info 可以给执行文件的信息里面添加你的版权信息，而且不可以被别人修改 1 2 3 4 5 6 7 \u0026#34;Info\u0026#34;: { \u0026#34;companyName\u0026#34;: \u0026#34;My Company Name\u0026#34;, \u0026#34;productName\u0026#34;: \u0026#34;Wails Vite\u0026#34;, \u0026#34;productVersion\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;copyright\u0026#34;: \u0026#34;Copyright©2024TigerJuice\u0026#34;, \u0026#34;comments\u0026#34;: \u0026#34;Built using Wails (https://wails.io)\u0026#34; }, 顺便推广下我用wails 封装的PokeRogue 桌面程序 PokeRogueWinDesk\n涉及到后端的一些数据交互 用本地网络套接字去传输，纯web 这套逻辑 使用wails 的go 函数bind，可以通过运行时js 库导入到js 当中 使用wails 提供的运行时管道，应用起来，示例如下 1 2 3 4 5 6 7 8 9 10 11 func (a *App) syncWriter(str string) { var tmp string if strings.HasSuffix(str, \u0026#34;\\n\u0026#34;) { tmp = fmt.Sprintf(\u0026#34;%s: %s\u0026#34;, time.Now().Local().Format(time.RFC3339), str) } else { tmp = fmt.Sprintf(\u0026#34;%s: %s\\n\u0026#34;, time.Now().Local().Format(time.RFC3339), str) } fmt.Fprintf(a.fw, tmp) runtime.EventsEmit(a.ctx, \u0026#34;sse\u0026#34;, tmp) } 上面这个是一个函数用来将日志消息通过管道同步到前端，前端订阅如下，\n1 2 3 EventsOn(\u0026#34;sse\u0026#34;, (data) =\u0026gt; { printlog(data); }); 其实整套逻辑就是pub sub 的一种实现，这个反过来前端js 去推送，后端去订阅也是一样的\n项目组织结构 1 2 3 4 5 6 7 8 -project --/build --/frontend --/pkg --/wailsjs --app.go --main.go --wails.json 可以通过增加 /pkg 目录来管理go 的后端逻辑，要是已经有一个后端的代码包，也可以提升到和project 同级目录下，然后通过 go mod 里面用 replace pkg v0.0.0=\u0026gt; ../pkg 来关联\n","date":"2024-12-12T11:47:28+08:00","image":"https://picsum.photos/seed/2e89fdd0/800/600","permalink":"https://daidaij.github.io/p/wails-build/","title":"使用wails 构建桌面程序实操"},{"content":"elasticsearch 菜鸟查询手册 正如你所知道的那样，本人由于不擅长elk 组件，出于学习的目的对用途更加广泛的 elasticsearch 这个核心技术栈的检索api 做了初步的了解和学习\n索引 \u0026amp; 文档 es 的文档和索引的结构如下： /\u0026lt;索引:_index\u0026gt;/\u0026lt;文档类型:_type\u0026gt;/\u0026lt;ID:_id\u0026gt;\n基本操作 新增文档 可以通过 PUT 方法去自动创建索引，在请求体body 中通过携带多组键值形式的字段，单个文档的提交就是这样简单 查询对应文档 就是对同一个路由调用 GET 方法，在响应的json 中 _source._key 的json 路径来获取键对应的字段 删除 对应文档 也是调用同路由的DELETE 方法，指定执行删除操作的主分区可能会不可用，可以通过 timeout 参数来控制这个不可用的时间范围，删除时可以通过if_seq_no和 if_primary_term ，另外删除的时候如果路由分片指定错误，删除操作不会发生 根据查询删除匹配的文档，POST /_index/_delete_by_query 更新文档也是一个套路，\n1 2 3 4 5 6 7 8 9 { \u0026#34;script\u0026#34; : { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.counter += params.count\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;params\u0026#34; : { \u0026#34;count\u0026#34; : 4 } } } 可以看到 是通过 script 中对应 source 字段中对的简单表达式 和 params 中的参数来更新数据的\n","date":"2024-12-11T17:02:52+08:00","image":"https://picsum.photos/seed/d0b120fd/800/600","permalink":"https://daidaij.github.io/p/elasticsearch/","title":"elasticsearch learing note"},{"content":"Linux 工具集合 cli类 bat 替代cat yazi 文件浏览器 lsd 替代ls fastfetch SCC 代码统计 starship shell prompt lnav 日志查看器 ripgrep 反向搜索 软件类 kitty 高性能终端模拟器 hugo 静态博客生成器 obsidian markdown 编辑器 Tiling Assiant gnome 平铺管理器 vscode 火焰截图 雾凇拼音 copyQ 语言类 uv 替代pip 和 conda 高效 ruff python lint protoc protobuf 编译器 meson 轻量级编译构建工具 ninja 构建工具 docker 和 docker-compose 以及 dpanel alias 1 2 3 4 5 6 7 8 9 10 alias hgrep=\u0026#34;history | grep \u0026#34; alias cat=\u0026#34;bat\u0026#34; alias gm=\u0026#34;git commit -m\u0026#34; alias ls=\u0026#34;lsd -a\u0026#34; alias ll=\u0026#34;lsd -al\u0026#34; alias sudo=\u0026#34;sudo\u0026#34; alias reposhow=\u0026#39;git remote show\u0026#39; alias repoadd=\u0026#39;git remote add\u0026#39; alias fy=\u0026#34;trans :en\u0026#34; alias ff=\u0026#34;fastfetch\u0026#34; Kitty 1 2 3 4 5 6 7 8 9 include ./theme.conf font_family Hack Nerd Font Mono font_size 14.0 window_padding_width 10 tab_bar_min_tabs 1 tab_bar_edge bottom tab_bar_style powerline tab_powerline_style slanted tab_title_template {title}{\u0026#39; :{}:\u0026#39;.format(num_windows) if num_windows \u0026gt; 1 else \u0026#39;\u0026#39;} ","date":"2024-12-06T15:32:49+08:00","image":"https://picsum.photos/seed/9fc93c63/800/600","permalink":"https://daidaij.github.io/p/linux-%E5%B7%A5%E5%85%B7%E9%9B%86%E5%90%88/","title":"Linux 工具集合"},{"content":"redis Lua 实用脚本 计数器 1 2 3 4 5 6 7 8 9 10 11 local key = \u0026#34;daily_data:\u0026#34;.. tostring(ARGV[1]) local increment = tonumber(ARGV[2]) local currentValue = redis.call(\u0026#39;GET\u0026#39;, key) if currentValue == false then redis.call(\u0026#39;SET\u0026#39;, key, increment) return increment else local newValue = tonumber(currentValue) + increment redis.call(\u0026#39;SET\u0026#39;, key, newValue) -- redis.call(\u0026#39;SETEX\u0026#39;, key,86400 ,newValue) return newValue end 对应的复用操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import redis import hashlib r = redis.Redis() # 定义 Lua 脚本 lua_script = \u0026#34;\u0026#34;\u0026#34; local key = \u0026#34;daily_data:\u0026#34;.. tostring(ARGV[1]) local increment = tonumber(ARGV[2]) local currentValue = redis.call(\u0026#39;GET\u0026#39;, key) if currentValue == false then redis.call(\u0026#39;SET\u0026#39;, key, increment) return increment else local newValue = tonumber(currentValue) + increment redis.call(\u0026#39;SET\u0026#39;, key, newValue) return newValue end \u0026#34;\u0026#34;\u0026#34; # 加载脚本并获取 SHA1 校验和 sha = r.script_load(lua_script) # 使用已加载的脚本执行操作 date = \u0026#39;20241106\u0026#39; increment_value = 10 new_value = r.evalsha(sha, 2, date, increment_value) print(f\u0026#34;更新后的单日累加数据：{new_value}\u0026#34;) # 再次使用已加载的脚本执行操作 new_value = r.evalsha(sha, 2, date, increment_value) print(f\u0026#34;再次更新后的单日累加数据：{new_value}\u0026#34;) 对应的go 代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 const luaScript := ` local key = KEYS[1] local change = ARGV[1] local value = redis.call(\u0026#34;GET\u0026#34;, key) if not value then value = 0 end value = value + change redis.call(\u0026#34;SET\u0026#34;, key, value) return value ` const DefaultPrefix = \u0026#34;Default\u0026#34; func getToDayKey() string { return fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, DefaultPrefix, time.Now().Local().Format(time.DateOnly)) } func Init(){ cmd := redis.ScriptLoad(luaScript) if err:= cmd.Err();err!=nil{ log.Debug().Msgf(\u0026#34;转载lua 脚本错误 %v\u0026#34;,err) } } func Update(delta int)(int,err){ var incrBy = redis.NewScript(luaScript) keys := getToDayKey() values := delta return incrBy.Run(ctx, rdb, keys, values...).Int() } 分布式集群限流 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 local key = KEYS[1] # 这里能进一步将 limit 和 window 写成常量来避免传值 local limit = tonumber(ARGV[1]) local window = tonumber(ARGV[2]) local now = tonumber(redis.call(\u0026#39;TIME\u0026#39;)[1]) local countKey = key.. \u0026#39;:\u0026#39;.. math.floor(now / window) local count = tonumber(redis.call(\u0026#39;GET\u0026#39;, countKey) or 0) if count \u0026lt; limit then redis.call(\u0026#39;INCR\u0026#39;, countKey) redis.call(\u0026#39;EXPIRE\u0026#39;, countKey, window) return 1 else return 0 end 分布式锁 1 2 3 4 5 6 7 8 9 10 11 12 13 local key = KEYS[1] local value = ARGV[1] local ttl = tonumber(ARGV[2]) local existingValue = redis.call(\u0026#39;GET\u0026#39;, key) if existingValue == false then redis.call(\u0026#39;SET\u0026#39;, key, value, \u0026#39;PX\u0026#39;, ttl) return true elseif existingValue == value then redis.call(\u0026#39;PEXPIRE\u0026#39;, key, ttl) return true else return false end ","date":"2024-12-05T14:42:14+08:00","image":"https://picsum.photos/seed/d0ba0f6f/800/600","permalink":"https://daidaij.github.io/p/luascript/","title":"Luascript"},{"content":"ebpf postgresql慢查询日志服务 因为需要使用用户控件探针来做绑定，需要可执行文件有符号信息，所以需要在编译pg 时启用 \u0026ndash;enable-debug\n用户态程序 这里主要是实现一个日志，可以通过systemd 来管理这个慢查询日志，目前没给这个程序做配置文件管理的部分，但是实现起来也是很简单的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 //go:build amd64 package main import ( \u0026#34;bytes\u0026#34; \u0026#34;encoding/binary\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/signal\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;github.com/cilium/ebpf/link\u0026#34; \u0026#34;github.com/cilium/ebpf/perf\u0026#34; \u0026#34;github.com/cilium/ebpf/rlimit\u0026#34; \u0026#34;golang.org/x/sys/unix\u0026#34; ) // 这个是bpf2go 的生成代码 //go:generate go run github.com/cilium/ebpf/cmd/bpf2go -cc clang -target amd64 -type event bpf dbstats.c -- -I../headers const ( symbol = \u0026#34;exec_simple_query\u0026#34; ) var binPath string var logPath string var logFile *os.File var pid int var slow int func init() { flag.StringVar(\u0026amp;binPath, \u0026#34;P\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;the path of postgres\u0026#34;) flag.IntVar(\u0026amp;pid, \u0026#34;p\u0026#34;, 0, \u0026#34;pid\u0026#34;) flag.IntVar(\u0026amp;slow, \u0026#34;t\u0026#34;, 200, \u0026#34;the threshold of slow query\u0026#34;) flag.StringVar(\u0026amp;logPath, \u0026#34;l\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;the log output file path\u0026#34;) } func Config() bool { flag.Parse() if binPath == \u0026#34;\u0026#34; \u0026amp;\u0026amp; pid == 0 { fmt.Printf(\u0026#34;invalid argument path [%s] pid %d\u0026#34;, binPath, pid) return true } if binPath == \u0026#34;\u0026#34; { if path, err := filepath.EvalSymlinks(fmt.Sprintf(\u0026#34;/proc/%d/exe\u0026#34;, pid)); err != nil { fmt.Printf(\u0026#34;pid %d err %v\u0026#34;, pid, err) return true } else { binPath = path } } if logPath != \u0026#34;\u0026#34; { path, err := filepath.Abs(logPath) if err != nil { fmt.Printf(\u0026#34;get abs path failed %v\\n\u0026#34;, path) } if file, err := os.OpenFile(path, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666); err != nil { fmt.Printf(\u0026#34;open log file failed %v\\n\u0026#34;, err) } else { logFile = file log.SetOutput(file) } } if slow \u0026lt; 0 || slow \u0026gt; 2000 { return true } return false } func main() { shouldReturn := Config() if shouldReturn { return } defer func(file *os.File) { if file != nil { file.Close() } }(logFile) stopper := make(chan os.Signal, 1) signal.Notify(stopper, os.Interrupt, syscall.SIGTERM) // Allow the current process to lock memory for eBPF resources. if err := rlimit.RemoveMemlock(); err != nil { log.Fatal(err) } // Load pre-compiled programs and maps into the kernel. objs := bpfObjects{} if err := loadBpfObjects(\u0026amp;objs, nil); err != nil { log.Fatalf(\u0026#34;loading objects: [%v]\u0026#34;, err) } defer objs.Close() // 200ms 是慢查询上限 objs.SlowMap.Put(0, slow) // Open an ELF binary and read its symbols. ex, err := link.OpenExecutable(binPath) if err != nil { log.Fatalf(\u0026#34;opening executable: %s\u0026#34;, err) } // Open a Uretprobe at the exit point of the symbol and attach // the pre-compiled eBPF program to it. up, err := ex.Uretprobe(symbol, objs.UretprobeExecSimpleQuery, nil) if err != nil { log.Fatalf(\u0026#34;creating uretprobe: %s\u0026#34;, err) } defer up.Close() do, err := ex.Uprobe(symbol, objs.UprobeExecSimpleQuery, nil) if err != nil { log.Fatalf(\u0026#34;creating uprobe: %s\u0026#34;, err) } defer do.Close() // Open a perf event reader from userspace on the PERF_EVENT_ARRAY map // described in the eBPF C program. rd, err := perf.NewReader(objs.Events, os.Getpagesize()) if err != nil { log.Fatalf(\u0026#34;creating perf event reader: %s\u0026#34;, err) } defer rd.Close() go func() { // Wait for a signal and close the perf reader, // which will interrupt rd.Read() and make the program exit. \u0026lt;-stopper log.Println(\u0026#34;Received signal, exiting program..\u0026#34;) if err := rd.Close(); err != nil { log.Fatalf(\u0026#34;closing perf event reader: %s\u0026#34;, err) } }() log.Printf(\u0026#34;Listening for events..\u0026#34;) // bpfEvent is generated by bpf2go. var event bpfEvent for { record, err := rd.Read() if err != nil { if errors.Is(err, perf.ErrClosed) { return } log.Printf(\u0026#34;reading from perf event reader: %s\u0026#34;, err) continue } if record.LostSamples != 0 { log.Printf(\u0026#34;perf event ring buffer full, dropped %d samples\u0026#34;, record.LostSamples) continue } // Parse the perf event entry into a bpfEvent structure. if err := binary.Read(bytes.NewBuffer(record.RawSample), binary.LittleEndian, \u0026amp;event); err != nil { log.Printf(\u0026#34;parsing perf event: %s\u0026#34;, err) continue } log.Printf(\u0026#34;%s:%f/ms\\n\u0026#34;, unix.ByteSliceToString(event.Cmd[:]), float64(event.Timestamp)/1000000.0) } } ebpf uprobe 这部分代码主要是对命令的拷贝和exec_simple_query函数执行的计时\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 //go:build ignore #include \u0026#34;common.h\u0026#34; #include \u0026#34;bpf_tracing.h\u0026#34; char __license[] SEC(\u0026#34;license\u0026#34;) = \u0026#34;Dual MIT/GPL\u0026#34;; #define MAX_DATA_LEN 256 u32 slowThreshold=0; const u32 timebase = 1000000; struct event { u64 pid; u64 timestamp; u8 cmd[MAX_DATA_LEN]; }; struct bpf_map_def SEC(\u0026#34;maps\u0026#34;) slow_map = { .type = BPF_MAP_TYPE_ARRAY, .key_size = sizeof(u32), .value_size = sizeof(u32), .max_entries = 1, }; struct bpf_map_def SEC(\u0026#34;maps\u0026#34;) kprobe_map = { .type = BPF_MAP_TYPE_HASH, .key_size = sizeof(u64), .value_size = sizeof(struct event), .max_entries = 256, }; struct { __uint(type, BPF_MAP_TYPE_PERF_EVENT_ARRAY); } events SEC(\u0026#34;.maps\u0026#34;); // Force emitting struct event into the ELF. const struct event *unused __attribute__((unused)); SEC(\u0026#34;uretprobe/exec_simple_query\u0026#34;) int uretprobe_exec_simple_query(struct pt_regs *ctx) { u64 pid = bpf_get_current_pid_tgid()\u0026gt;\u0026gt;32; struct event * valp = bpf_map_lookup_elem(\u0026amp;kprobe_map, \u0026amp;pid); if (valp) { valp-\u0026gt;timestamp = bpf_ktime_get_ns()- (valp-\u0026gt;timestamp); if(slowThreshold\u0026gt;0||valp-\u0026gt;timestamp\u0026gt;slowThreshold){ bpf_perf_event_output(ctx, \u0026amp;events, BPF_F_CURRENT_CPU, valp, sizeof(struct event)); } bpf_map_delete_elem(\u0026amp;kprobe_map, \u0026amp;pid); return 0; } return 0; } SEC(\u0026#34;uprobe/exec_simple_query\u0026#34;) int uprobe_exec_simple_query(struct pt_regs *ctx) { if(slowThreshold==0){ int key =0; u32 * ret = (u32*)bpf_map_lookup_elem(\u0026amp;slow_map, \u0026amp;key); if(ret) slowThreshold = *ret*timebase; } struct event event; u64 pid = bpf_get_current_pid_tgid()\u0026gt;\u0026gt;32; u64 ts = bpf_ktime_get_ns(); event.pid = pid; event.timestamp = ts; char * sql_string = (char*)PT_REGS_PARM1(ctx); bpf_probe_read(\u0026amp;event.cmd,sizeof(event.cmd),sql_string); bpf_map_update_elem(\u0026amp;kprobe_map,\u0026amp;pid,\u0026amp;event,BPF_ANY); return 0; } 这里因为ebp-go 没有从go 程序处修改ebpf 中全局变量的接口，这里是通过一个map array 来间接传递这个慢查询阈值，最大支持2s 的慢查询上限，要是觉得这个上限比较低，可以将数据类型改成 u64，这里理论上只会在初次触发时进行全局变量的赋值，后续使用全局变量做快速路径\n","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/ebpf-postgresql%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%97%A5%E5%BF%97%E6%9C%8D%E5%8A%A1/","title":"ebpf postgresql慢查询日志服务"},{"content":"ETCD 手册 KV 操作 WithIgnoreLease() 使用租约时可以用这个，当key 不存在时会返回错误 WithPrevKV() 可以返回更新前的KV值 WithIgnoreValue() 普通put 使用这个key不存在时会返回错误 WithSort(clientv3.SortByKey, clientv3.SortDescend) 可以让在查询的时候使用特定的排序方式 WithPrefix() 这可以可以按照key，查找前缀是key字符串的所有值； Get() 使用的WithRev(presp.Header.Revision) ，中的版本号可以时某次put操作返回的版本号，我觉得get的其实也是可以的；\n看了下源码 ResponseHeader 这东西里面塞了： ClusterId 和这个消息交互的集群的id\nMemberId 节点id Revision 消息版本 RaftTerm 选举的周期\n1 2 3 4 5 6 // 这里获取版本后，该版本之前的历史数据存储开始进行合并压缩 // 这里会生成快照吗？ 按照文档上说这个操作应该是要定时进行的 compRev := resp.Header.Revision // specify compact revision of your choice ctx, cancel = context.WithTimeout(context.Background(), requestTimeout) _, err = cli.Compact(ctx, compRev) func (Maintenance).Status(ctx Context, endpoint string) 可以获取集群的状态 func (Maintenance).Defragment(ctx Context, endpoint string) 这可以开启etcd 的碎片整理\n授权管理 先是简单的通过用户名密码来验证\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 // 这部分可以手动来进行的 // etcdctl --user root role add r if _, err = cli.RoleAdd(context.TODO(), \u0026#34;r\u0026#34;); err != nil { log.Fatal(err) } // etcdctl --user root role grant-permission r foo zoo // 使用 -prefix=true 可以仅指定开头前缀 if _, err = cli.RoleGrantPermission(context.TODO(),\u0026#34;r\u0026#34;, \u0026#34;foo\u0026#34;, \u0026#34;zoo\u0026#34;, clientv3.PermissionType(clientv3.PermReadWrite),); err != nil { log.Fatal(err) } // etcdctl --user root user add u --new-user-password 123 if _, err = cli.UserAdd(context.TODO(), \u0026#34;u\u0026#34;, \u0026#34;123\u0026#34;); err != nil { log.Fatal(err) } // etcdctl --user root user grant-role u r if _, err = cli.UserGrantRole(context.TODO(), \u0026#34;u\u0026#34;, \u0026#34;r\u0026#34;); err != nil { log.Fatal(err) } // etcdctl auth enable if _, err = cli.AuthEnable(context.TODO()); err != nil { log.Fatal(err) } // 这里使用 root 角色的用户来登录 rootCli, err := clientv3.New(clientv3.Config{ Endpoints: exampleEndpoints(), DialTimeout: dialTimeout, Username: \u0026#34;root\u0026#34;, Password: \u0026#34;123\u0026#34;, }) if err != nil { log.Fatal(err) } defer rootCli.Close() // root 用户可以获取别的 用户或者角色的数据 etcdctl --user root role get r resp, err := rootCli.RoleGet(context.TODO(), \u0026#34;r\u0026#34;) if err != nil { log.Fatal(err) } // 可以获得 角色权限的信息 fmt.Printf(\u0026#34;user u permission: key %q, range end %q\\n\u0026#34;, resp.Perm[0].Key, resp.Perm[0].RangeEnd) // 这里关闭身份校验 etcdctl auth disable if _, err = rootCli.AuthDisable(context.TODO()); err != nil { log.Fatal(err) } 建立客户端连接时使用的证书\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 tlsInfo := transport.TLSInfo{ CertFile: \u0026#34;/tmp/test-certs/test-name-1.pem\u0026#34;, KeyFile: \u0026#34;/tmp/test-certs/test-name-1-key.pem\u0026#34;, TrustedCAFile: \u0026#34;/tmp/test-certs/trusted-ca.pem\u0026#34;, } tlsConfig, err := tlsInfo.ClientConfig() if err != nil { log.Fatal(err) } cli, err := clientv3.New(clientv3.Config{ Endpoints: exampleEndpoints(), DialTimeout: dialTimeout, TLS: tlsConfig, }) 事务 STM is an interface for software transactional memory. 事务使用 MVCC多版本控制，在事务执行的函数类使用 STM 来读写键值\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 // Txn 这个简单的事务接口，还是基于客户端连接来的 kvc := clientv3.NewKV(cli) _, err = kvc.Put(context.TODO(), \u0026#34;key\u0026#34;, \u0026#34;xyz\u0026#34;) if err != nil { log.Fatal(err) } ctx, cancel := context.WithTimeout(context.Background(), requestTimeout) // if 条件成立 会执行 then 分支的修改，否则会执行else 分支的操作 _, err = kvc.Txn(ctx). // txn value comparisons are lexical If(clientv3.Compare(clientv3.Value(\u0026#34;key\u0026#34;), \u0026#34;\u0026gt;\u0026#34;, \u0026#34;abc\u0026#34;)). // the \u0026#34;Then\u0026#34; runs, since \u0026#34;xyz\u0026#34; \u0026gt; \u0026#34;abc\u0026#34; Then(clientv3.OpPut(\u0026#34;key\u0026#34;, \u0026#34;XYZ\u0026#34;)). // the \u0026#34;Else\u0026#34; does not run Else(clientv3.OpPut(\u0026#34;key\u0026#34;, \u0026#34;ABC\u0026#34;)). Commit() // exchange := func(stm concurrency.STM) { from, to := rand.Intn(totalAccounts), rand.Intn(totalAccounts) if from == to { // nothing to do return } // read values fromK, toK := fmt.Sprintf(\u0026#34;accts/%d\u0026#34;, from), fmt.Sprintf(\u0026#34;accts/%d\u0026#34;, to) fromV, toV := stm.Get(fromK), stm.Get(toK) fromInt, toInt := 0, 0 fmt.Sscanf(fromV, \u0026#34;%d\u0026#34;, \u0026amp;fromInt) fmt.Sscanf(toV, \u0026#34;%d\u0026#34;, \u0026amp;toInt) // transfer amount xfer := fromInt / 2 fromInt, toInt = fromInt-xfer, toInt+xfer // write back stm.Put(fromK, fmt.Sprintf(\u0026#34;%d\u0026#34;, fromInt)) stm.Put(toK, fmt.Sprintf(\u0026#34;%d\u0026#34;, toInt)) return } // concurrently exchange values between accounts var wg sync.WaitGroup wg.Add(10) for i := 0; i \u0026lt; 10; i++ { go func() { defer wg.Done() if _, serr := concurrency.NewSTM(cli, func(stm concurrency.STM) error { exchange(stm) return nil }); serr != nil { log.Fatal(serr) } }() } wg.Wait() 普通的 kv api 其实也有一个Txn ,但是同一个key 只能修改一次\n1 2 3 4 5 6 7 8 9 10 11 12 13 orderingKv := ordering.NewKV(cli.KV, func(op clientv3.Op, resp clientv3.OpResponse, prevRev int64) error { return errOrderViolation }) orderingTxn := orderingKv.Txn(ctx) _, err = orderingTxn.If( clientv3.Compare(clientv3.Value(\u0026#34;b\u0026#34;), \u0026#34;\u0026gt;\u0026#34;, \u0026#34;a\u0026#34;), ).Then( clientv3.OpGet(\u0026#34;foo\u0026#34;), ).Commit() if err != nil { t.Fatal(err) } 租约 租约有点像 go 里面的上下文，租约过期时会撤销掉这期间的更改；同时在func (Lease).Revoke(ctx Context, id LeaseID) 释放租约的时候，之前修改会被视作失效了；func (Lease).KeepAliveOnce(ctx Context, id LeaseID) 可以手动续约，避免租约超期被取消了；\nkey 和 Lease 是多对一的关系。一个 key 最多只能挂绑定一个 Lease ，但是一个 Lease 上能挂多个 key 。租约在申请下来后，关联的操作，我觉得全是修改，会被关联到这个租约的 map 里面，这段事件应该是独占这些个 key 的所有权，所以加进来的key修改，在租约失效的时候，反向调用Txn 来删除这些key，就能把之前的版本恢复\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 lease, err := cli.Grant(context.Background(), 100) if err != nil { t.Fatal(err) } // 每个会话会有一个唯一的ID 和TTL 存活时间 s, err := concurrency.NewSession(cli, concurrency.WithLease(lease.ID)) if err != nil { t.Fatal(err) } defer s.Close() assert.Equal(t, s.Lease(), lease.ID) go s.Orphan() select { case \u0026lt;-s.Done(): case \u0026lt;-time.After(time.Millisecond * 100): t.Fatal(\u0026#34;session did not get orphaned as expected\u0026#34;) } 使用租约来控制的会话会比租约更早结束，以免出现并发控制的问题？这个和上面的互斥锁连用就可以实现租约时长来控制的互斥锁，超时会退出，并撤销操作？ 另外可以给租约设置 TTL 也就是生存时间\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 s, err := concurrency.NewSession(cli, concurrency.WithTTL(setTTL)) if err != nil { t.Fatal(err) } defer s.Close() leaseID := s.Lease() // TTL retrieved should be less than the set TTL, but not equal to default:60 or exprired:-1 resp, err := cli.Lease.TimeToLive(context.Background(), leaseID) if err != nil { t.Log(err) } if resp.TTL == -1 { t.Errorf(\u0026#34;client lease should not be expired: %d\u0026#34;, resp.TTL) } if resp.TTL == 60 { t.Errorf(\u0026#34;default TTL value is used in the session, instead of set TTL: %d\u0026#34;, setTTL) } if resp.TTL \u0026gt;= int64(setTTL) || resp.TTL \u0026lt; int64(setTTL)-20 { t.Errorf(\u0026#34;Session TTL from lease should be less, but close to set TTL %d, have: %d\u0026#34;, setTTL, resp.TTL) } 这里可以看到 租约的实际时间是比设置的要短的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 lease, err := cli.Grant(context.Background(), 100) if err != nil { t.Fatal(err) } s, err := concurrency.NewSession(cli, concurrency.WithLease(lease.ID)) if err != nil { t.Fatal(err) } defer s.Close() assert.Equal(t, s.Lease(), lease.ID) // 主要是通过 会话的上下文的Done 来控制会话内操作的退出 childCtx, cancel := context.WithCancel(s.Ctx()) defer cancel() go s.Orphan() select { case \u0026lt;-childCtx.Done(): case \u0026lt;-time.After(time.Millisecond * 100): t.Fatal(\u0026#34;child context of session context is not canceled\u0026#34;) } 会话和 go 原生的上下文的使用； 总结一下：\n租约加 会话加互斥锁 可以实现分布式锁 租约加会话加 上下文，可以取消会话内协程的执行 分布式锁 etcd 3有个并发api ，调用这个api 可以实现分布式锁，锁会持有到主动解锁或者租期到了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 新建会话是一个标准流程表，因为下面申请锁需要通过一个会话来进行 s1, err := concurrency.NewSession(cli) if err != nil { t.Fatal(err) } defer s1.Close() m1 := concurrency.NewMutex(s1, \u0026#34;/my-lock/\u0026#34;) if err = m1.Lock(context.TODO()); err != nil { t.Fatal(err) } // 这之间就是s1 获得锁的临界区 if err := m1.Unlock(context.TODO()); err != nil { t.Fatal(err) } 如果先调用解锁，会得到ErrLockReleased 也就是锁已经被释放了，或者没有获得锁，总而言之就是当前没有持有锁\n服务发现和注册 实际是etcd根据mainID去磁盘查数据，磁盘中数据以revision.main+revision.sub为key(bbolt 数据库中的key)，所以就会依次遍历出所有的版本数据。同时判断遍历到的value中的key(etcd中的key)是不是用户watch的，是则推送给用户。\n这里每次都会遍历数据库性能可能会很差，实际使用时一般用户只会关注最新的revision，不会去关注旧数据。\n采用了MVCC，以一种优雅的方式解决了锁带来的问题。执行写操作或删除操作时不会再原数据上修改而是创建一个新版本。这样并发的读取操作仍然可以读取老版本的数据，写操作也可以同时进行。这个模式的好处在于读操作不再阻塞，事实上根本就不需要锁。 客户端读key的时候指定一个版本号，服务端保证返回比这个版本号更新的数据，但不保证返回最新的数据。 MVCC能最大化地实现高效地读写并发，尤其是高效地读，非常适合读多写少的场景。\n客户端使用watch 来获取服务端地址\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 var serviceTarget = \u0026#34;Hello\u0026#34; type remoteService struct { name string nodes map[string]string mutex sync.Mutex } service = \u0026amp;remoteService { name: serviceTarget } kv := clientv3.NewKV(etcdClient) rangeResp, err := kv.Get(context.TODO(), service.name, clientv3.WithPrefix()) if err != nil { panic(err) } service.mutex.Lock() for _, kv := range rangeResp.Kvs { service.nodes[string(kv.Key)] = string(kv.Value) } service.mutex.Unlock() go watchServiceUpdate(etcdClient, service) // 监控服务目录下的事件 func watchServiceUpdate(etcdClient clientv3.Client, service *remoteService) { watcher := clientv3.NewWatcher(client) // Watch 服务目录下的更新 watchChan := watcher.Watch(context.TODO(), service.name, clientv3.WithPrefix()) for watchResp := range watchChan { // 这里对增删时间的响应，会使用互斥锁来解决并发的数据修改问题 for _, event := range watchResp.Events { service.mutex.Lock() switch (event.Type) { case mvccpb.PUT://PUT事件，目录下有了新key service.nodes[string(event.Kv.Key)] = string(event.Kv.Value) case mvccpb.DELETE://DELETE事件，目录中有key被删掉(Lease过期，key 也会被删掉) delete(service.nodes, string(event.Kv.Key)) } service.mutex.Unlock() } } } 服务端主要是注意租约的维护\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 // 将服务注册到etcd上 func RegisterServiceToETCD(ServiceTarget string, value string) { dir = strings.TrimRight(ServiceTarget, \u0026#34;/\u0026#34;) + \u0026#34;/\u0026#34; client, err := clientv3.New(clientv3.Config{ Endpoints: []string{\u0026#34;localhost:2379\u0026#34;}, DialTimeout: 5 * time.Second, }) if err != nil { panic(err) } kv := clientv3.NewKV(client) lease := clientv3.NewLease(client) var curLeaseId clientv3.LeaseID = 0 for { if curLeaseId == 0 { leaseResp, err := lease.Grant(context.TODO(), 10) if err != nil { panic(err) } key := ServiceTarget + fmt.Sprintf(\u0026#34;%d\u0026#34;, leaseResp.ID) if _, err := kv.Put(context.TODO(), key, value, clientv3.WithLease(leaseResp.ID)); err != nil { panic(err) } curLeaseId = leaseResp.ID } else { // 续约租约，如果租约已经过期将curLeaseId复位到0重新走创建租约的逻辑 if _, err := lease.KeepAliveOnce(context.TODO(), curLeaseId); err == rpctypes.ErrLeaseNotFound { curLeaseId = 0 continue } } time.Sleep(time.Duration(1) * time.Second) } } 使用 watch 监视的时候 clientv3.WithRev(1) 可以指定从哪个版本开始获取，clientv3.WithFragment() 会允许服务端将事件分页发送过来\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 select { case ws := \u0026lt;-wch: // 没启用分页的时候，因为对应的 key 的值太大了，旧没接收到 if !fragment \u0026amp;\u0026amp; exceedRecvLimit { if len(ws.Events) != 0 { t.Fatalf(\u0026#34;expected 0 events with watch fragmentation, got %d\u0026#34;, len(ws.Events)) } exp := \u0026#34;code = ResourceExhausted desc = grpc: received message larger than max (\u0026#34; if !strings.Contains(ws.Err().Error(), exp) { t.Fatalf(\u0026#34;expected \u0026#39;ResourceExhausted\u0026#39; error, got %v\u0026#34;, ws.Err()) } return } // 启用分页将每次发送的数据分成限制内大小后，拿到的分页数，这个事件本身是键值对的一个切片，里面的元素是类似CPP 的 pair 这种键值二元组 if len(ws.Events) != 10 { t.Fatalf(\u0026#34;expected 10 events with watch fragmentation, got %d\u0026#34;, len(ws.Events)) } if ws.Err() != nil { t.Fatalf(\u0026#34;unexpected error %v\u0026#34;, ws.Err()) } case \u0026lt;-time.After(testutil.RequestTimeout): t.Fatalf(\u0026#34;took too long to receive events\u0026#34;) } 使用 cfg.ClientMaxCallRecvMsgSize = 1.5 * 1024 * 1024 修改集群配置时，会限制集群给客户端发送消息大小\n观测 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import( grpcprom \u0026#34;github.com/grpc-ecosystem/go-grpc-prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; ) // 这样在客户端的 grpc 连接里面塞两个Prometheus的中间件进去 cli, err := clientv3.New(clientv3.Config{ Endpoints: exampleEndpoints(), DialOptions: []grpc.DialOption{ grpc.WithUnaryInterceptor(grpcprom.UnaryClientInterceptor), grpc.WithStreamInterceptor(grpcprom.StreamClientInterceptor), }, }) if err!=nil{ log.Fatal(err) } defer cli.close() // 开个 http 服务端 ln, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:0\u0026#34;) if err != nil { log.Fatal(err) } defer ln.close() http.Serve(ln, promhttp.Handler()) // 现在就可以被监听到了 调优 io优先级\n1 sudo ionice -c2 -n0 -p `pgrep etcd` 快照触发数量\n1 etcd --snapshot-count=5000 心跳和选举时间\n1 etcd --heartbeat-interval=100 --election-timeout=500 cpu\n1 echo performance | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor 一些维护操作 1 2 3 4 5 6 7 8 9 etcdctl member list -w table # 可以查看节点信息 etcdctl move-leader XXID --endpoints 127.0.0.1:2379 etcdctl member remove xxxID # 重新将一个节点添加到集群里面来 etcdctl member add etcd01 --peer-urls=\u0026#34;https://xxxxxxx:2380\u0026#34; # 对某个节点存储快照 etcdctl --endpoints=https://10.184.4.240:2380 snapshot save snapshot.db # 从节点快照恢复数据 etcdctl snapshot restore snapshot.db --name etcd01 --initial-cluster etcd01=https://10.184.4.238:2379,etcd02=https://10.184.4.239:2379,etcd03=https://10.184.4.240:2379 --initial-cluster-token etcd-cluster --initial-advertise-peer-urls https://10.184.4.238:2380 使用客户端api 也是可以实现上面的操作的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // 添加一个 节点进来 2380 一般是这个端口，用来做集群间通信的，那个2379的是用监听客户端的 mresp, err := cli.MemberAdd(context.Background(), []string{\u0026#34;http://localhost:32380\u0026#34;}) if err != nil { log.Fatal(err) } fmt.Println(\u0026#34;added member.PeerURLs:\u0026#34;, mresp.Member.PeerURLs) fmt.Println(\u0026#34;members count:\u0026#34;, len(mresp.Members)) // Restore original cluster state _, err = cli.MemberRemove(context.Background(), mresp.Member.ID) if err != nil { log.Fatal(err) } // 这个添加进来做从节点？ mresp, err := cli.MemberAddAsLearner(context.Background(), []string{\u0026#34;http://localhost:32381\u0026#34;}) if err != nil { log.Fatal(err) } // 这里用来获取集群的节点列表 resp, err := cli.MemberList(context.Background()) if err != nil { log.Fatal(err) } // 修改节点的内部通信地址 peerURLs := []string{\u0026#34;http://localhost:12380\u0026#34;} _, err = cli.MemberUpdate(context.Background(), resp.Members[0].ID, peerURLs) if err != nil { log.Fatal(err) } 快照 etcd 的快照和虚拟机的快照比较类似，是摸一个时间点etcd 节点的所有数据；快照是一个checkpoint，避免因为wal 数据被无限制写入，导致体量超大，通过checkpoint做一个记录，后续的wal可以做增量，checkpoint生成的快照充当的应该是快照前的数据，发生修改后的数据会在wal上，(也不能这么说，因为wal记录本来就是修改记录)。\n","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/etcd-go-%E6%89%8B%E5%86%8C/","title":"etcd go 手册"},{"content":"faketcp 原理简介 背景 通过udp 实现多路传输时，可能会被运营商通过qos策略给丢掉；通过rawsocket 机制在udp报文上增加tcp 头来伪装成tcp包，通过增加一部封拆包头的代价来换取udp通信的可用性\n代码实现 这里没有做具体的udp包体封装的例子，但是字节流数据传输可见是完整的，将一个udp报文转成[]byte切片来使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 package main import ( \u0026#34;encoding/binary\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;sync\u0026#34; // \u0026#34;github.com/xitongsys/ethernet-go/header\u0026#34; ) type TCP struct { SrcPort uint16 DstPort uint16 Seq uint32 Ack uint32 Offset uint8 Flags uint8 Win uint16 Checksum uint16 UrgPointer uint16 Opt uint32 } type IPv4Pseudo struct { Src uint32 Dst uint32 Reserved uint8 Protocol uint8 Len uint16 } func (h *IPv4Pseudo) Marshal() []byte { headerLen := int(12) res := make([]byte, headerLen) binary.BigEndian.PutUint32(res[0:], h.Src) binary.BigEndian.PutUint32(res[4:], h.Dst) res[8] = byte(h.Reserved) res[9] = byte(h.Protocol) binary.BigEndian.PutUint16(res[10:], h.Len) return res } func (h *TCP) Marshal() []byte { res := make([]byte, 20) binary.BigEndian.PutUint16(res, h.SrcPort) binary.BigEndian.PutUint16(res[2:], h.DstPort) binary.BigEndian.PutUint32(res[4:], h.Seq) binary.BigEndian.PutUint32(res[8:], h.Ack) res[12] = byte(h.Offset) res[13] = byte(h.Flags) binary.BigEndian.PutUint16(res[14:], h.Win) binary.BigEndian.PutUint16(res[16:], h.Checksum) binary.BigEndian.PutUint16(res[18:], h.UrgPointer) return res } func ReCalTcpCheckSum(bs []byte, src, dst uint32) error { if len(bs) \u0026lt; 20 { return fmt.Errorf(\u0026#34;too short\u0026#34;) } ipps := IPv4Pseudo{} ipps.Src = src ipps.Dst = dst ipps.Reserved = 0 ipps.Protocol = 6 ipps.Len = uint16(len(bs)) ippsbs := ipps.Marshal() tcpbs := bs tcpbs[16] = 0 tcpbs[17] = 0 if len(tcpbs)%2 == 1 { tcpbs = append(tcpbs, byte(0)) } s := uint32(0) for i := 0; i \u0026lt; len(ippsbs); i += 2 { s += uint32(binary.BigEndian.Uint16(ippsbs[i : i+2])) } for i := 0; i \u0026lt; len(tcpbs); i += 2 { s += uint32(binary.BigEndian.Uint16(tcpbs[i : i+2])) } for (s \u0026gt;\u0026gt; 16) \u0026gt; 0 { s = (s \u0026gt;\u0026gt; 16) + (s \u0026amp; 0xffff) } binary.BigEndian.PutUint16(tcpbs[16:], ^uint16(s)) return nil } // https://sourcegraph.com/github.com/xitongsys/ethernet-go@master/-/blob/header/tcp.go func server(ch chan\u0026lt;- struct{}, wg *sync.WaitGroup) { ipaddr, _ := net.ResolveIPAddr(\u0026#34;ip4\u0026#34;, \u0026#34;127.0.0.2\u0026#34;) con, err := net.ListenIP(\u0026#34;ip4:6\u0026#34;, ipaddr) if err != nil { log.Fatalf(\u0026#34;tcp listening failed %+v\\n\u0026#34;, err) } buf := make([]byte, 1024) fmt.Printf(\u0026#34;开始接收%v\\n\u0026#34;, con.LocalAddr()) ch \u0026lt;- struct{}{} for range 3 { { n, from, _ := con.ReadFromIP(buf) if n \u0026gt; 20 { fmt.Printf(\u0026#34;[%s] for [%s]\\n\u0026#34;, string(buf[20:n]), from.String()) } else { fmt.Printf(\u0026#34;too short %d\\n\u0026#34;, n) } } } close(ch) con.Close() wg.Done() } const ( FIN = 0x01 SYN = 0x02 RST = 0x04 PSH = 0x08 ACK = 0x10 URG = 0x20 ECE = 0x40 CWR = 0x80 ) func BuildTcpHeader(src, dst uint32, sport, dport uint16, flag uint8, data []byte) []byte { tcpheader := \u0026amp;TCP{ SrcPort: sport, DstPort: dport, Seq: 1, Ack: 1, Offset: 0x50, Flags: flag, Win: ^uint16(0), Checksum: 0, UrgPointer: 0, } result := make([]byte, len(data)+20) copy(result, tcpheader.Marshal()) copy(result[20:], data) ReCalTcpCheckSum(result, src, dst) return result } func client(ch \u0026lt;-chan struct{}) { laddr, _ := net.ResolveIPAddr(\u0026#34;ip4\u0026#34;, \u0026#34;127.0.0.3\u0026#34;) raddr, _ := net.ResolveIPAddr(\u0026#34;ip4\u0026#34;, \u0026#34;127.0.0.2\u0026#34;) con, err := net.DialIP(\u0026#34;ip4:6\u0026#34;, laddr, raddr) if err != nil { log.Fatalf(\u0026#34;ip conn create failed %+v\\n\u0026#34;, err) } \u0026lt;-ch for i := range 3 { buf := BuildTcpHeader(binary.BigEndian.Uint32(laddr.IP.To4()), binary.BigEndian.Uint32(raddr.IP.To4()), 7742, 7743, SYN, []byte(fmt.Sprintf(\u0026#34;test data packet%d\u0026#34;, i))) con.Write(buf) } \u0026lt;-ch fmt.Println(\u0026#34;has write\u0026#34;) con.Close() } func main() { var wg sync.WaitGroup ch := make(chan struct{}) wg.Add(1) go server(ch, \u0026amp;wg) fmt.Println(\u0026#34;server start\u0026#34;) client(ch) fmt.Println(\u0026#34;client close\u0026#34;) wg.Wait() } 后期会补一些原理图上去\n","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/faketcp-%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D/","title":"faketcp 原理介绍"},{"content":"gRPC 使用手册 环境准备 grpc 是使用protobuf 协议的 需要安装对应的编译器\n1 2 go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28 go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2 定义好 .proto 文件之后可以使用 protoc 编译器来生成对应语言的代码\n1 2 protoc --go_out=./proto/ --go_opt=paths=source_relative --go-grpc_out=./proto/ --go-grpc_opt=paths=source_relative ./proto/your.proto 基础的流程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 // 单次调用 ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() resp, err := client.UnaryEcho(ctx, \u0026amp;ecpb.EchoRequest{Message: message}) if err != nil { log.Fatalf(\u0026#34;client.UnaryEcho(_) = _, %v: \u0026#34;, err) } // 流接收 func recvMessage(stream pb.Echo_BidirectionalStreamingEchoClient, wantErrCode codes.Code) { res, err := stream.Recv() if status.Code(err) != wantErrCode { log.Fatalf(\u0026#34;stream.Recv() = %v, %v; want _, status.Code(err)=%v\u0026#34;, res, err, wantErrCode) } if err != nil { fmt.Printf(\u0026#34;stream.Recv() returned expected error %v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;received message %q\\n\u0026#34;, res.GetMessage()) } // 在接受流的时候要验证 err 是不是EOF for { in, err := stream.Recv() if err != nil { fmt.Printf(\u0026#34;server: error receiving from stream: %v\\n\u0026#34;, err) if err == io.EOF { return nil } return err } fmt.Printf(\u0026#34;echoing message %q\\n\u0026#34;, in.Message) stream.Send(\u0026amp;pb.EchoResponse{Message: in.Message}) } OAuth token 验证 因为有两种rpc 调用 一种是 单次调用 一种是流式调用； 在客户端 client 建立连接时使用的opts 中使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // fetchToken 表示获取token 的动作,使用 tokensource 获取带时效时间的 token perRPC := oauth.TokenSource{TokenSource: oauth2.StaticTokenSource(fetchToken())} creds, err := credentials.NewClientTLSFromFile(data.Path(\u0026#34;x509/ca_cert.pem\u0026#34;), \u0026#34;x.test.example.com\u0026#34;) if err != nil { log.Fatalf(\u0026#34;failed to load credentials: %v\u0026#34;, err) } opts := []grpc.DialOption{ // In addition to the following grpc.DialOption, callers may also use // the grpc.CallOption grpc.PerRPCCredentials with the RPC invocation // itself. // See: https://godoc.org/google.golang.org/grpc#PerRPCCredentials grpc.WithPerRPCCredentials(perRPC), // oauth.TokenSource requires the configuration of transport // credentials. grpc.WithTransportCredentials(creds), } 在服务端则是要通过拦截器来分别处理两种 rpc 调用的验证\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // 流式的 验证 func ensureValidToken(ctx context.Context, req any, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (any, error) { md, ok := metadata.FromIncomingContext(ctx) if !ok { return nil, errMissingMetadata } // 下面这个是将客户端传的token 和服务器端的校验逻辑来比较 if !valid(md[\u0026#34;authorization\u0026#34;]) { return nil, errInvalidToken } // Continue execution of handler after ensuring a valid token. return handler(ctx, req) } cert, err := tls.LoadX509KeyPair(data.Path(\u0026#34;x509/server_cert.pem\u0026#34;), data.Path(\u0026#34;x509/server_key.pem\u0026#34;)) if err != nil { log.Fatalf(\u0026#34;failed to load key pair: %s\u0026#34;, err) } opts := []grpc.ServerOption{ grpc.UnaryInterceptor(ensureValidToken), // Enable TLS for all incoming connections. grpc.Creds(credentials.NewServerTLSFromCert(\u0026amp;cert)), } 这里也是可以用 go-grpc-middleware 提供的auth 中间件来实现验证函数的包装\n取消调用 取消调用里面要在用grpc 调用时传入上下文作为第一个参数来控制rpc 调用过程；\n1 2 3 4 5 6 7 ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) stream, err := c.BidirectionalStreamingEcho(ctx) if err != nil { log.Fatalf(\u0026#34;error creating stream: %v\u0026#34;, err) } cancel() // 此时已经取消任务了， 压缩请求 1 2 3 // 旧版本在 NewClient() 的时候传一个 grpc.WithCompressor(grpc.NewGZIPCompressor()) // 新版本需要在调用的时候传入 grpc.UseCompressor(gzip.Name) grpc限流 用go-grpc-middleware实现一个接口来在grpc 中间件里做限流，限流中间件必须排在后面，避免令牌被浪费了，使用原生的方式可以基于服务来做特定任务的限流； 在示例中用定时器触发模拟限流机制产生，当服务端调用阻塞的时候，退出后续的批量任务，\n请求失败重试策略配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 var retryPolicy = `{ \u0026#34;methodConfig\u0026#34;: [{ \u0026#34;name\u0026#34;: [{\u0026#34;service\u0026#34;: \u0026#34;grpc.examples.echo.Echo\u0026#34;}], //应用的服务 \u0026#34;waitForReady\u0026#34;: true,\t// 是否等待 \u0026#34;retryPolicy\u0026#34;: { \u0026#34;MaxAttempts\u0026#34;: 4, \u0026#34;InitialBackoff\u0026#34;: \u0026#34;.01s\u0026#34;, \u0026#34;MaxBackoff\u0026#34;: \u0026#34;.01s\u0026#34;, \u0026#34;BackoffMultiplier\u0026#34;: 1.0, \u0026#34;RetryableStatusCodes\u0026#34;: [ \u0026#34;UNAVAILABLE\u0026#34; ] } }]}` // use grpc.WithDefaultServiceConfig() to set service config func retryDial() (*grpc.ClientConn, error) { return grpc.NewClient(*addr, grpc.WithTransportCredentials(insecure.NewCredentials()), grpc.WithDefaultServiceConfig(retryPolicy)) } 等待对端恢复 1 2 // 在需要等待对端恢复服务的时候可以加入这个option grpc.WaitForReady(true) 携带元数据 这个元数据有点像 http 里面的 header 的作用，携带一些用于配置的的内容 客户端这边需要用\n1 2 3 4 5 6 metadata.Pairs(\u0026#34;timestamp\u0026#34;, time.Now().Format(timestampFormat)) // 来添加组装键值对，两个字符串作为一组，转换成一个KV对，键值对的键可以有重复的 ctx := metadata.NewOutgoingContext(context.Background(), md) // 然后 封装成一个上下文通过 grpc 调用传过去 var header, trailer metadata.MD r, err := c.UnaryEcho(ctx, \u0026amp;pb.EchoRequest{Message: message}, grpc.Header(\u0026amp;header), grpc.Trailer(\u0026amp;trailer)) // 这个 header 和 trailer 是 服务端对这个元数据做交互\n1 2 3 4 5 6 7 md, ok := metadata.FromIncomingContext(ctx) header := metadata.New(map[string]string{\u0026#34;location\u0026#34;: \u0026#34;MTV\u0026#34;, \u0026#34;timestamp\u0026#34;: time.Now().Format(timestampFormat)}) grpc.SendHeader(ctx, header) // 执行grpc 服务 // 下面逻辑要在defer 函数里面执行 trailer := metadata.Pairs(\u0026#34;timestamp\u0026#34;, time.Now().Format(timestampFormat)) grpc.SetTrailer(ctx, trailer) 感觉可以用来做rpc 调用的时延监控，或者调用前后状态的跟踪点\ngrpc 长连接保活 1 2 3 4 5 6 7 var kacp = keepalive.ClientParameters{ Time: 10 * time.Second, // send pings every 10 seconds if there is no activity Timeout: time.Second, // wait 1 second for ping ack before considering the connection dead PermitWithoutStream: true, // send pings even without active streams } // 新建客户端时带上这个 grpc.DialOption conn, err := grpc.NewClient(*addr, grpc.WithTransportCredentials(insecure.NewCredentials()), grpc.WithKeepaliveParams(kacp)) 负载平衡 默认的连接构建策略是 使用首个配置构建两件，如果需要使用负载平衡机制\n1 2 3 4 5 6 // 使用轮转策略 roundrobinConn, err := grpc.NewClient( fmt.Sprintf(\u0026#34;%s:///%s\u0026#34;, exampleScheme, exampleServiceName), grpc.WithDefaultServiceConfig(`{\u0026#34;loadBalancingConfig\u0026#34;: [{\u0026#34;round_robin\u0026#34;:{}}]}`), // This sets the initial balancing policy. grpc.WithTransportCredentials(insecure.NewCredentials()), ) ","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/grpc-%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/","title":"gRPC 使用手册"},{"content":"llm 知识抽取管线 利用prompt模板，从文档中抽取出知识，并控制输出格式，完成知识抽取管线\nprompt模板 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 KE_PROMPT= \u0026#34;\u0026#34;\u0026#34;你现在是一个用于抽取结构化信息的知识抽取模型，请按照遵守下面的步骤提取结构化的实体关系: - 步骤 - 1. 识别所有在实体类型列表:{node_lists}中给出类型的实体，提取以下信息，同时保持实体一致性： - name 实体名称，尽量简单明确，不要包含多余信息； - type 实体类型，必须是实体类型列表中给出的类型； - desc 实体描述，可以实体属性和相关活动的描述； 将每个实体输出为json格式，其格式如下，键值内容不要包含单双引号，格式如下： {{\u0026#34;name\u0026#34;:\u0026#34;\u0026lt;实体名称\u0026gt;\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;\u0026lt;实体类型\u0026gt;\u0026#34;,\u0026#34;desc\u0026#34;:\u0026#34;\u0026lt;实体描述\u0026gt;\u0026#34;}}； 2. 针对步骤1中获取的实体，识别所有在关系类型列表:{relation_lists}中给出类型的关系，提取以下信息： - src 源实体的名称，即步骤1中标识的name - dst 目标实体的名称，即步骤1中标识 - rel 关系类型，必须是关系类型列表中给出的类型； - rel_desc 说明源实体和目的实体存在实体关系的原因 将每个关系输出转化成以下的json格式:格式如下: {{\u0026#34;src\u0026#34;:\u0026#34;\u0026lt;源实体名称\u0026gt;\u0026#34;,\u0026#34;dst\u0026#34;:\u0026#34;\u0026lt;目标实体名称\u0026gt;\u0026#34;,\u0026#34;rel\u0026#34;:\u0026#34;\u0026lt;关系类型\u0026gt;\u0026#34;,\u0026#34;rel_desc\u0026#34;:\u0026#34;\u0026lt;关系描述\u0026gt;\u0026#34;\u0026#34;}} 3. 请保证按照上述规则输出，不要输出其他内容。 - 真实数据 - ############# {text} ############# 输出:\u0026#34;\u0026#34;\u0026#34; def main(): node_lists = [\u0026#34;人物\u0026#34;, \u0026#34;地点\u0026#34;, \u0026#34;组织\u0026#34;,\u0026#34;事件\u0026#34;] relation_lists = [\u0026#34;位于\u0026#34;, \u0026#34;就职于\u0026#34;,\u0026#34;发生了\u0026#34;,\u0026#34;谈论\u0026#34;] test = \u0026#34;昨天实验室的牛师兄带着常师哥去面馆吃了八十八碗面，然后谈论面上项目的一些筹划，准备结合从所在的大数据实验室的重点方向挖掘项目创新点\u0026#34; print(KE_PROMPT.format(node_lists=node_lists,relation_lists=relation_lists,text=test)) if __name__ == \u0026#39;__main__\u0026#39;: main() openai 或者gpt 大模型http 非流式调用 这部分可以参照之前的那个chatgpt 桌面版调用不同厂商的sdk 接口，用来组装成流水线\n","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/llm-%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96%E7%AE%A1%E7%BA%BF/","title":"llm 知识抽取管线"},{"content":"Prometheus exporter pprof 优化 这个工程是 基于 Prometheus client-go 的库来开发的，这个库的主要流程是将 collector 的数据用http server的形式通过metrics 路由交出去，背景状况是现在这个收集器的收集间隔很长基本上是6个小时更新一次，但是负责抓取这些指标数据的服务leader 又不同意改成6小时的大跨度，所以之前的措施是把硬件采集的数据缓存起来，但是更新后的版本cpu占用率和响应时间都没太大优化，需要定位这个问题\n怎么采集后台运行进程性能数据 使用ps -ef | grep xxx 来获取进程号pid pidstat -u -p pid 15 4 来采集进程的cpu 占用率 top 按t 和m 来切换 cpu 和 内存排序 go 使用 _ \u0026ldquo;net/http/pprof\u0026rdquo; 然后非http 网络服务就再增加一个拉起http 服务的几行代码 使用curl -o cpupgo.out http://your_address:your_port/debug/pprof/profile?seconds=60 来采样一分钟的运行数据 go tool pprof -http=:9000 cpupgo.out 使用 pprof 工具开启一个网络服务在web网页上查看性能采样 使用 perf 工具来采样 基本上会用到 record 和 report 这些，然后转成火山图来分析 问题追踪 问题1. 缓存为什么没有生效(降低延迟减少耗时操作) 通过go 的pprof 对后台运行的服务采样后发现大部分cpu 时间在生成Metric 相关的结构体上，同时缓存的数据格式是json，取json 数据会用到仿射，这使得组装Metric 的过程中充斥着大量的耗时操作，于是选择将json 数据缓存改成 Metric 数据缓冲，和时效时间戳一起封装成一个抽象的容器，在未过期时会将，slice 里面所有的Metric 通过管道发送出去，过期时会将slice 长度重置，发送Metric 的同时将其append 到slice 里面缓存，总结来说，缓存生效了，但是又没完全覆盖到所有耗时操作上。\n改进后的缓存实现，在应用中遇到了新的问题，缓存应用后没被触发？\n问题2. 缓存为什么没被触发？ 复盘对比了两种缓存机制和Collect 方法被调用的过程时发现，Collect 中声明的对象在每次调用时重新创建的，之前json 缓存是用的全局变量，所以创建前后用的都是一个缓存；这里将新的缓存实现也没大改，给新建的这些对象实例也做个全局缓存，没过期失效前这些实例就不会被重新创建，减少了一些再分配构建的过程，通过预留的 cache stat handler 可以看到缓存除了初次和过期时未命中外，其余时刻缓存全命中，符合预期\n新的全局缓冲实现生效了，将cpu 占用率降低到原先的30%，同时内存占用差别不大，但剩下的30%还能不能继续优化呢？\n终极优化方案 通过对 Prometheus client go 的源码阅读，确定了相应http 响应的整个构造流程，脑中浮现了一个比较极端的想法，缓冲响应；\n这个适合用来缓存响应内容在一段时间内不会发生改动的 http handler 接口对象\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 /* get resp cache code start */ type respCacheWriter struct { header http.Header expireat time.Time statuscode int buf []byte update bool } func newRespCacheWriter() *respCacheWriter { return \u0026amp;respCacheWriter{ header: make(http.Header, 3), buf: make([]byte, 0, 1024*4), } } func (r *respCacheWriter) NotExpire() bool { return time.Now().Before(r.expireat)\u0026amp;\u0026amp;!r.update } func (r *respCacheWriter) Update(interval time.Duration) { r.expireat = time.Now().Add(interval) r.update = false } func (r *respCacheWriter) Header() http.Header { return r.header } func (r *respCacheWriter) SetUpdate() http.Header { return r.update = true } func (r *respCacheWriter) WriteHeader(statusCode int) { r.statuscode = statusCode } func (r *respCacheWriter) Write(p []byte) (int, error) { if p == nil { return 0, fmt.Errorf(\u0026#34;Write []byte length should not be zero\u0026#34;) } r.buf = append(r.buf, p...) return len(r.buf), nil } type GetRespCache struct { interval time.Duration cache map[string]*respCacheWriter next http.Handler } func NewGetRespCache(i time.Duration,next http.Handler)GetRespCache{ return GetRespCache{ interval: i, cache: make(map[string]*respCacheWriter,1), next: next, } } // 这个方法其实也可以 转成私有的，但是不会修改cache 状态所以无所谓 func (g *GetRespCache)UpdateCache()bool{ key := fmt.Sprintf(\u0026#34;key%v\u0026#34;, r.URL.Query()) if c,ok:=g.cache[key];key\u0026amp;\u0026amp;c!=nil{ c.SetUpdate() } } func (g *GetRespCache) ServeHTTP(w http.ResponseWriter, r *http.Request) { key := fmt.Sprintf(\u0026#34;key%v\u0026#34;, r.URL.Query()) if val, ok := g.cache[key]; ok \u0026amp;\u0026amp; val != nil\u0026amp;\u0026amp;val.NotExpire() { g.generateResp(val, w) } else { // 调用 write 方法时 缓存响应的 header resp := newRespCacheWriter() g.next.ServeHTTP(resp, r) g.generateResp(resp, w) g.update(key, resp) } } func (g *GetRespCache) update(key string, resp *respCacheWriter) { // 这里会更新过期时间和下一个响应状态 resp.Update(g.interval) g.cache[key] = resp } func (*GetRespCache) generateResp(val *respCacheWriter, w http.ResponseWriter) { for k, s := range val.header { for _, v := range s { w.Header().Set(k, v) } } w.WriteHeader(val.statuscode) w.Write(val.buf) } /* get resp cache code end */ 可以看到其实就是通过中间的代理接口，将被代理的handler 函数的修改缓存起来，根据get 请求的query值来做hash 返回响应的； 过期的时效间隔这里倒是比较粗，用的是同一个过期间隔；\n这个get 缓存方案是我最看好的:\n第一点基本上是即插即用，迁移性好兼容性好， 第二点是性能更好，缓存占用少，还剩去了内部handler 处理的时间 这个之所以能用在这个场景上，其实是需求造成的，抓取端不改动，数据供应端又允许缓存；所以这个getcache 的方案理论上有奇效，但是最终还是没应用上这个，确定是按照问题2解决后的实现方案来。\n","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/prometheus-exporter-pprof-%E4%BC%98%E5%8C%96/","title":"Prometheus exporter pprof 优化"},{"content":"TailReader 一个反向迭代器 面对大文本获取最后的消息，向前遍历go 目前没有现成的接口\n设计思路 seek 反向移动offset ，然后bytes 判断不同系统上的换行符在哪里？ 将一次分割后的字节数组，缓存起来，留待下次取分行字节 在多次未能查看到换行符的时候，默认是3KB 就提前终止提交失败，避免因为错误遍历大二进制文件 有缓存机制，大小文件效率都不差，适合做类似 tail -n 3 这种文本获取行为 目前支持的 empty 行跳过行为比较简单，其实更接近于剔除前缀换行符, 如果需要跳过空行，可以嵌套个if 判断len(temp)\u0026gt;0 面对极小文件可以直接用ReadFile 来切split，避免带来额外的复杂度 实现细节 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 type TailReader struct { rc *os.File buf []byte // 用来缓存剩余字节 temp []byte // 提供给 Read sep []byte // 兼容不同系统架构分隔符 offset int64 // 记录offset size int64 // 文件大小 skipempty bool // 控制是否跳过空行行为 atEnd bool // 记录offset 是否被移动到文件开始位置了 } var ( Sep_win = []byte(\u0026#34;\\r\\n\u0026#34;) Sep_linux = []byte(\u0026#34;\\n\u0026#34;) ) func NewTailReader(fname string, sep []byte, skip bool) (*TailReader, error) { file, err := os.Open(fname) if err != nil { return nil, err } stat, _ := file.Stat() size := stat.Size() var offset int64 = 1024 if size \u0026lt; offset { offset = size } _, errs := file.Seek(int64(-offset), 2) if errs != nil { return nil, errs } offset2, _ := file.Seek(0, io.SeekCurrent) fmt.Printf(\u0026#34;seek to offset %d, file size is %d\\n\u0026#34;, offset2, size) atEnd := false if offset == size { atEnd = true } return \u0026amp;TailReader{rc: file, buf: make([]byte, 0, 1024), temp: make([]byte, 1024), sep: sep, skipempty: skip, offset: int64(offset), size: size, atEnd: atEnd}, nil } func (t *TailReader) Close() { t.rc.Close() } func (t *TailReader) ReadBytes() ([]byte, error) { // 如果上次缓存没清完，检查是否有换行符 sepsize := 0 if t.skipempty { sepsize = len(t.sep) } // 处理上次遗留的缓存 if len(t.buf) \u0026gt; 0 { if idx := bytes.LastIndex(t.buf, t.sep); idx != -1 { temp := append([]byte{}, t.buf[idx+sepsize:]...) t.buf = t.buf[:idx] return temp, nil } if t.atEnd { p := slices.Clone(t.buf[:len(t.buf)]) t.buf = t.buf[:0] return p, nil } } if t.size \u0026lt; t.offset { return nil, io.EOF } // 拷贝重置缓存 var p []byte // 先将这部分尾巴给卸除出去 if len(t.buf) \u0026gt; 0 { p = append([]byte{}, t.buf...) } n, err := t.rc.Read(t.temp) if err == nil \u0026amp;\u0026amp; n \u0026gt; 0 { idx := bytes.LastIndex(t.temp[:n], t.sep) if idx != -1 { temp := append([]byte{}, t.temp[idx+sepsize:n]...) temp = append(temp, p...) t.buf = t.buf[:0] t.buf = append(t.buf, t.temp[:idx]...) if err := t.move(n); err != nil { return nil, err } return temp, nil } var cur, next []byte cur = slices.Concat(t.temp[:n], p) if err := t.move(n); err != nil { return nil, err } // 用来预防二进制大文件，堆爆slice for i := 0; i \u0026lt; 3 \u0026amp;\u0026amp; idx == -1; i++ { n, err = t.rc.ReadAt(t.buf, 0) if err != nil { return nil, err } if err := t.move(n); err != nil { return nil, err } idx = bytes.LastIndex(t.buf[:n], t.sep) if idx != -1 { next = slices.Concat(t.temp[idx:n], cur) // 尽量复用 t.buf = t.buf[:0] t.buf = append(t.buf, t.temp[:idx]...) break } next = slices.Concat(t.temp[:n], cur) } if idx != -1 { return nil, errors.New(\u0026#34;cant found sep in many times try\u0026#34;) } return next, nil } return nil, err } func (t *TailReader) move(delta int) error { t.offset += int64(delta) if t.offset \u0026gt; t.size { t.offset = t.size } // 避免重复移动 if t.offset \u0026lt;= t.size \u0026amp;\u0026amp; !t.atEnd { _, err := t.rc.Seek(-t.offset, 2) if err != nil { return err } if t.offset == t.size { t.atEnd = true } } if len(t.buf) \u0026gt; 0 { return nil } return io.EOF } 简单使用示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 f, err := NewTailReader(`test.txt`, utils.Sep_win, true) if err != nil { fmt.Printf(\u0026#34;error %s\u0026#34;, err.Error()) } defer f.Close() for { b, e := f.ReadBytes() // 一般是 io.EOF if e != nil { break } fmt.Printf(\u0026#34;%s\\t%v\\n\u0026#34;, string(b), e) } 一般用来读取尾部行，不太适合对日志这种增长文件去重复读取\n","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/tailreader-%E4%B8%80%E4%B8%AA%E5%8F%8D%E5%90%91%E8%BF%AD%E4%BB%A3%E5%99%A8/","title":"TailReader 一个反向迭代器"},{"content":"TLS 和 H2 配置 TLS 证书签发流程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 下面是 v3.ext 的内容 # authorityKeyIdentifier=keyid,issuer # basicConstraints=CA:FALSE # keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment # extendedKeyUsage = serverAuth # subjectAltName = @alt_names # [alt_names] # DNS.1=localhost # DNS.2=10.110.8.36 openssl genrsa \u0026gt; serve.key openssl req -new -key serve.key -out serve.csr -subj \u0026#34;/C=GB/L=China/O=hx/CN=localhost\u0026#34; -days 365 -addext \u0026#34;subjectAltName = DNS:localhost\u0026#34; # 个人感觉 这边加个 -addext \u0026#34;subjectAltName = DNS:localhost\u0026#34; 也可以替代 extfile openssl x509 -req -sha512 -days 365 -extfile v3.ext -signkey serve.key -in serve.csr -out serve.crt 三个文件：\nserve.key 私钥 serve.csr 公钥 serve.crt 给客户端用来验证服务的签名证书 代码实现TLS 和H2 服务端这边\n1 2 3 4 5 6 7 http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, req *http.Request) { io.WriteString(w, \u0026#34;hello, world!\\n\u0026#34;) }) // 这里要指向两个文件的路径 if e := http.ListenAndServeTLS(\u0026#34;:443\u0026#34;, \u0026#34;serve.crt\u0026#34;, \u0026#34;serve.key\u0026#34;, nil); e != nil { log.Fatal(\u0026#34;ListenAndServe: \u0026#34;, e) } 客户端这边\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 package main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;golang.org/x/net/http2\u0026#34; ) func loadCA(caFile string) *x509.CertPool { pool := x509.NewCertPool() if ca, e := os.ReadFile(caFile); e != nil { log.Fatal(\u0026#34;ReadFile: \u0026#34;, e) } else { pool.AppendCertsFromPEM(ca) } return pool } func main() { c := \u0026amp;http.Client{ // 这个会拿到 h2的协议 Transport: \u0026amp;http2.Transport{ TLSClientConfig: \u0026amp;tls.Config{RootCAs: loadCA(\u0026#34;../serve.crt\u0026#34;)}, AllowHTTP: true, }, // 启用下面这个注释的就是 https 的协议 // Transport: \u0026amp;http.Transport{ // TLSClientConfig: \u0026amp;tls.Config{RootCAs: loadCA(\u0026#34;../serve.crt\u0026#34;)}, // }, } if resp, e := c.Get(\u0026#34;https://localhost:443/\u0026#34;); e != nil { log.Fatal(\u0026#34;http.Client.Get: \u0026#34;, e) } else { defer resp.Body.Close() io.Copy(os.Stdout, resp.Body) fmt.Printf(\u0026#34;Got response %d: %s \\n\u0026#34;, resp.StatusCode, resp.Proto) } } ","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/tls-%E5%92%8C-h2-%E9%85%8D%E7%BD%AE/","title":"TLS 和 H2 配置"},{"content":"分布式一致性哈希 Golang 实现 哈希环定义 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import \u0026#34;hash/crc32\u0026#34; type Hash func(data []byte) uint32 // 用crc32 var defaultHashFn = crc32.ChecksumIEEE // 哈希环 // 注意，非线程安全，业务需要自行加锁 type HashRing struct { hash Hash // 每个真实节点的虚拟节点数量 replicas int // 哈希环，按照节点哈希值排序 ring []int // 用来在新增节点时去重的 keys []string // 节点哈希值到真实节点字符串，哈希映射的逆过程 nodes map[int]string } func NewHashRing(r int,fn Hash)*HashRing(){ if fn==nil{ fn = defaultHashFn } return \u0026amp;HashRing{ replicas: r, hash: fn, ring: make([]int,0,8*r), keys: make([]string,0,8), nodes: make(map[int]string,8), } } 方法实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import ( \u0026#34;slices\u0026#34; \u0026#34;sort\u0026#34; \u0026#34;strconv\u0026#34; ) func (h *HashRing) Add(nodes ...string) { for _, k := range nodes { exist := false for _, val := range h.keys { if val == k { exist = true } } if !exist { h.keys = append(h.keys, k) for i := 0; i \u0026lt; h.replicas; i++ { hash := int(h.hash([]byte(strconv.Itoa(i) + k))) h.ring = append(h.ring, hash) h.nodes[hash] = k } } } slices.Sort(h.ring) } func (h *HashRing) Len() int { return len(h.keys) } func (h *HashRing) Get(key string) string { if h.Len() == 0 { return \u0026#34;\u0026#34; } hash := int(h.hash([]byte(key))) // Binary search for appropriate replica. idx := sort.Search(len(h.ring), func(i int) bool { return h.ring[i] \u0026gt;= hash }) // Means we have cycled back to the first replica. if idx == len(h.ring) { idx = 0 } return h.nodes[h.ring[idx]] } func (h *HashRing)Rest(){ h.ring = h.ring[:0] h.keys = h.keys[:0] clear(h.nodes) } Add() 方法是用来创建节点环的,在一些节点退出后，可能需要reset 哈希环来再次重构哈希环 Reset() 是用来重置hash环信息的 Get() 就是用来获取 一致性哈希映射到的 节点的字符串上 ","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7hash-golang-%E5%AE%9E%E7%8E%B0/","title":"分布式一致性hash golang 实现"},{"content":"矢量化扩展 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 #include \u0026lt;array\u0026gt; #include \u0026lt;chrono\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026#34;boost/core/noncopyable.hpp\u0026#34; using std::chrono::duration_cast; using std::chrono::high_resolution_clock; class BenchTimer : boost::noncopyable { high_resolution_clock::time_point tp; std::string name; std::vector\u0026lt;std::string\u0026gt; rd; std::vector\u0026lt;double\u0026gt; dt; public: inline void st(const std::string \u0026amp;name) { this-\u0026gt;name = name; tp = high_resolution_clock::now(); } inline void end() { dt.emplace_back(duration_cast\u0026lt;std::chrono::nanoseconds\u0026gt;(high_resolution_clock::now() - tp).count()); rd.emplace_back(this-\u0026gt;name); } void showBenchTest() { for (int i = 0, size = static_cast\u0026lt;int\u0026gt;(rd.size()); i \u0026lt; size; i++) { std::cout \u0026lt;\u0026lt; rd[i] \u0026lt;\u0026lt; \u0026#34;:\u0026#34; \u0026lt;\u0026lt; dt[i] / 1000 \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } } }; int main() { std::vector\u0026lt;int\u0026gt; arr_a(256, 0); std::vector\u0026lt;int\u0026gt; arr_b(256, 100); typedef int int256 __attribute__((vector_size(256 * sizeof(int)))); int256 a; int256 b; for (int i = 0; i \u0026lt; 256; i++) { arr_a[i] = i; a[i] = i; arr_b[i] = 256 - i; b[i] = 256 - i; } int t = 10000; auto bench = BenchTimer(); #pragma clang optimize off bench.st(\u0026#34;foreach\u0026#34;); while (t \u0026gt; 0) { for (auto i = 0; i \u0026lt; 256; i++) { arr_a[i] = 2 * arr_b[i]; } t--; } bench.end(); t = 10000; bench.st(\u0026#34;vec\u0026#34;); while (t \u0026gt; 0) { a = 2 * b; t--; } bench.end(); #pragma clang optimize on bench.showBenchTest(); std::cout \u0026lt;\u0026lt; \u0026#34;2*b[0]==\u0026#34; \u0026lt;\u0026lt; a[0] \u0026lt;\u0026lt; std::endl; } bench测试结果 foreach:673.4 vec:4.8 2*b[0]==512\n按照600/5 也有120倍的加速差距了，据我观察这种大轮次的外部循环可能对vec 的缓存hit更友好点，总之是有优化效果的\n","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/%E7%9F%A2%E9%87%8F%E5%8C%96%E6%89%A9%E5%B1%95/","title":"矢量化扩展"},{"content":"https://\n文件多格式压缩 使用了一个压缩集合库来实现的这部分功能 compress\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 package kit import ( \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;syscall\u0026#34; gzip \u0026#34;github.com/klauspost/compress/gzip\u0026#34; snappy \u0026#34;github.com/klauspost/compress/s2\u0026#34; zip \u0026#34;github.com/klauspost/compress/zip\u0026#34; zstd \u0026#34;github.com/klauspost/compress/zstd\u0026#34; ) /* compress file func start */ // go build:+ linux var osChown = os.Chown type ( CompressType int ZWriter interface { io.WriteCloser Flush() error } ) const ( GZIP_TYEP CompressType = iota ZSTD_TYPE SNAPPY_TYPE ZIP_TYPE ) var suffixs = [4]string{\u0026#34;.gz\u0026#34;, \u0026#34;.zst\u0026#34;, \u0026#34;.snappy\u0026#34;, \u0026#34;.zip\u0026#34;} func chown(name string, info os.FileInfo) error { f, err := os.OpenFile(name, os.O_CREATE|os.O_WRONLY|os.O_TRUNC, info.Mode()) if err != nil { return err } f.Close() stat := info.Sys().(*syscall.Stat_t) return osChown(name, int(stat.Uid), int(stat.Gid)) } func CompressLogFile(src, dst string, ct CompressType, rm bool) (err error) { f, err := os.Open(src) if err != nil { return fmt.Errorf(\u0026#34;failed to open log file: %v\u0026#34;, err) } defer f.Close() if dst == src { dst += \u0026#34;out\u0026#34; } fi, err := os.Stat(src) if err != nil { return fmt.Errorf(\u0026#34;failed to stat log file: %v\u0026#34;, err) } if !strings.HasSuffix(dst, suffixs[ct]) { dst += suffixs[ct] } if err := chown(dst, fi); err != nil { return fmt.Errorf(\u0026#34;failed to chown compressed log file: %v\u0026#34;, err) } // If this file already exists, we presume it was created by // a previous attempt to compress the log file. zf, err := os.OpenFile(dst, os.O_CREATE|os.O_TRUNC|os.O_WRONLY, fi.Mode()) if err != nil { return fmt.Errorf(\u0026#34;failed to open compressed log file: %v\u0026#34;, err) } defer zf.Close() if ct == ZIP_TYPE { w := zip.NewWriter(zf) _, filename := filepath.Split(src) temp, err := w.Create(filename) if err != nil { return err } if _, err := io.Copy(temp, f); err != nil { w.Close() return err } if err := w.Close(); err != nil { return err } } else { var zwriter ZWriter switch ct { case GZIP_TYEP: zwriter = gzip.NewWriter(zf) case SNAPPY_TYPE: zwriter = snappy.NewWriter(zf, snappy.WriterSnappyCompat()) case ZSTD_TYPE: zwriter, _ = zstd.NewWriter(zf) } defer func() { if err != nil { os.Remove(dst) err = fmt.Errorf(\u0026#34;failed to compress log file: %v\u0026#34;, err) } }() if n, err := io.Copy(zwriter, f); err != nil { zwriter.Close() return err } else { fmt.Printf(\u0026#34;debug %d byets has beed write to dst path %s\u0026#34;, n, dst) } zwriter.Flush() if err := zwriter.Close(); err != nil { return err } } if err := zf.Close(); err != nil { return err } if err := f.Close(); err != nil { return err } if rm { if err := os.Remove(src); err != nil { return err } } return nil } 这部分主要是通过枚举加分支逻辑来处理不同压缩格式的处理，通过io.Copy 来做数据管道\n","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/%E6%96%87%E4%BB%B6%E5%A4%9A%E6%A0%BC%E5%BC%8F%E5%8E%8B%E7%BC%A9%E5%B7%A5%E5%85%B7rust%E5%AE%9E%E7%8E%B0/","title":"文件多格式压缩工具rust实现"},{"content":"消息订阅实现 原理 go 的消息管道可以看成一个并发安全的队列，每个订阅者将字节的收信队列添加到SubHub 里面，按照订阅的topic 和 channel 的关系用一个 map[string]chan 来实现关联，当Hub 接受到对应topic 的消息推送的时候，会给slice 里面的收信队列发事件消息\n实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 import ( \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) type HEvent struct { Data interface{} Topic string } type HEventData chan HEvent type HEventDataArray []HEventData //一个topic 可以有多个消费者 type HEventBus struct { sub map[string]HEventDataArray rm sync.RWMutex } func HEventSrv() *HEventBus { return h } func (h *HEventBus) Sub(topic string, ch HEventData) { h.rm.Lock() if chanEvent, ok := h.sub[topic]; ok { h.sub[topic] = append(chanEvent, ch) } else { h.sub[topic] = append([]HEventData{}, ch) } defer h.rm.Unlock() } func (h *HEventBus) Push(topic string, data interface{}) { h.rm.RLock() defer h.rm.RUnlock() if chanEvent, ok := h.sub[topic]; ok { for _, ch := range chanEvent { ch \u0026lt;- HEvent{ Data: data, Topic: topic, } } } } func (h *HEventBus) PushFullDrop(topic string, data interface{}) { h.rm.RLock() defer h.rm.RUnlock() if chanEvent, ok := h.sub[topic]; ok { for _, ch := range chanEvent { select { case ch \u0026lt;- HEvent{ Data: data, Topic: topic, }: case \u0026lt;-time.After(time.Second): } } } } ","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/%E6%B6%88%E6%81%AF%E8%AE%A2%E9%98%85%E5%AE%9E%E7%8E%B0/","title":"消息订阅实现"},{"content":"雪花算法实现 雪花算法 0 - 0000000000 0000000000 0000000000 0000000000 0 - 0000000000 - 000000000000\n符号位 时间戳 机器码 序列号\n41位毫秒级时间戳，10位机器id 12位序列号 这个起始时间sEpoch其实不太重要，因为一般业务不会超过10年，这个69-(10~20)总还有50年的余量的，比较重要的一点就是如果一个毫秒内生产的序列id 过多就得等待到下一个毫秒窗口内\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 package utils // https://sourcegraph.com/github.com/sohaha/zlsgo/-/blob/zstring/snowflake.go // 学习 zlsgo的总结 import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) const ( sEpoch = 1474802888000 // 这个可以是业务上线的时间 TimeBase = int64(1000000) MaxWorkID = 1\u0026lt;\u0026lt;5 - 1 MaxDataID = MaxWorkID MaxSeqID = 1\u0026lt;\u0026lt;12 - 1 MaskWorkID = 1\u0026lt;\u0026lt;10 - 1 MaskSeqID = 1\u0026lt;\u0026lt;12 - 1 ) type SnowFlakeBuilder struct { start int64 workid int64 seqid int64 lock sync.Mutex } func NewBuilder(st time.Time, data int, worker int) *SnowFlakeBuilder { if data \u0026lt; 0 || data \u0026gt; MaxDataID || worker \u0026lt; 0 || worker \u0026gt; MaxWorkID || st.After(time.Now()) { panic(\u0026#34;invaild argument error\u0026#34;) } return \u0026amp;SnowFlakeBuilder{ start: st.UnixNano() / TimeBase, workid: int64(data\u0026lt;\u0026lt;5|worker) \u0026amp; MaskWorkID, } } // 类似自旋锁的逻辑，持续试图获取下一毫秒的时间资源 func (s *SnowFlakeBuilder) waitToNextMS(last int64) int64 { ts := time.Now().UnixNano() / TimeBase for { if ts \u0026lt;= last { ts = time.Now().UnixNano() / TimeBase } else { break } } return ts } func (s *SnowFlakeBuilder) GetID() (int64, error) { s.lock.Lock() defer s.lock.Unlock() ts := time.Now().UnixNano() / TimeBase if ts == s.start { s.seqid = (s.seqid + 1) \u0026amp; MaskSeqID if s.seqid == 0 { ts = s.waitToNextMS(ts) } } else { s.seqid = 0 } if ts \u0026lt; s.start { return 0, fmt.Errorf(\u0026#34;clock moved backwards, refuse gen id\u0026#34;) } s.start = ts ts = (ts-sEpoch)\u0026lt;\u0026lt;22 | s.workid\u0026lt;\u0026lt;12 | s.seqid return ts, nil } ","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/","title":"雪花算法实现"},{"content":"一个gptapi 桌面套件 使用了pyside6 和qfluentwidget 做的界面，qfluent 主要是用了侧边导航栏，后期考虑去掉这个依赖进一步减少编译后执行文件大小。 chatdesk 桌面套件连接 功能 markdown 自动导出 国产api {千问，豆包，混元，千帆，DeepSeek，Kimi} 支持多轮对话，但是暂时没做单轮多轮的动态切换 支持api 厂商动态切换，左下角label 显示当前使用的api后端 work线程实现异步读取响应流，但是考虑到防误触重发的问题，读取完成前，发送按钮不能再次点击 支持静态模型切换，目前配置文件存于工作目录，比使用环境变量更安全些 实验特性，多api 同时响应回答，目前没对多轮会话做支持； 主页面动态增长对话框 这边没有完全考虑仿照qq 那种一左一右的对话布局，因为比较麻烦；主要是在Scrollarea可滚动区域里面，添加重写的文本框；文本框在内容更新和变动时会自动适应内容大小，固定宽度；文本框超出滚动区域可视高度时会调整整个滚动区域的最小高度，避免压缩文本框；\nTODO： 因为会根据滚动区域里面文本框的累积高度来调整滚动区域最小高度，避免压缩文本框，会在特定情况下发生侧边滚动条调变，后期可能考虑优化； 给模型切换设计更加灵活的触发方式； 支持历史记忆管理，增强多轮会话的关联性； 支持一些链模式，或者说对内容做多轮归纳增强；\n会话窗口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 # 一个单线程的异步执行者 class MyThead(QThread): def __init__(self,parent): super().__init__(parent) self.queue =Queue(10) self.exist = False def addtask(self,fn): self.queue.put(fn) def stop(self): self.exist =True self.queue.put(lambda: logging.debug(\u0026#34;线程shutdown\u0026#34;) ) def run(self): while not self.exist: try: fn =self.queue.get(timeout=1) if fn is None: break if self.exist: break try: fn() except Exception as e: logging.debug(f\u0026#34;线程内部执行错误{e=}\u0026#34;) break except Empty: if self.exist: break continue # 简单的 对话界面窗口 class Flow(QWidget): def __init__(self): super().__init__() self.layout_ = QVBoxLayout(self) self.layout_.setAlignment(Qt.AlignmentFlag.AlignTop) self.setLayout(self.layout_) def addWidget(self,w:QWidget): self.layout_.addWidget(w) class TextArea(QTextEdit): def __init__(self,parent): super().__init__() self.setReadOnly(True) self.setVerticalScrollBarPolicy(Qt.ScrollBarPolicy.ScrollBarAlwaysOff) self.m_txtDefaultHeight = 50 self.setMaximumHeight(50) self.setMinimumHeight(50) self.top,self.bottom = self.contentsMargins().top(), self.contentsMargins().bottom() self.ctx = \u0026#34;\u0026#34; self.pa = parent # 隐藏时间戳，后续可能会用到类似的注释 self.ud = f\u0026#34;\u0026lt;!-- TS:{datetime.now().strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;)} --\u0026gt;\u0026#34; self.ds = f\u0026#34;{datetime.now().strftime(\u0026#39;%Y_%m_%d\u0026#39;)}\u0026#34; self.saved = False self.callback :Callable[[float],int] self.lastH = -1.0 def insertMarkdown(self,s:str): self.ctx +=s self.setMarkdown(self.ctx) self.checkResize() def regist(self,callback:Callable[[float],int] ): self.callback = callback # 适应窗口变动 @Slot() def checkResize(self): # print(\u0026#34;更新变动\u0026#34;) h = self.document().size().height() outer =-1 if h\u0026gt;max(self.lastH,outer): self.lastH = h outer =self.callback(h+self.top+self.bottom) if h\u0026gt;self.m_txtDefaultHeight: self.m_txtDefaultHeight = math.floor(h) self.setMaximumHeight(math.floor(h)) self.setMinimumHeight(math.floor(h)) rect = self.geometry() self.setGeometry(rect.x(),rect.y(),rect.width(),math.floor(h)+self.top+self.bottom) # 导出md 文件 @Slot() def export2File(self): if self.saved: return with open(f\u0026#34;./chart_{self.ds}.md\u0026#34;,mode=\u0026#34;a+\u0026#34;,encoding=\u0026#34;utf8\u0026#34;) as fw: size = self.ctx.split(\u0026#34;-----\u0026#34;,maxsplit=1) if len(size) \u0026gt;1 and len(size[1])\u0026gt;0: fw.write(self.ctx) fw.write(f\u0026#34;\\n{self.ud}\\n\u0026#34;) self.saved =True def wheelEvent(self,e :QWheelEvent): self.pa.wheelEvent(e) def resizeEvent(self, e: QResizeEvent) -\u0026gt; None: self.document().setTextWidth(self.viewport().width()) margins = self.contentsMargins() height = int(self.document().size().height() + margins.top() + margins.bottom()) self.setMaximumHeight(height) self.setMinimumHeight(height) class MyApp(QWidget): sig = Signal(str) def __init__(self, *args,**kwargs) -\u0026gt; None: super().__init__(*args,**kwargs) self.setWindowTitle(\u0026#34;PYSIDE6 demo\u0026#34;) self.id =0 self.gid =0 self.resize(800,450) self.m_end =0 self.set_ui() self.ctx =\u0026#34;\u0026#34; # 这部分还可以整理一下 def set_ui(self): self.scrollarea = SingleDirectionScrollArea() self.scrollarea.setWidgetResizable(True) self.scrollbar = self.scrollarea.verticalScrollBar() self.flow = Flow() self.create_txt() self.flow.addWidget(self.txt) self.flow.setMinimumHeight(self.scrollarea.size().height()) self.scrollarea.setWidget(self.flow) self.button = PrimaryPushButton(\u0026#34;发送\u0026#34;) self.inputtext = LineEdit() self.inputtext.setPlaceholderText(\u0026#34;请输入你的想法\u0026#34;) self.laout = QGridLayout(self) # self.appendWS([self.text,self.button,self.inputtext]) self.laout.addWidget(self.scrollarea,0,0,3,4) self.laout.addWidget(self.inputtext,4,0,1,3) self.laout.addWidget(self.button,4,3,1,1) self.button.clicked.connect(self.magic) self.agent = None self.worker = MyThead(self) self.setStyleSheet(\u0026#34;QPushButton{border-width:1px;border-radius:5px;background-color: #3366FF ;}\\ QLabel{border:3px solid black ;border-radius:5px;color:black}\u0026#34;) # 动态切换api 后端 def changeAgent(self,s): if s in Agents: if s==da: return self.agent = Agents[s] # 根据内容更新滚动条高度 def changeScrollBar(self, f:float): nexth =-1 if self.m_end +f \u0026gt;= self.flow.minimumHeight(): nexth = int(self.m_end+f)+50 self.flow.setMinimumHeight(nexth) self.scrollbar.setValue(self.scrollbar.maximum()) return nexth def create_txt(self): if self.gid\u0026gt;1: self.m_end =self.txt.geometry().y()+self.txt.geometry().height()+self.txt.top+self.txt.bottom self.sig.disconnect(self.txt.insertMarkdown) self.txt = TextArea(self) self.scrollbar.setValue(self.scrollbar.maximum()) self.txt.regist(self.changeScrollBar) self.flow.addWidget(self.txt) self.sig.connect(self.txt.insertMarkdown) def appendWS(self,l:list[QWidget]): for it in l: self.layout.addWidget(it) # 异步任务 def loopread(self,s:str): # 多api 响应，目前没做多轮兼容 if useALL: for k,a in Agents.items(): try: self.append(f\u0026#34;\\n\\n`{k}`: \\n\\n\u0026#34;) a.StreamCall(s,self.append) except Exception as e: logging.debug(f\u0026#34;多轮提问错误{e=}\u0026#34;) else: self.agent.StreamCall(s,self.append) # type: ignore self.button.setEnabled(True) if saveHistory: self.txt.export2File() def append(self,s): self.sig.emit(s) def showMessage(self,ty:str,s:str): TeachingTip.create( target=self.button, # type: ignore icon=InfoBarIcon.ERROR, title=ty, content=s, isClosable=True, tailPosition=TeachingTipTailPosition.BOTTOM_RIGHT, duration=2000, parent=self ) logging.debug(f\u0026#34;type={ty},{s}\u0026#34;) @Slot() def magic(self): if self.agent is None: # 延迟代理配置，使用户可以先通过界面生成配置文件后再重启后使用 if len(Agents)==0 or da == \u0026#34;\u0026#34;: self.showMessage(\u0026#34;Error:\u0026#34;,\u0026#34;请先在设置页面配置一个chatgpt api的接口设置\u0026#34;) else: self.agent = Agents[da] self.gid +=1 if self.gid\u0026gt;1: self.create_txt() if self.inputtext.text()!=\u0026#34;\u0026#34;: self.button.setDisabled(True) self.txt.insertMarkdown(f\u0026#34;#### Q: {self.inputtext.text()}\\n-----\\n**A:** \\n\u0026#34;) # 延迟启动异步线程 if not self.worker.isRunning(): self.worker.start() self.worker.addtask(lambda : self.loopread(self.inputtext.text())) # 用来处理线程关闭问题 def close(self) -\u0026gt; bool: self.worker.stop() self.worker.wait() self.worker.quit() self.worker.terminate() return super().close() def closeEvent(self, event) -\u0026gt; None: self.worker.stop() self.worker.wait() self.worker.quit() self.worker.terminate() event.accept() return super().closeEvent(event) api 后端 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 # 千问，Kimi,deepseek # 一直有ssl 验证的ca调用后面会排查一下怎么跳过 class openai: def __init__(self,a:auth) -\u0026gt; None: self.key =a.sk self.url =urlMap[a.type] self.client = OpenAI(api_key=self.key,base_url=self.url) self.history = [{\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;You are a helpful assistant.\u0026#39;}] self.model = ModelMap[a.type][0] self.isMulti = False def setModel(self,s:str): self.model = s def StreamCall(self,s:str,fn:Callable[[str],None]): if not self.isMulti: self.history = [] self.history.append({\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: f\u0026#39;{s}\u0026#39;}) cmp = self.client.chat.completions.create( model=self.model, messages=self.history, # type: ignore stream=True ) temp = \u0026#34;\u0026#34; for chunk in cmp: ctx = chunk.choices[0].delta.content try: if ctx and len(ctx)\u0026gt;0: fn(ctx) except Exception as e: logging.debug(f\u0026#34;streamcall {e=}\u0026#34;) if ctx: temp +=ctx if self.isMulti: self.history.append({\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: f\u0026#39;{temp}\u0026#39;}) # 百度千帆 class qf: def __init__(self,sk=None,ak=None) -\u0026gt; None: if sk==None or ak==None: raise ValueError(f\u0026#34;{sk=},{ak=} is invalid\u0026#34;) self.client = qianfan.ChatCompletion(ak=ak,sk=sk) self.model = ModelMap[\u0026#34;千帆\u0026#34;][0] self.history = [] self.isMulti =False def setModel(self,s:str): self.model = s def StreamCall(self,s:str,fn:Callable[[str],None]): temp = \u0026#34;\u0026#34; if not self.isMulti: self.history = [] self.history.append({\u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;,\u0026#34;content\u0026#34;:f\u0026#34;{s}\u0026#34;}) resp = self.client.do(model=self.model,messages=self.history,stream=True) for chunk in resp: ctx = chunk[\u0026#34;body\u0026#34;][\u0026#34;result\u0026#34;] try: if len(ctx)\u0026gt;0: fn(ctx) except Exception as e: showMessage(\u0026#34;错误\u0026#34;,f\u0026#34;{e=}\u0026#34;,None,None) temp +=ctx if self.isMulti: self.history.append({\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: f\u0026#39;{temp}\u0026#39;}) import json # 混元 from tencentcloud.common.common_client import CommonClient from tencentcloud.common import credential from tencentcloud.common.profile.client_profile import ClientProfile from tencentcloud.common.profile.http_profile import HttpProfile class hy: def __init__(self,sk=None,ak=None) -\u0026gt; None: if sk==None or ak==None: raise ValueError(f\u0026#34;{sk=},{ak=} is invalid\u0026#34;) self.model = ModelMap[\u0026#34;混元\u0026#34;][0] self.message = {\u0026#34;Messages\u0026#34;:[],\u0026#34;Stream\u0026#34;:True,\u0026#34;Model\u0026#34;:self.model} self.history =self.message[\u0026#34;Messages\u0026#34;] self.isMulti =False cred = credential.Credential(ak, sk) httpProfile = HttpProfile() httpProfile.endpoint = \u0026#34;hunyuan.tencentcloudapi.com\u0026#34; clientProfile = ClientProfile() clientProfile.httpProfile = httpProfile params = \u0026#34;{}\u0026#34;; self.client = CommonClient(\u0026#34;hunyuan\u0026#34;, \u0026#34;2023-09-01\u0026#34;, cred, \u0026#34;\u0026#34;, profile=clientProfile) def setModel(self,s:str): self.model = s self.message[\u0026#34;Model\u0026#34;] = self.model def StreamCall(self,s:str,fn:Callable[[str],None]): temp = \u0026#34;\u0026#34; if not self.isMulti: self.message[\u0026#34;Messages\u0026#34;] = [] self.message[\u0026#34;Messages\u0026#34;].append({\u0026#34;Role\u0026#34;:\u0026#34;user\u0026#34;,\u0026#34;Content\u0026#34;:f\u0026#34;{s}\u0026#34;}) resp = self.client.call_sse(\u0026#34;ChatCompletions\u0026#34;, self.message) for chunk in resp: ctx = json.loads(chunk[\u0026#34;data\u0026#34;])[\u0026#34;Choices\u0026#34;][0][\u0026#34;Delta\u0026#34;][\u0026#34;Content\u0026#34;] # ctx = chunk[\u0026#34;body\u0026#34;][\u0026#34;result\u0026#34;] try: if len(ctx)\u0026gt;0: fn(ctx) except Exception as e: showMessage(\u0026#34;错误\u0026#34;,f\u0026#34;{e=}\u0026#34;,None,None) temp +=ctx if self.isMulti: self.history.append({\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: f\u0026#39;{temp}\u0026#39;}) # 豆包的这个是比较恶心的，在ak，sk，上还要一个modelid 形式的key from volcenginesdkarkruntime import Ark class db: def __init__(self,sk=None,ak=None,model=None) -\u0026gt; None: if sk==None or ak==None or model==None: raise ValueError(f\u0026#34;{sk=},{ak=} is invalid\u0026#34;) self.client = Ark(ak=ak,sk=sk,base_url=\u0026#34;https://ark.cn-beijing.volces.com/api/v3\u0026#34;) self.model = model self.history = [] self.isMulti =False def setModel(self,s:str): self.model = s def StreamCall(self,s:str,fn:Callable[[str],None]): temp = \u0026#34;\u0026#34; if not self.isMulti: self.history = [] self.history.append({\u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;,\u0026#34;content\u0026#34;:f\u0026#34;{s}\u0026#34;}) resp: Stream[ChatCompletionChunk] = self.client.chat.completions.create(model=self.model,messages=self.history,stream=True) # type: ignore for chunk in resp: if not chunk.choices: continue ctx: str = chunk.choices[0].delta.content # type: ignore try: if len(ctx)\u0026gt;0: fn(ctx) except Exception as e: showMessage(\u0026#34;错误\u0026#34;,f\u0026#34;{e=}\u0026#34;,None,None) temp +=ctx if self.isMulti: self.history.append({\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: f\u0026#39;{temp}\u0026#39;}) 实现这个异步响应就是通过调用agent 接口的 streamcall ，通过传入的s 表示请求消息，fn表示每轮执行的回调函数，这里的ctx 之类的返回需要增加一个if ctx检查防止某些情况下拿到的对象是None 出现错误\n","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/%E4%B8%80%E4%B8%AAgptapi-%E6%A1%8C%E9%9D%A2%E5%A5%97%E4%BB%B6/","title":"一个gptapi 桌面套件"},{"content":"一个range 风格的范围迭代器封装实现 go 里面有个range 的表达式可以遍历很多容器，最近应该是1.21支持range int,这看起来很像 python的 range 了，很舒服，想着很久没用cpp 写东西了就准备按照迭代器整一个，做小点，也不用什么模板；\n第一种宏定义 这种其实类似 哪些OIer 常用的 for_each 宏\n1 #define rangeI(_I, _end) for (i = 0; i \u0026lt; (_end); i++) 其实就是一个包装常见的 for 表达式头的一个宏，限制挺多，优点是几乎零开销；\n迭代器类封装实现 这个东西实现上并未考虑太多性能,应该会需要创建两个对象，这个有点像cpp 20里面增加的，ranges::itoa_views()；但是这个功能更单一，仅用于整型范围迭代，而且接口也是更使用，提供，start，end，step，支持for range双向遍历\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class RangeIter { private: int step; int cur_val; public: RangeIter(int step, int val) : step(step), cur_val(val) {} // these three methods form the basis of an iterator for use with a rangeIter-based for loop bool operator!=(const RangeIter \u0026amp;other) const { if (step \u0026lt; 0) { return cur_val != other.cur_val \u0026amp;\u0026amp; cur_val \u0026gt; other.cur_val; } return cur_val != other.cur_val \u0026amp;\u0026amp; cur_val \u0026lt; other.cur_val; } // this method must be defined after the definition of IntVector since it needs to use it int operator*() const { return cur_val; } const RangeIter \u0026amp;operator++() { cur_val += step; return *this; } }; class Range { private: int _st, _end, _step; bool args_check() const { if (_step == 0) { return false; } if (_step \u0026lt; 0 \u0026amp;\u0026amp; _st \u0026lt;= _end) { return false; } return _step \u0026lt;= 0 || _st \u0026lt; _end; } public: Range(int start, int end, int step) : _st(start), _end(end), _step(step) {} Range(int start, int end) : _st(start), _end(end), _step(1) {} explicit Range(int end) : _st(0), _end(end), _step(1) {} RangeIter iter() { return RangeIter{_step, _st}; } RangeIter cbegin() const { if (!args_check()) { throw std::invalid_argument(\u0026#34;step should match the exper start+n*step\u0026gt;end\u0026#34;); } return RangeIter{_step, _st}; } RangeIter cend() const { // if (!args_check()){ // throw std::invalid_argument(\u0026#34;step should match the exper start+n*step\u0026gt;end\u0026#34;); // } return RangeIter{_step, _end}; } RangeIter begin() { if (!args_check()) { throw std::invalid_argument(\u0026#34;step should match the exper start+n*step\u0026gt;end\u0026#34;); } return RangeIter{_step, _st}; } RangeIter end() { return RangeIter{_step, _end}; } }; 这里对 begin 做了参数检查，避免意外的错误情况，导致无限循环之类的情况，其实正常情况应该是把RangeIter 这个迭代器实现给塞到 private 域里面去，避免他人骚操作；但这里只是一个简单的演示demo就不用考虑太多；\n调用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 int main() { std::vector\u0026lt;int\u0026gt; val{1, 2, 3}; int i = 0; rangeI(i, val.size()) { std::cout \u0026lt;\u0026lt; val[i] \u0026lt;\u0026lt; std::endl; } // 1 // 2 // 3 try { for (auto i : Range(static_cast\u0026lt;int\u0026gt;(val.size()), 0, -1)) { std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;;\\n\u0026#34;; } } catch (std::logic_error \u0026amp;e) { std::cout \u0026lt;\u0026lt; \u0026#34;error\u0026#34; \u0026lt;\u0026lt; e.what() \u0026lt;\u0026lt; std::endl; } // 3; // 2; // 1; std::cout \u0026lt;\u0026lt; std::endl; } 总之复习了for range和迭代器实现\n","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/%E4%B8%80%E4%B8%AArange-%E9%A3%8E%E6%A0%BC%E7%9A%84%E8%8C%83%E5%9B%B4%E8%BF%AD%E4%BB%A3%E5%99%A8%E5%B0%81%E8%A3%85%E5%AE%9E%E7%8E%B0/","title":"一个range 风格的范围迭代器封装实现"},{"content":"异步日志写实现 设计逻辑 内置 sync.pool 获取缓存，将 p 写入 bytes.Buffer，写入成功就将 buff 入队，然后使用轮询函数在循环中一直取队列里面的 buffer，使用刷写将缓存内容落盘，同时将缓存返回池中。在进程做优雅退出的时候，关联到异步写者，让其 for range 剩余的缓存，连续落盘； 落盘写函数逻辑：\n要写入的字节流长度是否大于缓存剩下的部分？ 是将当前写缓存内容刷写落盘， 否 直接追加到写缓存后面 异步轮询的 poller 函数会从队列里面取出 next writer，如果不是空的就直接执行缓存操作，这部分用 select 来做个定时操作，如果较长时间没有新的日志进来，就先把缓存里面有的数据落盘 go 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 type FixSizeLargeBuff struct { buf []byte } const Megabit = 1024 * 1024 func NewFixSizeLargeBuff() *FixSizeLargeBuff { return \u0026amp;FixSizeLargeBuff{buf: make([]byte, 0, Megabit)} } func (f *FixSizeLargeBuff) Avail() int { return Megabit - len(f.buf) } func (f *FixSizeLargeBuff) Reset() { f.buf = f.buf[:0] } func (f *FixSizeLargeBuff) Append(p []byte) (int, error) { if f.Avail() \u0026lt; len(p) { return 0, fmt.Errorf(\u0026#34;no avail free bytes\u0026#34;) } f.buf = append(f.buf, p...) return len(p), nil } type SimpleAsyncWriter struct { data chan *FixSizeLargeBuff curbuff *FixSizeLargeBuff buffpool sync.Pool wt io.Writer lock sync.Mutex wg sync.WaitGroup ct *time.Ticker last time.Time active chan struct{} } func NewSimpleAsyncWriter(w io.Writer, limit int) *SimpleAsyncWriter { ret := \u0026amp;SimpleAsyncWriter{ data: make(chan *FixSizeLargeBuff, limit), buffpool: sync.Pool{New: func() any { return NewFixSizeLargeBuff() }}, wt: w, lock: sync.Mutex{}, active: make(chan struct{}), ct: time.NewTicker(1 * time.Second), } ret.addCount() go ret.poller() return ret } func (s *SimpleAsyncWriter) addCount() { s.wg.Add(1) } func (s *SimpleAsyncWriter) Write(p []byte) (int, error) { select { case \u0026lt;-s.active: return 0, ErrorWriteAsyncerIsClosed default: } s.last = time.Now() s.lock.Lock() defer s.lock.Unlock() select { case \u0026lt;-s.active: return 0, ErrorWriteAsyncerIsClosed case \u0026lt;-s.ct.C: if s.curbuff.Avail() \u0026gt; 0 \u0026amp;\u0026amp; time.Since(s.last) \u0026gt; 5*time.Second { s.data \u0026lt;- s.curbuff s.curbuff = s.buffpool.Get().(*FixSizeLargeBuff) } default: if s.curbuff == nil { s.curbuff = s.buffpool.Get().(*FixSizeLargeBuff) } if len(p) \u0026gt; s.curbuff.Avail() { s.data \u0026lt;- s.curbuff s.curbuff = s.buffpool.Get().(*FixSizeLargeBuff) } } if n, err := s.curbuff.Append(p); err != nil { return n, err } return len(p), nil } func (s *SimpleAsyncWriter) poller() { defer func() { for i := len(s.data); i \u0026gt; 0; i-- { d := \u0026lt;-s.data s.wt.Write(d.buf) } if s.curbuff.Avail() \u0026gt; 0 { s.wt.Write(s.curbuff.buf) } close(s.data) s.data = nil s.ct.Stop() s.wg.Done() }() for { select { case \u0026lt;-s.active: goto outer case d := \u0026lt;-s.data: s.wt.Write(d.buf) d.Reset() s.buffpool.Put(d) } } outer: } func (s *SimpleAsyncWriter) Stop() { s.active \u0026lt;- struct{}{} s.wg.Wait() } 基准测试 使用 law 的 benckmark 测试并给 BlackHoleWriter 类的 Writer 增加了时延模拟真实的落盘耗时，使用随机预先生成的字节数组队列来模拟真实负载填充，下面是 benckmark 测试的耗时\n第一列是测试项-cpu 数，第二列是每秒钟执行的次数，第三列是耗时，第四列是每个操作分配的字节数(这个可能是我预生成的随机字节数组拷贝时产生的)，第五列是每个操作的分配次数\n可以看到在 write 写操作平均耗时和平均分配的字节数来看，都有比较明显的优化，同时因为利用 defer 和 wait 机制联动，可以保证在调用 stop 是已经写入缓存的内容可以安全落盘，可以优雅推出，同步机制使用原生 channel 管道\n特点\n轻量级，代码简洁封装少，使用原生操作实现 多缓冲，使用sync.Pool 实现多级缓冲 安全并发， 虽然使用了互斥锁，但是仅用在write 方法上，对性能影响足够小 ","date":"2024-12-02T22:36:03+08:00","permalink":"https://daidaij.github.io/p/%E5%BC%82%E6%AD%A5%E6%97%A5%E5%BF%97%E5%86%99%E5%AE%9E%E7%8E%B0/","title":"异步日志写实现"}]