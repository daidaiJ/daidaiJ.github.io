<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ai on 潘达窝</title><link>https://daidaij.github.io/categories/ai/</link><description>Recent content in Ai on 潘达窝</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>pandazhangs</copyright><lastBuildDate>Mon, 09 Feb 2026 10:40:46 +0800</lastBuildDate><atom:link href="https://daidaij.github.io/categories/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Vllm deploy</title><link>https://daidaij.github.io/p/vllm/</link><pubDate>Mon, 09 Feb 2026 10:40:46 +0800</pubDate><guid>https://daidaij.github.io/p/vllm/</guid><description>&lt;img src="https://picsum.photos/seed/2d04c5e3/800/600" alt="Featured image of post Vllm deploy" />&lt;h1 id="vllm-部署大模型手册">vLLM 部署大模型手册
&lt;/h1>&lt;hr>
&lt;p>以Qwen2.5-32B-Instruct 为例子&lt;br>
基本命令：&lt;br>
&lt;code>python3 -m vllm.entrypoints.openai.api_server --trust-remote-code --enable-prefix-caching --disable-log-requests --model /data --gpu-memory-utilization 0.90 -tp 8 --port 8000 --served-model-name Qwen2.5-32B-Instruct --max-model-len 32768&lt;/code>&lt;/p>
&lt;h2 id="vllm常见参数配置说明">vLLM常见参数配置说明
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-txt" data-lang="txt">&lt;span class="line">&lt;span class="cl">--model 挂载模型目录（与【挂载地址】保持一致）
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-tp 卡数（部署模型需要几张卡就写几张）
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--port 端口（默认8000即可）
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--max-model-len 最大上下文长度
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--served-model-name 模型名称
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--gpu-memory-utilization 0.90
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--max_num_seqs batch 里面的最大输入序列个数
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--enforce_eager 禁用图捕获优化参数，能节省一些显存
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--pipeline_parallel_size 一般是多节点部署时，按照这个参数去拆分模型层，多个节点构成流水线，但是会增加延迟
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--max_num_batched_tokens = batch_size * max_model_len， 是一个batch 中总的token 数
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--enable_expert_parallel 对部分MOE 专家模型，启用专家并行
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--kv-transfer-config &amp;#39;{&amp;#34;kv_connector&amp;#34;:&amp;#34;MooncakeConnector&amp;#34;,&amp;#34;kv_role&amp;#34;:&amp;#34;kv_consumer&amp;#34;}&amp;#39; 安装mooncake-transfer-engine 可以通过多级缓存和RDMA 来加速kv cache 传输，优化推理
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-dcp 这个计算公式是 tp数/（kv 头数*节点数） ，可以保持同个节点上不会重复载入
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">注：预估 tp 的值（卡数），简单预算公式（单位GB）：模型文件总大小 * 1.2 &amp;lt;= GPU显存大小
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="多节点部署">多节点部署
&lt;/h2>&lt;h3 id="mp-运行时">mp 运行时
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># master 节点&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">python3 -m vllm.entrypoints.openai.api_server --trust-remote-code --enable-prefix-caching --disable-log-requests --model /data --gpu-memory-utilization 0.90 -tp &lt;span class="m">2&lt;/span> --port &lt;span class="m">8000&lt;/span> --served-model-name Qwen2.5-32B-Instruct &lt;span class="c1"># 注意下面这几行 &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--nnodes &lt;span class="m">2&lt;/span> &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span>--node-rank &lt;span class="m">0&lt;/span> &lt;span class="se">\ &lt;/span>&lt;span class="c1"># 不同节点这个就这个rank 数不同，master 用0 &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--master-addr 192.168.0.101 &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span>--distributed-executor-backend mp
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="ray-计算后端">ray 计算后端
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 首先，先在master 节点&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ray start --head --port&lt;span class="o">=&lt;/span>&lt;span class="m">6379&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 然后其他worker 节点&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ray start --address&lt;span class="o">=&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">master_ip&lt;/span>&lt;span class="si">}&lt;/span>:6379
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 这时候ray 集群通信就建立起来了&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">python3 -m vllm.entrypoints.openai.api_server --trust-remote-code --enable-prefix-caching --disable-log-requests --model /data --gpu-memory-utilization 0.90 --port &lt;span class="m">8000&lt;/span> -tp &lt;span class="m">8&lt;/span> --pipeline-parallel-size &lt;span class="m">2&lt;/span> --served-model-name Qwen2.5-32B-Instruct &lt;span class="se">\ &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 注意下面这个&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--distributed-executor-backend ray
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>使用vllm 的示例脚本简化上述过程&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># master&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader --ray_cluster_size&lt;span class="o">=&lt;/span>&lt;span class="m">2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">python3 -m vllm.entrypoints.openai.api_server --trust-remote-code --enable-prefix-caching --disable-log-requests --model /data --gpu-memory-utilization 0.90 --port &lt;span class="m">8000&lt;/span> -tp &lt;span class="m">8&lt;/span> --pipeline-parallel-size &lt;span class="m">2&lt;/span> --served-model-name Qwen2.5-32B-Instruct
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># worker&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bash /vllm-workspace/examples/online_serving/multi-node-serving.sh worker --ray_cluster_size&lt;span class="o">=&lt;/span>&lt;span class="m">2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="volcano-分布式任务编排">Volcano 分布式任务编排
&lt;/h3>&lt;p>其实Volcano 本质上是对不同分布式任务运行时的编排调度管理工具，所以按照上面两种分布式运行时后端，创建job 时适配command 命令就行，volcano 会按照工作组做资源分配管理&lt;br>
参照 &lt;a class="link" href="https://docs.vllm.ai/projects/ascend/zh-cn/v0.13.0/user_guide/deployment_guide/using_volcano_kthena.html" target="_blank" rel="noopener"
>Volcano Kthena 在 Kubernetes 上部署多节点LLM推理&lt;/a>&lt;/p></description></item><item><title>结合lora和离线学习的llm蒸馏策略</title><link>https://daidaij.github.io/p/llm/</link><pubDate>Fri, 05 Dec 2025 16:29:45 +0800</pubDate><guid>https://daidaij.github.io/p/llm/</guid><description>&lt;img src="https://picsum.photos/seed/b6a2445d/800/600" alt="Featured image of post 结合lora和离线学习的llm蒸馏策略" />&lt;h1 id="大模型的lora-想法">大模型的lora 想法
&lt;/h1>&lt;p>大模型本身在推理部署的场景下对模型显存的占用量就是很夸张的，导致训练大模型成本很高，基于lora 在领域语料上继续学习垂直知识是一个低成本的最佳实践，最近组里也在测试蒸馏学习的最佳实践，便有了如下的想法，估计很多人都已经付诸于实践了，我这点东西只是记录下碎片化的idea。&lt;/p>
&lt;h2 id="简单的模型蒸馏">简单的模型蒸馏
&lt;/h2>&lt;p>在前几年bert刷分的时代，bert 蒸馏技术在印象中可以简单归纳为两种:&lt;/p>
&lt;ol>
&lt;li>logit 层感知学习，对照教师模型和学生模型在最后一个logit 层的张量损失，作为学生模型的学习损失&lt;/li>
&lt;li>hidden 层感知学习， 这个是将中间多层transformers 块的输出也考虑进来，逐个对照输出分布差异，将各层损失求和&lt;/li>
&lt;/ol>
&lt;p>这两种方法在资源紧张的情况下，可以通过保存张量，和输入预料关联制作成离线的蒸馏学习数据集：
好处是：&lt;/p>
&lt;ul>
&lt;li>蒸馏学习时可以省去一个教师大模型占据的显存资源，成本低；&lt;/li>
&lt;li>同时这部分数据可以多次复用，用时间去换空间；&lt;/li>
&lt;/ul>
&lt;h2 id="lora-微调-llm">lora 微调 llm
&lt;/h2>&lt;p>现在基于peft 的lora 微调流程其实已经很成熟了，lora 微调可以使得llm在领域预料上的finetune 时占用显存更小，便于持续学习这种模式训练llm，同时没有改变 logit 层的输出维度，使用lora 的同时还可以开量化，半精度训练来感知量化，减少因为精度损失带来的性能退化。&lt;/p>
&lt;h2 id="结合起来">结合起来
&lt;/h2>&lt;ol>
&lt;li>先用教师大模型提取数据集的离线张量数据&lt;/li>
&lt;li>学生模型用lora 开量化在相同数据上同时计算任务损失和教师模型的学习损失
最终学生模型会获得一个在单纯量化上利用蒸馏张量和训练恢复性能的lora 模块，这个流程标准化后蒸馏和微调小模型的成本会继续下降&lt;/li>
&lt;/ol></description></item></channel></rss>